
<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <title>Checkpoint Parameter Comparison</title>
    <style>
      body { font-family: system-ui, -apple-system, 'Segoe UI', sans-serif; margin: 2rem; }
      h1 { margin-bottom: 1rem; }
      .summary { margin-bottom: 1.5rem; }
      table { border-collapse: collapse; width: 100%; }
      th, td { border: 1px solid #ddd; padding: 0.4rem 0.6rem; }
      th { background: #f8f8f8; position: sticky; top: 0; }
    </style>
  </head>
  <body>
    <h1>Checkpoint Parameter Comparison</h1>
    <p class="summary">Columns: model 1 (large_vox_iter5.pt), model 2 (checkpoint_best.pt)</p>
    <style type="text/css">
#T_c6ca4_row0_col6, #T_c6ca4_row1_col6, #T_c6ca4_row532_col6, #T_c6ca4_row533_col6 {
  width: 10em;
}
#T_c6ca4_row0_col7, #T_c6ca4_row1_col7, #T_c6ca4_row532_col7, #T_c6ca4_row533_col7 {
  background-color: #000000;
  color: #f1f1f1;
}
#T_c6ca4_row2_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 54.2%, transparent 54.2%);
}
#T_c6ca4_row2_col7, #T_c6ca4_row393_col7 {
  background-color: #e3f399;
  color: #000000;
}
#T_c6ca4_row3_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 19.8%, transparent 19.8%);
}
#T_c6ca4_row3_col7, #T_c6ca4_row7_col7, #T_c6ca4_row9_col7, #T_c6ca4_row11_col7, #T_c6ca4_row12_col7, #T_c6ca4_row14_col7, #T_c6ca4_row15_col7, #T_c6ca4_row30_col7, #T_c6ca4_row40_col7, #T_c6ca4_row46_col7, #T_c6ca4_row56_col7, #T_c6ca4_row62_col7, #T_c6ca4_row78_col7, #T_c6ca4_row86_col7, #T_c6ca4_row88_col7, #T_c6ca4_row94_col7, #T_c6ca4_row102_col7, #T_c6ca4_row104_col7, #T_c6ca4_row110_col7, #T_c6ca4_row120_col7, #T_c6ca4_row126_col7, #T_c6ca4_row142_col7, #T_c6ca4_row158_col7, #T_c6ca4_row160_col7, #T_c6ca4_row170_col7, #T_c6ca4_row174_col7, #T_c6ca4_row176_col7, #T_c6ca4_row190_col7, #T_c6ca4_row192_col7, #T_c6ca4_row200_col7, #T_c6ca4_row206_col7, #T_c6ca4_row211_col7, #T_c6ca4_row222_col7, #T_c6ca4_row224_col7, #T_c6ca4_row232_col7, #T_c6ca4_row238_col7, #T_c6ca4_row259_col7, #T_c6ca4_row264_col7, #T_c6ca4_row280_col7, #T_c6ca4_row286_col7, #T_c6ca4_row296_col7, #T_c6ca4_row298_col7, #T_c6ca4_row302_col7, #T_c6ca4_row312_col7, #T_c6ca4_row314_col7, #T_c6ca4_row318_col7, #T_c6ca4_row328_col7, #T_c6ca4_row330_col7, #T_c6ca4_row350_col7, #T_c6ca4_row360_col7, #T_c6ca4_row366_col7, #T_c6ca4_row376_col7, #T_c6ca4_row382_col7, #T_c6ca4_row396_col7, #T_c6ca4_row399_col7, #T_c6ca4_row410_col7, #T_c6ca4_row413_col7, #T_c6ca4_row416_col7, #T_c6ca4_row418_col7, #T_c6ca4_row419_col7, #T_c6ca4_row426_col7, #T_c6ca4_row427_col7, #T_c6ca4_row430_col7, #T_c6ca4_row435_col7, #T_c6ca4_row440_col7, #T_c6ca4_row441_col7, #T_c6ca4_row442_col7, #T_c6ca4_row443_col7, #T_c6ca4_row446_col7, #T_c6ca4_row450_col7, #T_c6ca4_row458_col7, #T_c6ca4_row460_col7, #T_c6ca4_row464_col7, #T_c6ca4_row472_col7, #T_c6ca4_row481_col7, #T_c6ca4_row484_col7, #T_c6ca4_row487_col7, #T_c6ca4_row492_col7, #T_c6ca4_row518_col7, #T_c6ca4_row523_col7, #T_c6ca4_row531_col7, #T_c6ca4_row538_col7 {
  background-color: #016a38;
  color: #f1f1f1;
}
#T_c6ca4_row4_col6, #T_c6ca4_row114_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 8.7%, transparent 8.7%);
}
#T_c6ca4_row4_col7, #T_c6ca4_row20_col7, #T_c6ca4_row32_col7, #T_c6ca4_row66_col7, #T_c6ca4_row74_col7, #T_c6ca4_row114_col7, #T_c6ca4_row135_col7, #T_c6ca4_row237_col7, #T_c6ca4_row257_col7, #T_c6ca4_row394_col7, #T_c6ca4_row525_col7, #T_c6ca4_row526_col7 {
  background-color: #07753e;
  color: #f1f1f1;
}
#T_c6ca4_row5_col6, #T_c6ca4_row23_col6, #T_c6ca4_row27_col6, #T_c6ca4_row72_col6, #T_c6ca4_row331_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 5.1%, transparent 5.1%);
}
#T_c6ca4_row5_col7, #T_c6ca4_row18_col7, #T_c6ca4_row21_col7, #T_c6ca4_row23_col7, #T_c6ca4_row24_col7, #T_c6ca4_row27_col7, #T_c6ca4_row31_col7, #T_c6ca4_row35_col7, #T_c6ca4_row72_col7, #T_c6ca4_row96_col7, #T_c6ca4_row112_col7, #T_c6ca4_row144_col7, #T_c6ca4_row194_col7, #T_c6ca4_row197_col7, #T_c6ca4_row199_col7, #T_c6ca4_row203_col7, #T_c6ca4_row207_col7, #T_c6ca4_row212_col7, #T_c6ca4_row228_col7, #T_c6ca4_row244_col7, #T_c6ca4_row254_col7, #T_c6ca4_row272_col7, #T_c6ca4_row277_col7, #T_c6ca4_row279_col7, #T_c6ca4_row283_col7, #T_c6ca4_row287_col7, #T_c6ca4_row293_col7, #T_c6ca4_row295_col7, #T_c6ca4_row299_col7, #T_c6ca4_row303_col7, #T_c6ca4_row311_col7, #T_c6ca4_row315_col7, #T_c6ca4_row319_col7, #T_c6ca4_row327_col7, #T_c6ca4_row331_col7, #T_c6ca4_row335_col7, #T_c6ca4_row346_col7, #T_c6ca4_row368_col7, #T_c6ca4_row404_col7, #T_c6ca4_row409_col7, #T_c6ca4_row423_col7, #T_c6ca4_row433_col7, #T_c6ca4_row437_col7, #T_c6ca4_row445_col7, #T_c6ca4_row461_col7, #T_c6ca4_row474_col7, #T_c6ca4_row475_col7, #T_c6ca4_row476_col7, #T_c6ca4_row510_col7, #T_c6ca4_row536_col7 {
  background-color: #026c39;
  color: #f1f1f1;
}
#T_c6ca4_row6_col6, #T_c6ca4_row22_col6, #T_c6ca4_row429_col6, #T_c6ca4_row448_col6, #T_c6ca4_row507_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 1.8%, transparent 1.8%);
}
#T_c6ca4_row6_col7, #T_c6ca4_row10_col7, #T_c6ca4_row22_col7, #T_c6ca4_row25_col7, #T_c6ca4_row26_col7, #T_c6ca4_row28_col7, #T_c6ca4_row38_col7, #T_c6ca4_row41_col7, #T_c6ca4_row44_col7, #T_c6ca4_row51_col7, #T_c6ca4_row54_col7, #T_c6ca4_row57_col7, #T_c6ca4_row60_col7, #T_c6ca4_row67_col7, #T_c6ca4_row70_col7, #T_c6ca4_row73_col7, #T_c6ca4_row76_col7, #T_c6ca4_row83_col7, #T_c6ca4_row89_col7, #T_c6ca4_row92_col7, #T_c6ca4_row99_col7, #T_c6ca4_row105_col7, #T_c6ca4_row108_col7, #T_c6ca4_row115_col7, #T_c6ca4_row118_col7, #T_c6ca4_row121_col7, #T_c6ca4_row124_col7, #T_c6ca4_row131_col7, #T_c6ca4_row134_col7, #T_c6ca4_row136_col7, #T_c6ca4_row137_col7, #T_c6ca4_row140_col7, #T_c6ca4_row147_col7, #T_c6ca4_row150_col7, #T_c6ca4_row152_col7, #T_c6ca4_row153_col7, #T_c6ca4_row154_col7, #T_c6ca4_row156_col7, #T_c6ca4_row163_col7, #T_c6ca4_row166_col7, #T_c6ca4_row168_col7, #T_c6ca4_row169_col7, #T_c6ca4_row172_col7, #T_c6ca4_row179_col7, #T_c6ca4_row182_col7, #T_c6ca4_row184_col7, #T_c6ca4_row185_col7, #T_c6ca4_row188_col7, #T_c6ca4_row195_col7, #T_c6ca4_row198_col7, #T_c6ca4_row201_col7, #T_c6ca4_row202_col7, #T_c6ca4_row204_col7, #T_c6ca4_row214_col7, #T_c6ca4_row216_col7, #T_c6ca4_row217_col7, #T_c6ca4_row220_col7, #T_c6ca4_row227_col7, #T_c6ca4_row230_col7, #T_c6ca4_row233_col7, #T_c6ca4_row236_col7, #T_c6ca4_row240_col7, #T_c6ca4_row243_col7, #T_c6ca4_row246_col7, #T_c6ca4_row248_col7, #T_c6ca4_row249_col7, #T_c6ca4_row252_col7, #T_c6ca4_row256_col7, #T_c6ca4_row262_col7, #T_c6ca4_row265_col7, #T_c6ca4_row268_col7, #T_c6ca4_row275_col7, #T_c6ca4_row278_col7, #T_c6ca4_row281_col7, #T_c6ca4_row282_col7, #T_c6ca4_row284_col7, #T_c6ca4_row291_col7, #T_c6ca4_row294_col7, #T_c6ca4_row297_col7, #T_c6ca4_row300_col7, #T_c6ca4_row307_col7, #T_c6ca4_row310_col7, #T_c6ca4_row313_col7, #T_c6ca4_row316_col7, #T_c6ca4_row323_col7, #T_c6ca4_row326_col7, #T_c6ca4_row329_col7, #T_c6ca4_row332_col7, #T_c6ca4_row334_col7, #T_c6ca4_row339_col7, #T_c6ca4_row342_col7, #T_c6ca4_row344_col7, #T_c6ca4_row345_col7, #T_c6ca4_row348_col7, #T_c6ca4_row355_col7, #T_c6ca4_row358_col7, #T_c6ca4_row361_col7, #T_c6ca4_row362_col7, #T_c6ca4_row364_col7, #T_c6ca4_row371_col7, #T_c6ca4_row374_col7, #T_c6ca4_row377_col7, #T_c6ca4_row378_col7, #T_c6ca4_row380_col7, #T_c6ca4_row387_col7, #T_c6ca4_row388_col7, #T_c6ca4_row389_col7, #T_c6ca4_row395_col7, #T_c6ca4_row397_col7, #T_c6ca4_row400_col7, #T_c6ca4_row401_col7, #T_c6ca4_row403_col7, #T_c6ca4_row406_col7, #T_c6ca4_row407_col7, #T_c6ca4_row408_col7, #T_c6ca4_row411_col7, #T_c6ca4_row412_col7, #T_c6ca4_row414_col7, #T_c6ca4_row415_col7, #T_c6ca4_row417_col7, #T_c6ca4_row420_col7, #T_c6ca4_row421_col7, #T_c6ca4_row422_col7, #T_c6ca4_row424_col7, #T_c6ca4_row425_col7, #T_c6ca4_row428_col7, #T_c6ca4_row429_col7, #T_c6ca4_row431_col7, #T_c6ca4_row434_col7, #T_c6ca4_row436_col7, #T_c6ca4_row438_col7, #T_c6ca4_row439_col7, #T_c6ca4_row444_col7, #T_c6ca4_row447_col7, #T_c6ca4_row448_col7, #T_c6ca4_row449_col7, #T_c6ca4_row451_col7, #T_c6ca4_row453_col7, #T_c6ca4_row454_col7, #T_c6ca4_row455_col7, #T_c6ca4_row456_col7, #T_c6ca4_row459_col7, #T_c6ca4_row462_col7, #T_c6ca4_row463_col7, #T_c6ca4_row465_col7, #T_c6ca4_row467_col7, #T_c6ca4_row468_col7, #T_c6ca4_row469_col7, #T_c6ca4_row470_col7, #T_c6ca4_row473_col7, #T_c6ca4_row477_col7, #T_c6ca4_row478_col7, #T_c6ca4_row480_col7, #T_c6ca4_row482_col7, #T_c6ca4_row483_col7, #T_c6ca4_row485_col7, #T_c6ca4_row488_col7, #T_c6ca4_row489_col7, #T_c6ca4_row490_col7, #T_c6ca4_row493_col7, #T_c6ca4_row496_col7, #T_c6ca4_row497_col7, #T_c6ca4_row499_col7, #T_c6ca4_row502_col7, #T_c6ca4_row504_col7, #T_c6ca4_row507_col7, #T_c6ca4_row512_col7, #T_c6ca4_row515_col7, #T_c6ca4_row516_col7, #T_c6ca4_row517_col7, #T_c6ca4_row519_col7, #T_c6ca4_row522_col7, #T_c6ca4_row524_col7, #T_c6ca4_row527_col7, #T_c6ca4_row530_col7, #T_c6ca4_row534_col7 {
  background-color: #006837;
  color: #f1f1f1;
}
#T_c6ca4_row7_col6, #T_c6ca4_row24_col6, #T_c6ca4_row244_col6, #T_c6ca4_row460_col6, #T_c6ca4_row461_col6, #T_c6ca4_row476_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 4.7%, transparent 4.7%);
}
#T_c6ca4_row8_col6, #T_c6ca4_row35_col6, #T_c6ca4_row128_col6, #T_c6ca4_row226_col6, #T_c6ca4_row272_col6, #T_c6ca4_row363_col6, #T_c6ca4_row368_col6, #T_c6ca4_row437_col6, #T_c6ca4_row510_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 5.6%, transparent 5.6%);
}
#T_c6ca4_row8_col7, #T_c6ca4_row19_col7, #T_c6ca4_row29_col7, #T_c6ca4_row33_col7, #T_c6ca4_row39_col7, #T_c6ca4_row64_col7, #T_c6ca4_row80_col7, #T_c6ca4_row97_col7, #T_c6ca4_row113_col7, #T_c6ca4_row128_col7, #T_c6ca4_row129_col7, #T_c6ca4_row145_col7, #T_c6ca4_row148_col7, #T_c6ca4_row161_col7, #T_c6ca4_row177_col7, #T_c6ca4_row180_col7, #T_c6ca4_row186_col7, #T_c6ca4_row193_col7, #T_c6ca4_row209_col7, #T_c6ca4_row226_col7, #T_c6ca4_row242_col7, #T_c6ca4_row258_col7, #T_c6ca4_row260_col7, #T_c6ca4_row289_col7, #T_c6ca4_row305_col7, #T_c6ca4_row309_col7, #T_c6ca4_row321_col7, #T_c6ca4_row325_col7, #T_c6ca4_row333_col7, #T_c6ca4_row337_col7, #T_c6ca4_row341_col7, #T_c6ca4_row343_col7, #T_c6ca4_row347_col7, #T_c6ca4_row351_col7, #T_c6ca4_row352_col7, #T_c6ca4_row353_col7, #T_c6ca4_row357_col7, #T_c6ca4_row359_col7, #T_c6ca4_row363_col7, #T_c6ca4_row367_col7, #T_c6ca4_row369_col7, #T_c6ca4_row375_col7, #T_c6ca4_row379_col7, #T_c6ca4_row383_col7, #T_c6ca4_row384_col7, #T_c6ca4_row385_col7, #T_c6ca4_row390_col7, #T_c6ca4_row452_col7, #T_c6ca4_row479_col7, #T_c6ca4_row486_col7, #T_c6ca4_row494_col7, #T_c6ca4_row495_col7, #T_c6ca4_row508_col7, #T_c6ca4_row514_col7, #T_c6ca4_row539_col7 {
  background-color: #036e3a;
  color: #f1f1f1;
}
#T_c6ca4_row9_col6, #T_c6ca4_row14_col6, #T_c6ca4_row15_col6, #T_c6ca4_row427_col6, #T_c6ca4_row538_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 4.2%, transparent 4.2%);
}
#T_c6ca4_row10_col6, #T_c6ca4_row67_col6, #T_c6ca4_row83_col6, #T_c6ca4_row136_col6, #T_c6ca4_row246_col6, #T_c6ca4_row307_col6, #T_c6ca4_row371_col6, #T_c6ca4_row374_col6, #T_c6ca4_row414_col6, #T_c6ca4_row468_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 2.6%, transparent 2.6%);
}
#T_c6ca4_row11_col6, #T_c6ca4_row12_col6, #T_c6ca4_row56_col6, #T_c6ca4_row88_col6, #T_c6ca4_row192_col6, #T_c6ca4_row222_col6, #T_c6ca4_row238_col6, #T_c6ca4_row396_col6, #T_c6ca4_row416_col6, #T_c6ca4_row493_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 4.3%, transparent 4.3%);
}
#T_c6ca4_row13_col6, #T_c6ca4_row43_col6, #T_c6ca4_row49_col6, #T_c6ca4_row59_col6, #T_c6ca4_row65_col6, #T_c6ca4_row81_col6, #T_c6ca4_row205_col6, #T_c6ca4_row225_col6, #T_c6ca4_row285_col6, #T_c6ca4_row301_col6, #T_c6ca4_row373_col6, #T_c6ca4_row479_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 6.6%, transparent 6.6%);
}
#T_c6ca4_row13_col7, #T_c6ca4_row17_col7, #T_c6ca4_row34_col7, #T_c6ca4_row37_col7, #T_c6ca4_row43_col7, #T_c6ca4_row47_col7, #T_c6ca4_row49_col7, #T_c6ca4_row55_col7, #T_c6ca4_row59_col7, #T_c6ca4_row63_col7, #T_c6ca4_row65_col7, #T_c6ca4_row75_col7, #T_c6ca4_row79_col7, #T_c6ca4_row81_col7, #T_c6ca4_row91_col7, #T_c6ca4_row95_col7, #T_c6ca4_row109_col7, #T_c6ca4_row125_col7, #T_c6ca4_row141_col7, #T_c6ca4_row164_col7, #T_c6ca4_row173_col7, #T_c6ca4_row178_col7, #T_c6ca4_row189_col7, #T_c6ca4_row205_col7, #T_c6ca4_row225_col7, #T_c6ca4_row285_col7, #T_c6ca4_row292_col7, #T_c6ca4_row301_col7, #T_c6ca4_row308_col7, #T_c6ca4_row317_col7, #T_c6ca4_row336_col7, #T_c6ca4_row349_col7, #T_c6ca4_row365_col7, #T_c6ca4_row373_col7, #T_c6ca4_row381_col7, #T_c6ca4_row432_col7, #T_c6ca4_row466_col7, #T_c6ca4_row509_col7, #T_c6ca4_row513_col7 {
  background-color: #04703b;
  color: #f1f1f1;
}
#T_c6ca4_row16_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 12.3%, transparent 12.3%);
}
#T_c6ca4_row16_col7, #T_c6ca4_row266_col7 {
  background-color: #0e8245;
  color: #f1f1f1;
}
#T_c6ca4_row17_col6, #T_c6ca4_row63_col6, #T_c6ca4_row164_col6, #T_c6ca4_row317_col6, #T_c6ca4_row432_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 6.7%, transparent 6.7%);
}
#T_c6ca4_row18_col6, #T_c6ca4_row21_col6, #T_c6ca4_row203_col6, #T_c6ca4_row207_col6, #T_c6ca4_row254_col6, #T_c6ca4_row283_col6, #T_c6ca4_row311_col6, #T_c6ca4_row335_col6, #T_c6ca4_row404_col6, #T_c6ca4_row409_col6, #T_c6ca4_row475_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 5.2%, transparent 5.2%);
}
#T_c6ca4_row19_col6, #T_c6ca4_row129_col6, #T_c6ca4_row145_col6, #T_c6ca4_row260_col6, #T_c6ca4_row305_col6, #T_c6ca4_row321_col6, #T_c6ca4_row369_col6, #T_c6ca4_row383_col6, #T_c6ca4_row486_col6, #T_c6ca4_row508_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 6.0%, transparent 6.0%);
}
#T_c6ca4_row20_col6, #T_c6ca4_row257_col6, #T_c6ca4_row467_col6, #T_c6ca4_row525_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 8.9%, transparent 8.9%);
}
#T_c6ca4_row25_col6, #T_c6ca4_row38_col6, #T_c6ca4_row76_col6, #T_c6ca4_row92_col6, #T_c6ca4_row118_col6, #T_c6ca4_row230_col6, #T_c6ca4_row236_col6, #T_c6ca4_row281_col6, #T_c6ca4_row406_col6, #T_c6ca4_row421_col6, #T_c6ca4_row455_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 2.8%, transparent 2.8%);
}
#T_c6ca4_row26_col6, #T_c6ca4_row51_col6, #T_c6ca4_row179_col6, #T_c6ca4_row182_col6, #T_c6ca4_row184_col6, #T_c6ca4_row188_col6, #T_c6ca4_row378_col6, #T_c6ca4_row395_col6, #T_c6ca4_row454_col6, #T_c6ca4_row516_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 2.4%, transparent 2.4%);
}
#T_c6ca4_row28_col6, #T_c6ca4_row170_col6, #T_c6ca4_row190_col6, #T_c6ca4_row204_col6, #T_c6ca4_row282_col6, #T_c6ca4_row344_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 3.6%, transparent 3.6%);
}
#T_c6ca4_row29_col6, #T_c6ca4_row148_col6, #T_c6ca4_row333_col6, #T_c6ca4_row349_col6, #T_c6ca4_row509_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 6.5%, transparent 6.5%);
}
#T_c6ca4_row30_col6, #T_c6ca4_row240_col6, #T_c6ca4_row296_col6, #T_c6ca4_row330_col6, #T_c6ca4_row334_col6, #T_c6ca4_row350_col6, #T_c6ca4_row366_col6, #T_c6ca4_row481_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 3.4%, transparent 3.4%);
}
#T_c6ca4_row31_col6, #T_c6ca4_row474_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 4.9%, transparent 4.9%);
}
#T_c6ca4_row32_col6, #T_c6ca4_row181_col6, #T_c6ca4_row191_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 9.3%, transparent 9.3%);
}
#T_c6ca4_row33_col6, #T_c6ca4_row39_col6, #T_c6ca4_row97_col6, #T_c6ca4_row186_col6, #T_c6ca4_row209_col6, #T_c6ca4_row375_col6, #T_c6ca4_row452_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 6.3%, transparent 6.3%);
}
#T_c6ca4_row34_col6, #T_c6ca4_row37_col6, #T_c6ca4_row47_col6, #T_c6ca4_row55_col6, #T_c6ca4_row75_col6, #T_c6ca4_row336_col6, #T_c6ca4_row365_col6, #T_c6ca4_row424_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 6.8%, transparent 6.8%);
}
#T_c6ca4_row36_col6, #T_c6ca4_row45_col6, #T_c6ca4_row61_col6, #T_c6ca4_row77_col6, #T_c6ca4_row85_col6, #T_c6ca4_row130_col6, #T_c6ca4_row143_col6, #T_c6ca4_row241_col6, #T_c6ca4_row304_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 7.7%, transparent 7.7%);
}
#T_c6ca4_row36_col7, #T_c6ca4_row42_col7, #T_c6ca4_row45_col7, #T_c6ca4_row48_col7, #T_c6ca4_row53_col7, #T_c6ca4_row58_col7, #T_c6ca4_row61_col7, #T_c6ca4_row68_col7, #T_c6ca4_row69_col7, #T_c6ca4_row71_col7, #T_c6ca4_row77_col7, #T_c6ca4_row85_col7, #T_c6ca4_row87_col7, #T_c6ca4_row93_col7, #T_c6ca4_row101_col7, #T_c6ca4_row107_col7, #T_c6ca4_row111_col7, #T_c6ca4_row123_col7, #T_c6ca4_row127_col7, #T_c6ca4_row130_col7, #T_c6ca4_row138_col7, #T_c6ca4_row139_col7, #T_c6ca4_row143_col7, #T_c6ca4_row155_col7, #T_c6ca4_row157_col7, #T_c6ca4_row162_col7, #T_c6ca4_row221_col7, #T_c6ca4_row241_col7, #T_c6ca4_row250_col7, #T_c6ca4_row304_col7, #T_c6ca4_row324_col7, #T_c6ca4_row356_col7, #T_c6ca4_row405_col7, #T_c6ca4_row471_col7, #T_c6ca4_row498_col7, #T_c6ca4_row505_col7, #T_c6ca4_row528_col7, #T_c6ca4_row535_col7 {
  background-color: #05713c;
  color: #f1f1f1;
}
#T_c6ca4_row40_col6, #T_c6ca4_row46_col6, #T_c6ca4_row94_col6, #T_c6ca4_row102_col6, #T_c6ca4_row126_col6, #T_c6ca4_row158_col6, #T_c6ca4_row430_col6, #T_c6ca4_row502_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 3.9%, transparent 3.9%);
}
#T_c6ca4_row41_col6, #T_c6ca4_row262_col6, #T_c6ca4_row297_col6, #T_c6ca4_row300_col6, #T_c6ca4_row313_col6, #T_c6ca4_row377_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 3.2%, transparent 3.2%);
}
#T_c6ca4_row42_col6, #T_c6ca4_row53_col6, #T_c6ca4_row71_col6, #T_c6ca4_row107_col6, #T_c6ca4_row141_col6, #T_c6ca4_row162_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 7.2%, transparent 7.2%);
}
#T_c6ca4_row44_col6, #T_c6ca4_row252_col6, #T_c6ca4_row364_col6, #T_c6ca4_row388_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 2.9%, transparent 2.9%);
}
#T_c6ca4_row48_col6, #T_c6ca4_row58_col6, #T_c6ca4_row93_col6, #T_c6ca4_row111_col6, #T_c6ca4_row138_col6, #T_c6ca4_row356_col6, #T_c6ca4_row513_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 7.3%, transparent 7.3%);
}
#T_c6ca4_row50_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 10.1%, transparent 10.1%);
}
#T_c6ca4_row50_col7, #T_c6ca4_row116_col7, #T_c6ca4_row213_col7, #T_c6ca4_row503_col7, #T_c6ca4_row511_col7 {
  background-color: #0a7b41;
  color: #f1f1f1;
}
#T_c6ca4_row52_col6, #T_c6ca4_row90_col6, #T_c6ca4_row103_col6, #T_c6ca4_row159_col6, #T_c6ca4_row372_col6, #T_c6ca4_row491_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 7.9%, transparent 7.9%);
}
#T_c6ca4_row52_col7, #T_c6ca4_row84_col7, #T_c6ca4_row90_col7, #T_c6ca4_row103_col7, #T_c6ca4_row106_col7, #T_c6ca4_row117_col7, #T_c6ca4_row119_col7, #T_c6ca4_row122_col7, #T_c6ca4_row132_col7, #T_c6ca4_row133_col7, #T_c6ca4_row146_col7, #T_c6ca4_row149_col7, #T_c6ca4_row151_col7, #T_c6ca4_row159_col7, #T_c6ca4_row165_col7, #T_c6ca4_row167_col7, #T_c6ca4_row171_col7, #T_c6ca4_row175_col7, #T_c6ca4_row210_col7, #T_c6ca4_row234_col7, #T_c6ca4_row269_col7, #T_c6ca4_row270_col7, #T_c6ca4_row273_col7, #T_c6ca4_row276_col7, #T_c6ca4_row288_col7, #T_c6ca4_row340_col7, #T_c6ca4_row354_col7, #T_c6ca4_row372_col7, #T_c6ca4_row457_col7, #T_c6ca4_row491_col7, #T_c6ca4_row506_col7, #T_c6ca4_row529_col7 {
  background-color: #06733d;
  color: #f1f1f1;
}
#T_c6ca4_row54_col6, #T_c6ca4_row154_col6, #T_c6ca4_row348_col6, #T_c6ca4_row380_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 3.1%, transparent 3.1%);
}
#T_c6ca4_row57_col6, #T_c6ca4_row99_col6, #T_c6ca4_row108_col6, #T_c6ca4_row201_col6, #T_c6ca4_row214_col6, #T_c6ca4_row220_col6, #T_c6ca4_row489_col6, #T_c6ca4_row530_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 2.7%, transparent 2.7%);
}
#T_c6ca4_row60_col6, #T_c6ca4_row316_col6, #T_c6ca4_row323_col6, #T_c6ca4_row332_col6, #T_c6ca4_row400_col6, #T_c6ca4_row412_col6, #T_c6ca4_row434_col6, #T_c6ca4_row469_col6, #T_c6ca4_row477_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 3.0%, transparent 3.0%);
}
#T_c6ca4_row62_col6, #T_c6ca4_row110_col6, #T_c6ca4_row174_col6, #T_c6ca4_row206_col6, #T_c6ca4_row328_col6, #T_c6ca4_row426_col6, #T_c6ca4_row527_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 3.8%, transparent 3.8%);
}
#T_c6ca4_row64_col6, #T_c6ca4_row161_col6, #T_c6ca4_row258_col6, #T_c6ca4_row385_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 6.2%, transparent 6.2%);
}
#T_c6ca4_row66_col6, #T_c6ca4_row74_col6, #T_c6ca4_row135_col6, #T_c6ca4_row210_col6, #T_c6ca4_row270_col6, #T_c6ca4_row471_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 8.4%, transparent 8.4%);
}
#T_c6ca4_row68_col6, #T_c6ca4_row69_col6, #T_c6ca4_row123_col6, #T_c6ca4_row324_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 7.5%, transparent 7.5%);
}
#T_c6ca4_row70_col6, #T_c6ca4_row78_col6, #T_c6ca4_row286_col6, #T_c6ca4_row291_col6, #T_c6ca4_row302_col6, #T_c6ca4_row345_col6, #T_c6ca4_row413_col6, #T_c6ca4_row464_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 3.7%, transparent 3.7%);
}
#T_c6ca4_row73_col6, #T_c6ca4_row115_col6, #T_c6ca4_row124_col6, #T_c6ca4_row140_col6, #T_c6ca4_row147_col6, #T_c6ca4_row163_col6, #T_c6ca4_row358_col6, #T_c6ca4_row411_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 2.5%, transparent 2.5%);
}
#T_c6ca4_row79_col6, #T_c6ca4_row109_col6, #T_c6ca4_row381_col6, #T_c6ca4_row466_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 7.0%, transparent 7.0%);
}
#T_c6ca4_row80_col6, #T_c6ca4_row242_col6, #T_c6ca4_row289_col6, #T_c6ca4_row337_col6, #T_c6ca4_row351_col6, #T_c6ca4_row353_col6, #T_c6ca4_row379_col6, #T_c6ca4_row539_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 5.9%, transparent 5.9%);
}
#T_c6ca4_row82_col6, #T_c6ca4_row187_col6, #T_c6ca4_row370_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 9.1%, transparent 9.1%);
}
#T_c6ca4_row82_col7, #T_c6ca4_row181_col7, #T_c6ca4_row183_col7, #T_c6ca4_row187_col7, #T_c6ca4_row191_col7, #T_c6ca4_row215_col7, #T_c6ca4_row320_col7, #T_c6ca4_row370_col7, #T_c6ca4_row521_col7 {
  background-color: #08773f;
  color: #f1f1f1;
}
#T_c6ca4_row84_col6, #T_c6ca4_row101_col6, #T_c6ca4_row146_col6, #T_c6ca4_row221_col6, #T_c6ca4_row453_col6, #T_c6ca4_row529_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 7.8%, transparent 7.8%);
}
#T_c6ca4_row86_col6, #T_c6ca4_row142_col6, #T_c6ca4_row224_col6, #T_c6ca4_row360_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 4.1%, transparent 4.1%);
}
#T_c6ca4_row87_col6, #T_c6ca4_row157_col6, #T_c6ca4_row498_col6, #T_c6ca4_row535_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 7.4%, transparent 7.4%);
}
#T_c6ca4_row89_col6, #T_c6ca4_row105_col6, #T_c6ca4_row121_col6, #T_c6ca4_row131_col6, #T_c6ca4_row156_col6, #T_c6ca4_row233_col6, #T_c6ca4_row294_col6, #T_c6ca4_row339_col6, #T_c6ca4_row342_col6, #T_c6ca4_row387_col6, #T_c6ca4_row517_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 2.3%, transparent 2.3%);
}
#T_c6ca4_row91_col6, #T_c6ca4_row178_col6, #T_c6ca4_row487_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 6.9%, transparent 6.9%);
}
#T_c6ca4_row95_col6, #T_c6ca4_row125_col6, #T_c6ca4_row173_col6, #T_c6ca4_row189_col6, #T_c6ca4_row292_col6, #T_c6ca4_row308_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 7.1%, transparent 7.1%);
}
#T_c6ca4_row96_col6, #T_c6ca4_row194_col6, #T_c6ca4_row212_col6, #T_c6ca4_row228_col6, #T_c6ca4_row293_col6, #T_c6ca4_row303_col6, #T_c6ca4_row319_col6, #T_c6ca4_row327_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 5.5%, transparent 5.5%);
}
#T_c6ca4_row98_col6, #T_c6ca4_row263_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 9.9%, transparent 9.9%);
}
#T_c6ca4_row98_col7, #T_c6ca4_row100_col7, #T_c6ca4_row196_col7, #T_c6ca4_row208_col7, #T_c6ca4_row231_col7, #T_c6ca4_row247_col7, #T_c6ca4_row253_col7, #T_c6ca4_row263_col7, #T_c6ca4_row274_col7, #T_c6ca4_row386_col7, #T_c6ca4_row500_col7, #T_c6ca4_row501_col7 {
  background-color: #097940;
  color: #f1f1f1;
}
#T_c6ca4_row100_col6, #T_c6ca4_row231_col6, #T_c6ca4_row438_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 9.8%, transparent 9.8%);
}
#T_c6ca4_row104_col6, #T_c6ca4_row200_col6, #T_c6ca4_row202_col6, #T_c6ca4_row211_col6, #T_c6ca4_row280_col6, #T_c6ca4_row312_col6, #T_c6ca4_row318_col6, #T_c6ca4_row361_col6, #T_c6ca4_row376_col6, #T_c6ca4_row382_col6, #T_c6ca4_row407_col6, #T_c6ca4_row435_col6, #T_c6ca4_row443_col6, #T_c6ca4_row450_col6, #T_c6ca4_row522_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 3.5%, transparent 3.5%);
}
#T_c6ca4_row106_col6, #T_c6ca4_row119_col6, #T_c6ca4_row171_col6, #T_c6ca4_row175_col6, #T_c6ca4_row269_col6, #T_c6ca4_row288_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 8.2%, transparent 8.2%);
}
#T_c6ca4_row112_col6, #T_c6ca4_row277_col6, #T_c6ca4_row299_col6, #T_c6ca4_row346_col6, #T_c6ca4_row423_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 5.4%, transparent 5.4%);
}
#T_c6ca4_row113_col6, #T_c6ca4_row177_col6, #T_c6ca4_row180_col6, #T_c6ca4_row193_col6, #T_c6ca4_row341_col6, #T_c6ca4_row359_col6, #T_c6ca4_row390_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 6.1%, transparent 6.1%);
}
#T_c6ca4_row116_col6, #T_c6ca4_row208_col6, #T_c6ca4_row213_col6, #T_c6ca4_row274_col6, #T_c6ca4_row472_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 10.2%, transparent 10.2%);
}
#T_c6ca4_row117_col6, #T_c6ca4_row122_col6, #T_c6ca4_row149_col6, #T_c6ca4_row340_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 8.0%, transparent 8.0%);
}
#T_c6ca4_row120_col6, #T_c6ca4_row418_col6, #T_c6ca4_row442_col6, #T_c6ca4_row518_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 4.0%, transparent 4.0%);
}
#T_c6ca4_row127_col6, #T_c6ca4_row139_col6, #T_c6ca4_row155_col6, #T_c6ca4_row250_col6, #T_c6ca4_row528_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 7.6%, transparent 7.6%);
}
#T_c6ca4_row132_col6, #T_c6ca4_row133_col6, #T_c6ca4_row151_col6, #T_c6ca4_row354_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 8.1%, transparent 8.1%);
}
#T_c6ca4_row134_col6, #T_c6ca4_row150_col6, #T_c6ca4_row166_col6, #T_c6ca4_row195_col6, #T_c6ca4_row198_col6, #T_c6ca4_row227_col6, #T_c6ca4_row278_col6, #T_c6ca4_row326_col6, #T_c6ca4_row420_col6, #T_c6ca4_row439_col6, #T_c6ca4_row534_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 2.1%, transparent 2.1%);
}
#T_c6ca4_row137_col6, #T_c6ca4_row217_col6, #T_c6ca4_row463_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 1.7%, transparent 1.7%);
}
#T_c6ca4_row144_col6, #T_c6ca4_row199_col6, #T_c6ca4_row264_col6, #T_c6ca4_row279_col6, #T_c6ca4_row295_col6, #T_c6ca4_row523_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 5.0%, transparent 5.0%);
}
#T_c6ca4_row152_col6, #T_c6ca4_row168_col6, #T_c6ca4_row355_col6, #T_c6ca4_row449_col6, #T_c6ca4_row462_col6, #T_c6ca4_row473_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 1.9%, transparent 1.9%);
}
#T_c6ca4_row153_col6, #T_c6ca4_row185_col6, #T_c6ca4_row265_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 1.3%, transparent 1.3%);
}
#T_c6ca4_row160_col6, #T_c6ca4_row176_col6, #T_c6ca4_row232_col6, #T_c6ca4_row440_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 4.4%, transparent 4.4%);
}
#T_c6ca4_row165_col6, #T_c6ca4_row167_col6, #T_c6ca4_row273_col6, #T_c6ca4_row419_col6, #T_c6ca4_row505_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 8.3%, transparent 8.3%);
}
#T_c6ca4_row169_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 1.4%, transparent 1.4%);
}
#T_c6ca4_row172_col6, #T_c6ca4_row216_col6, #T_c6ca4_row310_col6, #T_c6ca4_row415_col6, #T_c6ca4_row428_col6, #T_c6ca4_row482_col6, #T_c6ca4_row488_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 2.2%, transparent 2.2%);
}
#T_c6ca4_row183_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 9.0%, transparent 9.0%);
}
#T_c6ca4_row196_col6, #T_c6ca4_row290_col6, #T_c6ca4_row322_col6, #T_c6ca4_row338_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 10.9%, transparent 10.9%);
}
#T_c6ca4_row197_col6, #T_c6ca4_row275_col6, #T_c6ca4_row287_col6, #T_c6ca4_row315_col6, #T_c6ca4_row410_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 5.3%, transparent 5.3%);
}
#T_c6ca4_row215_col6, #T_c6ca4_row386_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 9.5%, transparent 9.5%);
}
#T_c6ca4_row218_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 16.4%, transparent 16.4%);
}
#T_c6ca4_row218_col7 {
  background-color: #148e4b;
  color: #f1f1f1;
}
#T_c6ca4_row219_col6, #T_c6ca4_row223_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 10.7%, transparent 10.7%);
}
#T_c6ca4_row219_col7, #T_c6ca4_row223_col7, #T_c6ca4_row290_col7, #T_c6ca4_row322_col7, #T_c6ca4_row338_col7 {
  background-color: #0b7d42;
  color: #f1f1f1;
}
#T_c6ca4_row229_col6, #T_c6ca4_row306_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 11.0%, transparent 11.0%);
}
#T_c6ca4_row229_col7, #T_c6ca4_row306_col7, #T_c6ca4_row402_col7, #T_c6ca4_row520_col7 {
  background-color: #0c7f43;
  color: #f1f1f1;
}
#T_c6ca4_row234_col6, #T_c6ca4_row457_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 8.5%, transparent 8.5%);
}
#T_c6ca4_row235_col6, #T_c6ca4_row245_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 11.5%, transparent 11.5%);
}
#T_c6ca4_row235_col7, #T_c6ca4_row239_col7, #T_c6ca4_row245_col7 {
  background-color: #0d8044;
  color: #f1f1f1;
}
#T_c6ca4_row237_col6, #T_c6ca4_row394_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 8.6%, transparent 8.6%);
}
#T_c6ca4_row239_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 11.6%, transparent 11.6%);
}
#T_c6ca4_row243_col6, #T_c6ca4_row248_col6, #T_c6ca4_row362_col6, #T_c6ca4_row425_col6, #T_c6ca4_row459_col6, #T_c6ca4_row515_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 2.0%, transparent 2.0%);
}
#T_c6ca4_row247_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 10.0%, transparent 10.0%);
}
#T_c6ca4_row249_col6, #T_c6ca4_row389_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 1.2%, transparent 1.2%);
}
#T_c6ca4_row251_col6, #T_c6ca4_row255_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 14.6%, transparent 14.6%);
}
#T_c6ca4_row251_col7, #T_c6ca4_row255_col7 {
  background-color: #15904c;
  color: #f1f1f1;
}
#T_c6ca4_row253_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 9.7%, transparent 9.7%);
}
#T_c6ca4_row256_col6, #T_c6ca4_row268_col6, #T_c6ca4_row284_col6, #T_c6ca4_row329_col6, #T_c6ca4_row447_col6, #T_c6ca4_row484_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 3.3%, transparent 3.3%);
}
#T_c6ca4_row259_col6, #T_c6ca4_row276_col6, #T_c6ca4_row320_col6, #T_c6ca4_row458_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 9.4%, transparent 9.4%);
}
#T_c6ca4_row261_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 13.4%, transparent 13.4%);
}
#T_c6ca4_row261_col7 {
  background-color: #118848;
  color: #f1f1f1;
}
#T_c6ca4_row266_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 12.9%, transparent 12.9%);
}
#T_c6ca4_row267_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 16.6%, transparent 16.6%);
}
#T_c6ca4_row267_col7 {
  background-color: #219c52;
  color: #f1f1f1;
}
#T_c6ca4_row271_col6, #T_c6ca4_row503_col6, #T_c6ca4_row511_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 16.2%, transparent 16.2%);
}
#T_c6ca4_row271_col7 {
  background-color: #1b9950;
  color: #f1f1f1;
}
#T_c6ca4_row298_col6, #T_c6ca4_row531_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 4.5%, transparent 4.5%);
}
#T_c6ca4_row309_col6, #T_c6ca4_row384_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 5.7%, transparent 5.7%);
}
#T_c6ca4_row314_col6, #T_c6ca4_row441_col6, #T_c6ca4_row536_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 4.6%, transparent 4.6%);
}
#T_c6ca4_row325_col6, #T_c6ca4_row343_col6, #T_c6ca4_row347_col6, #T_c6ca4_row367_col6, #T_c6ca4_row399_col6, #T_c6ca4_row494_col6, #T_c6ca4_row495_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 5.8%, transparent 5.8%);
}
#T_c6ca4_row352_col6, #T_c6ca4_row357_col6, #T_c6ca4_row445_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 6.4%, transparent 6.4%);
}
#T_c6ca4_row391_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 58.1%, transparent 58.1%);
}
#T_c6ca4_row391_col7 {
  background-color: #d1ec86;
  color: #000000;
}
#T_c6ca4_row392_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 31.3%, transparent 31.3%);
}
#T_c6ca4_row392_col7 {
  background-color: #6bbf64;
  color: #000000;
}
#T_c6ca4_row393_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 91.4%, transparent 91.4%);
}
#T_c6ca4_row397_col6, #T_c6ca4_row403_col6, #T_c6ca4_row408_col6, #T_c6ca4_row417_col6, #T_c6ca4_row422_col6, #T_c6ca4_row431_col6, #T_c6ca4_row436_col6, #T_c6ca4_row444_col6, #T_c6ca4_row451_col6, #T_c6ca4_row456_col6, #T_c6ca4_row465_col6, #T_c6ca4_row470_col6, #T_c6ca4_row478_col6, #T_c6ca4_row485_col6, #T_c6ca4_row490_col6, #T_c6ca4_row499_col6, #T_c6ca4_row504_col6, #T_c6ca4_row512_col6, #T_c6ca4_row519_col6, #T_c6ca4_row524_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 56.4%, transparent 56.4%);
}
#T_c6ca4_row398_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 35.0%, transparent 35.0%);
}
#T_c6ca4_row398_col7 {
  background-color: #d3ec87;
  color: #000000;
}
#T_c6ca4_row401_col6, #T_c6ca4_row497_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 1.5%, transparent 1.5%);
}
#T_c6ca4_row402_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 11.3%, transparent 11.3%);
}
#T_c6ca4_row405_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 10.3%, transparent 10.3%);
}
#T_c6ca4_row433_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 9.2%, transparent 9.2%);
}
#T_c6ca4_row446_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 10.4%, transparent 10.4%);
}
#T_c6ca4_row480_col6, #T_c6ca4_row500_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 9.6%, transparent 9.6%);
}
#T_c6ca4_row483_col6, #T_c6ca4_row496_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 1.6%, transparent 1.6%);
}
#T_c6ca4_row492_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 11.4%, transparent 11.4%);
}
#T_c6ca4_row501_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 15.9%, transparent 15.9%);
}
#T_c6ca4_row506_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 23.6%, transparent 23.6%);
}
#T_c6ca4_row514_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 22.2%, transparent 22.2%);
}
#T_c6ca4_row520_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 22.9%, transparent 22.9%);
}
#T_c6ca4_row521_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 31.8%, transparent 31.8%);
}
#T_c6ca4_row526_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 26.5%, transparent 26.5%);
}
#T_c6ca4_row537_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 100.0%, transparent 100.0%);
}
#T_c6ca4_row537_col7 {
  background-color: #a50026;
  color: #f1f1f1;
}
</style>
<table id="T_c6ca4">
  <thead>
    <tr>
      <th class="blank level0" >&nbsp;</th>
      <th id="T_c6ca4_level0_col0" class="col_heading level0 col0" >name</th>
      <th id="T_c6ca4_level0_col1" class="col_heading level0 col1" >model 1 (large_vox_iter5.pt)</th>
      <th id="T_c6ca4_level0_col2" class="col_heading level0 col2" >model 2 (checkpoint_best.pt)</th>
      <th id="T_c6ca4_level0_col3" class="col_heading level0 col3" >delta_l2</th>
      <th id="T_c6ca4_level0_col4" class="col_heading level0 col4" >baseline_l2</th>
      <th id="T_c6ca4_level0_col5" class="col_heading level0 col5" >updated_l2</th>
      <th id="T_c6ca4_level0_col6" class="col_heading level0 col6" >relative_delta</th>
      <th id="T_c6ca4_level0_col7" class="col_heading level0 col7" >cosine_similarity</th>
      <th id="T_c6ca4_level0_col8" class="col_heading level0 col8" >max_abs_delta</th>
      <th id="T_c6ca4_level0_col9" class="col_heading level0 col9" >numel</th>
      <th id="T_c6ca4_level0_col10" class="col_heading level0 col10" >dtype</th>
      <th id="T_c6ca4_level0_col11" class="col_heading level0 col11" >shape</th>
      <th id="T_c6ca4_level0_col12" class="col_heading level0 col12" >message</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th id="T_c6ca4_level0_row0" class="row_heading level0 row0" >0</th>
      <td id="T_c6ca4_row0_col0" class="data row0 col0" >ctc_proj.bias</td>
      <td id="T_c6ca4_row0_col1" class="data row0 col1" >nan</td>
      <td id="T_c6ca4_row0_col2" class="data row0 col2" >ctc_proj.bias</td>
      <td id="T_c6ca4_row0_col3" class="data row0 col3" ></td>
      <td id="T_c6ca4_row0_col4" class="data row0 col4" ></td>
      <td id="T_c6ca4_row0_col5" class="data row0 col5" ></td>
      <td id="T_c6ca4_row0_col6" class="data row0 col6" ></td>
      <td id="T_c6ca4_row0_col7" class="data row0 col7" ></td>
      <td id="T_c6ca4_row0_col8" class="data row0 col8" ></td>
      <td id="T_c6ca4_row0_col9" class="data row0 col9" >nan</td>
      <td id="T_c6ca4_row0_col10" class="data row0 col10" >torch.float32</td>
      <td id="T_c6ca4_row0_col11" class="data row0 col11" >(32,)</td>
      <td id="T_c6ca4_row0_col12" class="data row0 col12" >missing from model 1</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row1" class="row_heading level0 row1" >1</th>
      <td id="T_c6ca4_row1_col0" class="data row1 col0" >ctc_proj.weight</td>
      <td id="T_c6ca4_row1_col1" class="data row1 col1" >nan</td>
      <td id="T_c6ca4_row1_col2" class="data row1 col2" >ctc_proj.weight</td>
      <td id="T_c6ca4_row1_col3" class="data row1 col3" ></td>
      <td id="T_c6ca4_row1_col4" class="data row1 col4" ></td>
      <td id="T_c6ca4_row1_col5" class="data row1 col5" ></td>
      <td id="T_c6ca4_row1_col6" class="data row1 col6" ></td>
      <td id="T_c6ca4_row1_col7" class="data row1 col7" ></td>
      <td id="T_c6ca4_row1_col8" class="data row1 col8" ></td>
      <td id="T_c6ca4_row1_col9" class="data row1 col9" >nan</td>
      <td id="T_c6ca4_row1_col10" class="data row1 col10" >torch.float32</td>
      <td id="T_c6ca4_row1_col11" class="data row1 col11" >(32, 1024)</td>
      <td id="T_c6ca4_row1_col12" class="data row1 col12" >missing from model 1</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row2" class="row_heading level0 row2" >2</th>
      <td id="T_c6ca4_row2_col0" class="data row2 col0" >encoder.layer_norm.bias</td>
      <td id="T_c6ca4_row2_col1" class="data row2 col1" >encoder.layer_norm.bias</td>
      <td id="T_c6ca4_row2_col2" class="data row2 col2" >av_romanizer.w2v_model.encoder.layer_norm.bias</td>
      <td id="T_c6ca4_row2_col3" class="data row2 col3" >1.6697</td>
      <td id="T_c6ca4_row2_col4" class="data row2 col4" >1.1872</td>
      <td id="T_c6ca4_row2_col5" class="data row2 col5" >2.13569</td>
      <td id="T_c6ca4_row2_col6" class="data row2 col6" >1.40642</td>
      <td id="T_c6ca4_row2_col7" class="data row2 col7" >0.627631</td>
      <td id="T_c6ca4_row2_col8" class="data row2 col8" >0.112366</td>
      <td id="T_c6ca4_row2_col9" class="data row2 col9" >1024.000000</td>
      <td id="T_c6ca4_row2_col10" class="data row2 col10" >torch.float32</td>
      <td id="T_c6ca4_row2_col11" class="data row2 col11" >(1024,)</td>
      <td id="T_c6ca4_row2_col12" class="data row2 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row3" class="row_heading level0 row3" >3</th>
      <td id="T_c6ca4_row3_col0" class="data row3 col0" >encoder.layer_norm.weight</td>
      <td id="T_c6ca4_row3_col1" class="data row3 col1" >encoder.layer_norm.weight</td>
      <td id="T_c6ca4_row3_col2" class="data row3 col2" >av_romanizer.w2v_model.encoder.layer_norm.weight</td>
      <td id="T_c6ca4_row3_col3" class="data row3 col3" >4.4596</td>
      <td id="T_c6ca4_row3_col4" class="data row3 col4" >8.6733</td>
      <td id="T_c6ca4_row3_col5" class="data row3 col5" >13.0296</td>
      <td id="T_c6ca4_row3_col6" class="data row3 col6" >0.514176</td>
      <td id="T_c6ca4_row3_col7" class="data row3 col7" >0.995971</td>
      <td id="T_c6ca4_row3_col8" class="data row3 col8" >0.250244</td>
      <td id="T_c6ca4_row3_col9" class="data row3 col9" >1024.000000</td>
      <td id="T_c6ca4_row3_col10" class="data row3 col10" >torch.float32</td>
      <td id="T_c6ca4_row3_col11" class="data row3 col11" >(1024,)</td>
      <td id="T_c6ca4_row3_col12" class="data row3 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row4" class="row_heading level0 row4" >4</th>
      <td id="T_c6ca4_row4_col0" class="data row4 col0" >encoder.layers.0.fc1.bias</td>
      <td id="T_c6ca4_row4_col1" class="data row4 col1" >encoder.layers.0.fc1.bias</td>
      <td id="T_c6ca4_row4_col2" class="data row4 col2" >av_romanizer.w2v_model.encoder.layers.0.fc1.bias</td>
      <td id="T_c6ca4_row4_col3" class="data row4 col3" >0.896795</td>
      <td id="T_c6ca4_row4_col4" class="data row4 col4" >3.95813</td>
      <td id="T_c6ca4_row4_col5" class="data row4 col5" >3.98692</td>
      <td id="T_c6ca4_row4_col6" class="data row4 col6" >0.22657</td>
      <td id="T_c6ca4_row4_col7" class="data row4 col7" >0.974545</td>
      <td id="T_c6ca4_row4_col8" class="data row4 col8" >0.0740356</td>
      <td id="T_c6ca4_row4_col9" class="data row4 col9" >4096.000000</td>
      <td id="T_c6ca4_row4_col10" class="data row4 col10" >torch.float32</td>
      <td id="T_c6ca4_row4_col11" class="data row4 col11" >(4096,)</td>
      <td id="T_c6ca4_row4_col12" class="data row4 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row5" class="row_heading level0 row5" >5</th>
      <td id="T_c6ca4_row5_col0" class="data row5 col0" >encoder.layers.0.fc1.weight</td>
      <td id="T_c6ca4_row5_col1" class="data row5 col1" >encoder.layers.0.fc1.weight</td>
      <td id="T_c6ca4_row5_col2" class="data row5 col2" >av_romanizer.w2v_model.encoder.layers.0.fc1.weight</td>
      <td id="T_c6ca4_row5_col3" class="data row5 col3" >38.1125</td>
      <td id="T_c6ca4_row5_col4" class="data row5 col4" >288.43</td>
      <td id="T_c6ca4_row5_col5" class="data row5 col5" >276.581</td>
      <td id="T_c6ca4_row5_col6" class="data row5 col6" >0.132138</td>
      <td id="T_c6ca4_row5_col7" class="data row5 col7" >0.991776</td>
      <td id="T_c6ca4_row5_col8" class="data row5 col8" >0.257812</td>
      <td id="T_c6ca4_row5_col9" class="data row5 col9" >4194304.000000</td>
      <td id="T_c6ca4_row5_col10" class="data row5 col10" >torch.float32</td>
      <td id="T_c6ca4_row5_col11" class="data row5 col11" >(4096, 1024)</td>
      <td id="T_c6ca4_row5_col12" class="data row5 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row6" class="row_heading level0 row6" >6</th>
      <td id="T_c6ca4_row6_col0" class="data row6 col0" >encoder.layers.0.fc2.bias</td>
      <td id="T_c6ca4_row6_col1" class="data row6 col1" >encoder.layers.0.fc2.bias</td>
      <td id="T_c6ca4_row6_col2" class="data row6 col2" >av_romanizer.w2v_model.encoder.layers.0.fc2.bias</td>
      <td id="T_c6ca4_row6_col3" class="data row6 col3" >0.140059</td>
      <td id="T_c6ca4_row6_col4" class="data row6 col4" >3.05563</td>
      <td id="T_c6ca4_row6_col5" class="data row6 col5" >2.9435</td>
      <td id="T_c6ca4_row6_col6" class="data row6 col6" >0.0458362</td>
      <td id="T_c6ca4_row6_col7" class="data row6 col7" >0.999608</td>
      <td id="T_c6ca4_row6_col8" class="data row6 col8" >0.0488281</td>
      <td id="T_c6ca4_row6_col9" class="data row6 col9" >1024.000000</td>
      <td id="T_c6ca4_row6_col10" class="data row6 col10" >torch.float32</td>
      <td id="T_c6ca4_row6_col11" class="data row6 col11" >(1024,)</td>
      <td id="T_c6ca4_row6_col12" class="data row6 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row7" class="row_heading level0 row7" >7</th>
      <td id="T_c6ca4_row7_col0" class="data row7 col0" >encoder.layers.0.fc2.weight</td>
      <td id="T_c6ca4_row7_col1" class="data row7 col1" >encoder.layers.0.fc2.weight</td>
      <td id="T_c6ca4_row7_col2" class="data row7 col2" >av_romanizer.w2v_model.encoder.layers.0.fc2.weight</td>
      <td id="T_c6ca4_row7_col3" class="data row7 col3" >31.1079</td>
      <td id="T_c6ca4_row7_col4" class="data row7 col4" >253.201</td>
      <td id="T_c6ca4_row7_col5" class="data row7 col5" >239.969</td>
      <td id="T_c6ca4_row7_col6" class="data row7 col6" >0.122859</td>
      <td id="T_c6ca4_row7_col7" class="data row7 col7" >0.993477</td>
      <td id="T_c6ca4_row7_col8" class="data row7 col8" >0.410156</td>
      <td id="T_c6ca4_row7_col9" class="data row7 col9" >4194304.000000</td>
      <td id="T_c6ca4_row7_col10" class="data row7 col10" >torch.float32</td>
      <td id="T_c6ca4_row7_col11" class="data row7 col11" >(1024, 4096)</td>
      <td id="T_c6ca4_row7_col12" class="data row7 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row8" class="row_heading level0 row8" >8</th>
      <td id="T_c6ca4_row8_col0" class="data row8 col0" >encoder.layers.0.final_layer_norm.bias</td>
      <td id="T_c6ca4_row8_col1" class="data row8 col1" >encoder.layers.0.final_layer_norm.bias</td>
      <td id="T_c6ca4_row8_col2" class="data row8 col2" >av_romanizer.w2v_model.encoder.layers.0.final_layer_norm.bias</td>
      <td id="T_c6ca4_row8_col3" class="data row8 col3" >0.284702</td>
      <td id="T_c6ca4_row8_col4" class="data row8 col4" >1.96778</td>
      <td id="T_c6ca4_row8_col5" class="data row8 col5" >1.96495</td>
      <td id="T_c6ca4_row8_col6" class="data row8 col6" >0.144682</td>
      <td id="T_c6ca4_row8_col7" class="data row8 col7" >0.98952</td>
      <td id="T_c6ca4_row8_col8" class="data row8 col8" >0.0865479</td>
      <td id="T_c6ca4_row8_col9" class="data row8 col9" >1024.000000</td>
      <td id="T_c6ca4_row8_col10" class="data row8 col10" >torch.float32</td>
      <td id="T_c6ca4_row8_col11" class="data row8 col11" >(1024,)</td>
      <td id="T_c6ca4_row8_col12" class="data row8 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row9" class="row_heading level0 row9" >9</th>
      <td id="T_c6ca4_row9_col0" class="data row9 col0" >encoder.layers.0.final_layer_norm.weight</td>
      <td id="T_c6ca4_row9_col1" class="data row9 col1" >encoder.layers.0.final_layer_norm.weight</td>
      <td id="T_c6ca4_row9_col2" class="data row9 col2" >av_romanizer.w2v_model.encoder.layers.0.final_layer_norm.weight</td>
      <td id="T_c6ca4_row9_col3" class="data row9 col3" >0.45082</td>
      <td id="T_c6ca4_row9_col4" class="data row9 col4" >4.11893</td>
      <td id="T_c6ca4_row9_col5" class="data row9 col5" >4.14468</td>
      <td id="T_c6ca4_row9_col6" class="data row9 col6" >0.109451</td>
      <td id="T_c6ca4_row9_col7" class="data row9 col7" >0.994067</td>
      <td id="T_c6ca4_row9_col8" class="data row9 col8" >0.280029</td>
      <td id="T_c6ca4_row9_col9" class="data row9 col9" >1024.000000</td>
      <td id="T_c6ca4_row9_col10" class="data row9 col10" >torch.float32</td>
      <td id="T_c6ca4_row9_col11" class="data row9 col11" >(1024,)</td>
      <td id="T_c6ca4_row9_col12" class="data row9 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row10" class="row_heading level0 row10" >10</th>
      <td id="T_c6ca4_row10_col0" class="data row10 col0" >encoder.layers.0.self_attn.k_proj.bias</td>
      <td id="T_c6ca4_row10_col1" class="data row10 col1" >encoder.layers.0.self_attn.k_proj.bias</td>
      <td id="T_c6ca4_row10_col2" class="data row10 col2" >av_romanizer.w2v_model.encoder.layers.0.self_attn.k_proj.bias</td>
      <td id="T_c6ca4_row10_col3" class="data row10 col3" >0.00413765</td>
      <td id="T_c6ca4_row10_col4" class="data row10 col4" >0.0605432</td>
      <td id="T_c6ca4_row10_col5" class="data row10 col5" >0.0574271</td>
      <td id="T_c6ca4_row10_col6" class="data row10 col6" >0.0683422</td>
      <td id="T_c6ca4_row10_col7" class="data row10 col7" >0.998934</td>
      <td id="T_c6ca4_row10_col8" class="data row10 col8" >0.000740051</td>
      <td id="T_c6ca4_row10_col9" class="data row10 col9" >1024.000000</td>
      <td id="T_c6ca4_row10_col10" class="data row10 col10" >torch.float32</td>
      <td id="T_c6ca4_row10_col11" class="data row10 col11" >(1024,)</td>
      <td id="T_c6ca4_row10_col12" class="data row10 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row11" class="row_heading level0 row11" >11</th>
      <td id="T_c6ca4_row11_col0" class="data row11 col0" >encoder.layers.0.self_attn.k_proj.weight</td>
      <td id="T_c6ca4_row11_col1" class="data row11 col1" >encoder.layers.0.self_attn.k_proj.weight</td>
      <td id="T_c6ca4_row11_col2" class="data row11 col2" >av_romanizer.w2v_model.encoder.layers.0.self_attn.k_proj.weight</td>
      <td id="T_c6ca4_row11_col3" class="data row11 col3" >14.4639</td>
      <td id="T_c6ca4_row11_col4" class="data row11 col4" >128.993</td>
      <td id="T_c6ca4_row11_col5" class="data row11 col5" >124.914</td>
      <td id="T_c6ca4_row11_col6" class="data row11 col6" >0.112129</td>
      <td id="T_c6ca4_row11_col7" class="data row11 col7" >0.994025</td>
      <td id="T_c6ca4_row11_col8" class="data row11 col8" >0.155754</td>
      <td id="T_c6ca4_row11_col9" class="data row11 col9" >1048576.000000</td>
      <td id="T_c6ca4_row11_col10" class="data row11 col10" >torch.float32</td>
      <td id="T_c6ca4_row11_col11" class="data row11 col11" >(1024, 1024)</td>
      <td id="T_c6ca4_row11_col12" class="data row11 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row12" class="row_heading level0 row12" >12</th>
      <td id="T_c6ca4_row12_col0" class="data row12 col0" >encoder.layers.0.self_attn.out_proj.bias</td>
      <td id="T_c6ca4_row12_col1" class="data row12 col1" >encoder.layers.0.self_attn.out_proj.bias</td>
      <td id="T_c6ca4_row12_col2" class="data row12 col2" >av_romanizer.w2v_model.encoder.layers.0.self_attn.out_proj.bias</td>
      <td id="T_c6ca4_row12_col3" class="data row12 col3" >0.396173</td>
      <td id="T_c6ca4_row12_col4" class="data row12 col4" >3.55277</td>
      <td id="T_c6ca4_row12_col5" class="data row12 col5" >3.28497</td>
      <td id="T_c6ca4_row12_col6" class="data row12 col6" >0.111511</td>
      <td id="T_c6ca4_row12_col7" class="data row12 col7" >0.996348</td>
      <td id="T_c6ca4_row12_col8" class="data row12 col8" >0.167969</td>
      <td id="T_c6ca4_row12_col9" class="data row12 col9" >1024.000000</td>
      <td id="T_c6ca4_row12_col10" class="data row12 col10" >torch.float32</td>
      <td id="T_c6ca4_row12_col11" class="data row12 col11" >(1024,)</td>
      <td id="T_c6ca4_row12_col12" class="data row12 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row13" class="row_heading level0 row13" >13</th>
      <td id="T_c6ca4_row13_col0" class="data row13 col0" >encoder.layers.0.self_attn.out_proj.weight</td>
      <td id="T_c6ca4_row13_col1" class="data row13 col1" >encoder.layers.0.self_attn.out_proj.weight</td>
      <td id="T_c6ca4_row13_col2" class="data row13 col2" >av_romanizer.w2v_model.encoder.layers.0.self_attn.out_proj.weight</td>
      <td id="T_c6ca4_row13_col3" class="data row13 col3" >18.3227</td>
      <td id="T_c6ca4_row13_col4" class="data row13 col4" >106.257</td>
      <td id="T_c6ca4_row13_col5" class="data row13 col5" >100.582</td>
      <td id="T_c6ca4_row13_col6" class="data row13 col6" >0.172438</td>
      <td id="T_c6ca4_row13_col7" class="data row13 col7" >0.9858</td>
      <td id="T_c6ca4_row13_col8" class="data row13 col8" >0.320312</td>
      <td id="T_c6ca4_row13_col9" class="data row13 col9" >1048576.000000</td>
      <td id="T_c6ca4_row13_col10" class="data row13 col10" >torch.float32</td>
      <td id="T_c6ca4_row13_col11" class="data row13 col11" >(1024, 1024)</td>
      <td id="T_c6ca4_row13_col12" class="data row13 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row14" class="row_heading level0 row14" >14</th>
      <td id="T_c6ca4_row14_col0" class="data row14 col0" >encoder.layers.0.self_attn.q_proj.bias</td>
      <td id="T_c6ca4_row14_col1" class="data row14 col1" >encoder.layers.0.self_attn.q_proj.bias</td>
      <td id="T_c6ca4_row14_col2" class="data row14 col2" >av_romanizer.w2v_model.encoder.layers.0.self_attn.q_proj.bias</td>
      <td id="T_c6ca4_row14_col3" class="data row14 col3" >1.0616</td>
      <td id="T_c6ca4_row14_col4" class="data row14 col4" >9.79107</td>
      <td id="T_c6ca4_row14_col5" class="data row14 col5" >9.6196</td>
      <td id="T_c6ca4_row14_col6" class="data row14 col6" >0.108425</td>
      <td id="T_c6ca4_row14_col7" class="data row14 col7" >0.994173</td>
      <td id="T_c6ca4_row14_col8" class="data row14 col8" >0.115234</td>
      <td id="T_c6ca4_row14_col9" class="data row14 col9" >1024.000000</td>
      <td id="T_c6ca4_row14_col10" class="data row14 col10" >torch.float32</td>
      <td id="T_c6ca4_row14_col11" class="data row14 col11" >(1024,)</td>
      <td id="T_c6ca4_row14_col12" class="data row14 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row15" class="row_heading level0 row15" >15</th>
      <td id="T_c6ca4_row15_col0" class="data row15 col0" >encoder.layers.0.self_attn.q_proj.weight</td>
      <td id="T_c6ca4_row15_col1" class="data row15 col1" >encoder.layers.0.self_attn.q_proj.weight</td>
      <td id="T_c6ca4_row15_col2" class="data row15 col2" >av_romanizer.w2v_model.encoder.layers.0.self_attn.q_proj.weight</td>
      <td id="T_c6ca4_row15_col3" class="data row15 col3" >13.9993</td>
      <td id="T_c6ca4_row15_col4" class="data row15 col4" >128.382</td>
      <td id="T_c6ca4_row15_col5" class="data row15 col5" >123.863</td>
      <td id="T_c6ca4_row15_col6" class="data row15 col6" >0.109044</td>
      <td id="T_c6ca4_row15_col7" class="data row15 col7" >0.99448</td>
      <td id="T_c6ca4_row15_col8" class="data row15 col8" >0.130371</td>
      <td id="T_c6ca4_row15_col9" class="data row15 col9" >1048576.000000</td>
      <td id="T_c6ca4_row15_col10" class="data row15 col10" >torch.float32</td>
      <td id="T_c6ca4_row15_col11" class="data row15 col11" >(1024, 1024)</td>
      <td id="T_c6ca4_row15_col12" class="data row15 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row16" class="row_heading level0 row16" >16</th>
      <td id="T_c6ca4_row16_col0" class="data row16 col0" >encoder.layers.0.self_attn.v_proj.bias</td>
      <td id="T_c6ca4_row16_col1" class="data row16 col1" >encoder.layers.0.self_attn.v_proj.bias</td>
      <td id="T_c6ca4_row16_col2" class="data row16 col2" >av_romanizer.w2v_model.encoder.layers.0.self_attn.v_proj.bias</td>
      <td id="T_c6ca4_row16_col3" class="data row16 col3" >0.218022</td>
      <td id="T_c6ca4_row16_col4" class="data row16 col4" >0.683059</td>
      <td id="T_c6ca4_row16_col5" class="data row16 col5" >0.590208</td>
      <td id="T_c6ca4_row16_col6" class="data row16 col6" >0.319185</td>
      <td id="T_c6ca4_row16_col7" class="data row16 col7" >0.951739</td>
      <td id="T_c6ca4_row16_col8" class="data row16 col8" >0.048645</td>
      <td id="T_c6ca4_row16_col9" class="data row16 col9" >1024.000000</td>
      <td id="T_c6ca4_row16_col10" class="data row16 col10" >torch.float32</td>
      <td id="T_c6ca4_row16_col11" class="data row16 col11" >(1024,)</td>
      <td id="T_c6ca4_row16_col12" class="data row16 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row17" class="row_heading level0 row17" >17</th>
      <td id="T_c6ca4_row17_col0" class="data row17 col0" >encoder.layers.0.self_attn.v_proj.weight</td>
      <td id="T_c6ca4_row17_col1" class="data row17 col1" >encoder.layers.0.self_attn.v_proj.weight</td>
      <td id="T_c6ca4_row17_col2" class="data row17 col2" >av_romanizer.w2v_model.encoder.layers.0.self_attn.v_proj.weight</td>
      <td id="T_c6ca4_row17_col3" class="data row17 col3" >15.5026</td>
      <td id="T_c6ca4_row17_col4" class="data row17 col4" >88.6685</td>
      <td id="T_c6ca4_row17_col5" class="data row17 col5" >83.6381</td>
      <td id="T_c6ca4_row17_col6" class="data row17 col6" >0.174838</td>
      <td id="T_c6ca4_row17_col7" class="data row17 col7" >0.985503</td>
      <td id="T_c6ca4_row17_col8" class="data row17 col8" >0.21582</td>
      <td id="T_c6ca4_row17_col9" class="data row17 col9" >1048576.000000</td>
      <td id="T_c6ca4_row17_col10" class="data row17 col10" >torch.float32</td>
      <td id="T_c6ca4_row17_col11" class="data row17 col11" >(1024, 1024)</td>
      <td id="T_c6ca4_row17_col12" class="data row17 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row18" class="row_heading level0 row18" >18</th>
      <td id="T_c6ca4_row18_col0" class="data row18 col0" >encoder.layers.0.self_attn_layer_norm.bias</td>
      <td id="T_c6ca4_row18_col1" class="data row18 col1" >encoder.layers.0.self_attn_layer_norm.bias</td>
      <td id="T_c6ca4_row18_col2" class="data row18 col2" >av_romanizer.w2v_model.encoder.layers.0.self_attn_layer_norm.bias</td>
      <td id="T_c6ca4_row18_col3" class="data row18 col3" >0.160751</td>
      <td id="T_c6ca4_row18_col4" class="data row18 col4" >1.18133</td>
      <td id="T_c6ca4_row18_col5" class="data row18 col5" >1.16318</td>
      <td id="T_c6ca4_row18_col6" class="data row18 col6" >0.136076</td>
      <td id="T_c6ca4_row18_col7" class="data row18 col7" >0.990717</td>
      <td id="T_c6ca4_row18_col8" class="data row18 col8" >0.039032</td>
      <td id="T_c6ca4_row18_col9" class="data row18 col9" >1024.000000</td>
      <td id="T_c6ca4_row18_col10" class="data row18 col10" >torch.float32</td>
      <td id="T_c6ca4_row18_col11" class="data row18 col11" >(1024,)</td>
      <td id="T_c6ca4_row18_col12" class="data row18 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row19" class="row_heading level0 row19" >19</th>
      <td id="T_c6ca4_row19_col0" class="data row19 col0" >encoder.layers.0.self_attn_layer_norm.weight</td>
      <td id="T_c6ca4_row19_col1" class="data row19 col1" >encoder.layers.0.self_attn_layer_norm.weight</td>
      <td id="T_c6ca4_row19_col2" class="data row19 col2" >av_romanizer.w2v_model.encoder.layers.0.self_attn_layer_norm.weight</td>
      <td id="T_c6ca4_row19_col3" class="data row19 col3" >0.507531</td>
      <td id="T_c6ca4_row19_col4" class="data row19 col4" >3.25871</td>
      <td id="T_c6ca4_row19_col5" class="data row19 col5" >3.06046</td>
      <td id="T_c6ca4_row19_col6" class="data row19 col6" >0.155746</td>
      <td id="T_c6ca4_row19_col7" class="data row19 col7" >0.989056</td>
      <td id="T_c6ca4_row19_col8" class="data row19 col8" >0.161469</td>
      <td id="T_c6ca4_row19_col9" class="data row19 col9" >1024.000000</td>
      <td id="T_c6ca4_row19_col10" class="data row19 col10" >torch.float32</td>
      <td id="T_c6ca4_row19_col11" class="data row19 col11" >(1024,)</td>
      <td id="T_c6ca4_row19_col12" class="data row19 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row20" class="row_heading level0 row20" >20</th>
      <td id="T_c6ca4_row20_col0" class="data row20 col0" >encoder.layers.1.fc1.bias</td>
      <td id="T_c6ca4_row20_col1" class="data row20 col1" >encoder.layers.1.fc1.bias</td>
      <td id="T_c6ca4_row20_col2" class="data row20 col2" >av_romanizer.w2v_model.encoder.layers.1.fc1.bias</td>
      <td id="T_c6ca4_row20_col3" class="data row20 col3" >0.778932</td>
      <td id="T_c6ca4_row20_col4" class="data row20 col4" >3.35932</td>
      <td id="T_c6ca4_row20_col5" class="data row20 col5" >3.46172</td>
      <td id="T_c6ca4_row20_col6" class="data row20 col6" >0.231872</td>
      <td id="T_c6ca4_row20_col7" class="data row20 col7" >0.974364</td>
      <td id="T_c6ca4_row20_col8" class="data row20 col8" >0.0813141</td>
      <td id="T_c6ca4_row20_col9" class="data row20 col9" >4096.000000</td>
      <td id="T_c6ca4_row20_col10" class="data row20 col10" >torch.float32</td>
      <td id="T_c6ca4_row20_col11" class="data row20 col11" >(4096,)</td>
      <td id="T_c6ca4_row20_col12" class="data row20 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row21" class="row_heading level0 row21" >21</th>
      <td id="T_c6ca4_row21_col0" class="data row21 col0" >encoder.layers.1.fc1.weight</td>
      <td id="T_c6ca4_row21_col1" class="data row21 col1" >encoder.layers.1.fc1.weight</td>
      <td id="T_c6ca4_row21_col2" class="data row21 col2" >av_romanizer.w2v_model.encoder.layers.1.fc1.weight</td>
      <td id="T_c6ca4_row21_col3" class="data row21 col3" >38.7436</td>
      <td id="T_c6ca4_row21_col4" class="data row21 col4" >284.994</td>
      <td id="T_c6ca4_row21_col5" class="data row21 col5" >273.066</td>
      <td id="T_c6ca4_row21_col6" class="data row21 col6" >0.135945</td>
      <td id="T_c6ca4_row21_col7" class="data row21 col7" >0.99127</td>
      <td id="T_c6ca4_row21_col8" class="data row21 col8" >0.25</td>
      <td id="T_c6ca4_row21_col9" class="data row21 col9" >4194304.000000</td>
      <td id="T_c6ca4_row21_col10" class="data row21 col10" >torch.float32</td>
      <td id="T_c6ca4_row21_col11" class="data row21 col11" >(4096, 1024)</td>
      <td id="T_c6ca4_row21_col12" class="data row21 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row22" class="row_heading level0 row22" >22</th>
      <td id="T_c6ca4_row22_col0" class="data row22 col0" >encoder.layers.1.fc2.bias</td>
      <td id="T_c6ca4_row22_col1" class="data row22 col1" >encoder.layers.1.fc2.bias</td>
      <td id="T_c6ca4_row22_col2" class="data row22 col2" >av_romanizer.w2v_model.encoder.layers.1.fc2.bias</td>
      <td id="T_c6ca4_row22_col3" class="data row22 col3" >0.144309</td>
      <td id="T_c6ca4_row22_col4" class="data row22 col4" >3.01773</td>
      <td id="T_c6ca4_row22_col5" class="data row22 col5" >2.90623</td>
      <td id="T_c6ca4_row22_col6" class="data row22 col6" >0.0478204</td>
      <td id="T_c6ca4_row22_col7" class="data row22 col7" >0.999522</td>
      <td id="T_c6ca4_row22_col8" class="data row22 col8" >0.0332031</td>
      <td id="T_c6ca4_row22_col9" class="data row22 col9" >1024.000000</td>
      <td id="T_c6ca4_row22_col10" class="data row22 col10" >torch.float32</td>
      <td id="T_c6ca4_row22_col11" class="data row22 col11" >(1024,)</td>
      <td id="T_c6ca4_row22_col12" class="data row22 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row23" class="row_heading level0 row23" >23</th>
      <td id="T_c6ca4_row23_col0" class="data row23 col0" >encoder.layers.1.fc2.weight</td>
      <td id="T_c6ca4_row23_col1" class="data row23 col1" >encoder.layers.1.fc2.weight</td>
      <td id="T_c6ca4_row23_col2" class="data row23 col2" >av_romanizer.w2v_model.encoder.layers.1.fc2.weight</td>
      <td id="T_c6ca4_row23_col3" class="data row23 col3" >34.1536</td>
      <td id="T_c6ca4_row23_col4" class="data row23 col4" >259.585</td>
      <td id="T_c6ca4_row23_col5" class="data row23 col5" >246.091</td>
      <td id="T_c6ca4_row23_col6" class="data row23 col6" >0.13157</td>
      <td id="T_c6ca4_row23_col7" class="data row23 col7" >0.992295</td>
      <td id="T_c6ca4_row23_col8" class="data row23 col8" >0.179688</td>
      <td id="T_c6ca4_row23_col9" class="data row23 col9" >4194304.000000</td>
      <td id="T_c6ca4_row23_col10" class="data row23 col10" >torch.float32</td>
      <td id="T_c6ca4_row23_col11" class="data row23 col11" >(1024, 4096)</td>
      <td id="T_c6ca4_row23_col12" class="data row23 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row24" class="row_heading level0 row24" >24</th>
      <td id="T_c6ca4_row24_col0" class="data row24 col0" >encoder.layers.1.final_layer_norm.bias</td>
      <td id="T_c6ca4_row24_col1" class="data row24 col1" >encoder.layers.1.final_layer_norm.bias</td>
      <td id="T_c6ca4_row24_col2" class="data row24 col2" >av_romanizer.w2v_model.encoder.layers.1.final_layer_norm.bias</td>
      <td id="T_c6ca4_row24_col3" class="data row24 col3" >0.247625</td>
      <td id="T_c6ca4_row24_col4" class="data row24 col4" >2.01097</td>
      <td id="T_c6ca4_row24_col5" class="data row24 col5" >1.9779</td>
      <td id="T_c6ca4_row24_col6" class="data row24 col6" >0.123137</td>
      <td id="T_c6ca4_row24_col7" class="data row24 col7" >0.992429</td>
      <td id="T_c6ca4_row24_col8" class="data row24 col8" >0.0670166</td>
      <td id="T_c6ca4_row24_col9" class="data row24 col9" >1024.000000</td>
      <td id="T_c6ca4_row24_col10" class="data row24 col10" >torch.float32</td>
      <td id="T_c6ca4_row24_col11" class="data row24 col11" >(1024,)</td>
      <td id="T_c6ca4_row24_col12" class="data row24 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row25" class="row_heading level0 row25" >25</th>
      <td id="T_c6ca4_row25_col0" class="data row25 col0" >encoder.layers.1.final_layer_norm.weight</td>
      <td id="T_c6ca4_row25_col1" class="data row25 col1" >encoder.layers.1.final_layer_norm.weight</td>
      <td id="T_c6ca4_row25_col2" class="data row25 col2" >av_romanizer.w2v_model.encoder.layers.1.final_layer_norm.weight</td>
      <td id="T_c6ca4_row25_col3" class="data row25 col3" >0.326213</td>
      <td id="T_c6ca4_row25_col4" class="data row25 col4" >4.41511</td>
      <td id="T_c6ca4_row25_col5" class="data row25 col5" >4.46182</td>
      <td id="T_c6ca4_row25_col6" class="data row25 col6" >0.0738855</td>
      <td id="T_c6ca4_row25_col7" class="data row25 col7" >0.997354</td>
      <td id="T_c6ca4_row25_col8" class="data row25 col8" >0.0487061</td>
      <td id="T_c6ca4_row25_col9" class="data row25 col9" >1024.000000</td>
      <td id="T_c6ca4_row25_col10" class="data row25 col10" >torch.float32</td>
      <td id="T_c6ca4_row25_col11" class="data row25 col11" >(1024,)</td>
      <td id="T_c6ca4_row25_col12" class="data row25 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row26" class="row_heading level0 row26" >26</th>
      <td id="T_c6ca4_row26_col0" class="data row26 col0" >encoder.layers.1.self_attn.k_proj.bias</td>
      <td id="T_c6ca4_row26_col1" class="data row26 col1" >encoder.layers.1.self_attn.k_proj.bias</td>
      <td id="T_c6ca4_row26_col2" class="data row26 col2" >av_romanizer.w2v_model.encoder.layers.1.self_attn.k_proj.bias</td>
      <td id="T_c6ca4_row26_col3" class="data row26 col3" >0.0109688</td>
      <td id="T_c6ca4_row26_col4" class="data row26 col4" >0.179663</td>
      <td id="T_c6ca4_row26_col5" class="data row26 col5" >0.171749</td>
      <td id="T_c6ca4_row26_col6" class="data row26 col6" >0.0610522</td>
      <td id="T_c6ca4_row26_col7" class="data row26 col7" >0.999065</td>
      <td id="T_c6ca4_row26_col8" class="data row26 col8" >0.00228882</td>
      <td id="T_c6ca4_row26_col9" class="data row26 col9" >1024.000000</td>
      <td id="T_c6ca4_row26_col10" class="data row26 col10" >torch.float32</td>
      <td id="T_c6ca4_row26_col11" class="data row26 col11" >(1024,)</td>
      <td id="T_c6ca4_row26_col12" class="data row26 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row27" class="row_heading level0 row27" >27</th>
      <td id="T_c6ca4_row27_col0" class="data row27 col0" >encoder.layers.1.self_attn.k_proj.weight</td>
      <td id="T_c6ca4_row27_col1" class="data row27 col1" >encoder.layers.1.self_attn.k_proj.weight</td>
      <td id="T_c6ca4_row27_col2" class="data row27 col2" >av_romanizer.w2v_model.encoder.layers.1.self_attn.k_proj.weight</td>
      <td id="T_c6ca4_row27_col3" class="data row27 col3" >19.3221</td>
      <td id="T_c6ca4_row27_col4" class="data row27 col4" >147.171</td>
      <td id="T_c6ca4_row27_col5" class="data row27 col5" >142.566</td>
      <td id="T_c6ca4_row27_col6" class="data row27 col6" >0.13129</td>
      <td id="T_c6ca4_row27_col7" class="data row27 col7" >0.991608</td>
      <td id="T_c6ca4_row27_col8" class="data row27 col8" >0.179688</td>
      <td id="T_c6ca4_row27_col9" class="data row27 col9" >1048576.000000</td>
      <td id="T_c6ca4_row27_col10" class="data row27 col10" >torch.float32</td>
      <td id="T_c6ca4_row27_col11" class="data row27 col11" >(1024, 1024)</td>
      <td id="T_c6ca4_row27_col12" class="data row27 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row28" class="row_heading level0 row28" >28</th>
      <td id="T_c6ca4_row28_col0" class="data row28 col0" >encoder.layers.1.self_attn.out_proj.bias</td>
      <td id="T_c6ca4_row28_col1" class="data row28 col1" >encoder.layers.1.self_attn.out_proj.bias</td>
      <td id="T_c6ca4_row28_col2" class="data row28 col2" >av_romanizer.w2v_model.encoder.layers.1.self_attn.out_proj.bias</td>
      <td id="T_c6ca4_row28_col3" class="data row28 col3" >0.334746</td>
      <td id="T_c6ca4_row28_col4" class="data row28 col4" >3.62743</td>
      <td id="T_c6ca4_row28_col5" class="data row28 col5" >3.39483</td>
      <td id="T_c6ca4_row28_col6" class="data row28 col6" >0.0922818</td>
      <td id="T_c6ca4_row28_col7" class="data row28 col7" >0.997647</td>
      <td id="T_c6ca4_row28_col8" class="data row28 col8" >0.0830078</td>
      <td id="T_c6ca4_row28_col9" class="data row28 col9" >1024.000000</td>
      <td id="T_c6ca4_row28_col10" class="data row28 col10" >torch.float32</td>
      <td id="T_c6ca4_row28_col11" class="data row28 col11" >(1024,)</td>
      <td id="T_c6ca4_row28_col12" class="data row28 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row29" class="row_heading level0 row29" >29</th>
      <td id="T_c6ca4_row29_col0" class="data row29 col0" >encoder.layers.1.self_attn.out_proj.weight</td>
      <td id="T_c6ca4_row29_col1" class="data row29 col1" >encoder.layers.1.self_attn.out_proj.weight</td>
      <td id="T_c6ca4_row29_col2" class="data row29 col2" >av_romanizer.w2v_model.encoder.layers.1.self_attn.out_proj.weight</td>
      <td id="T_c6ca4_row29_col3" class="data row29 col3" >19.5729</td>
      <td id="T_c6ca4_row29_col4" class="data row29 col4" >115.595</td>
      <td id="T_c6ca4_row29_col5" class="data row29 col5" >109.249</td>
      <td id="T_c6ca4_row29_col6" class="data row29 col6" >0.169322</td>
      <td id="T_c6ca4_row29_col7" class="data row29 col7" >0.986427</td>
      <td id="T_c6ca4_row29_col8" class="data row29 col8" >0.169922</td>
      <td id="T_c6ca4_row29_col9" class="data row29 col9" >1048576.000000</td>
      <td id="T_c6ca4_row29_col10" class="data row29 col10" >torch.float32</td>
      <td id="T_c6ca4_row29_col11" class="data row29 col11" >(1024, 1024)</td>
      <td id="T_c6ca4_row29_col12" class="data row29 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row30" class="row_heading level0 row30" >30</th>
      <td id="T_c6ca4_row30_col0" class="data row30 col0" >encoder.layers.1.self_attn.q_proj.bias</td>
      <td id="T_c6ca4_row30_col1" class="data row30 col1" >encoder.layers.1.self_attn.q_proj.bias</td>
      <td id="T_c6ca4_row30_col2" class="data row30 col2" >av_romanizer.w2v_model.encoder.layers.1.self_attn.q_proj.bias</td>
      <td id="T_c6ca4_row30_col3" class="data row30 col3" >0.953839</td>
      <td id="T_c6ca4_row30_col4" class="data row30 col4" >10.7764</td>
      <td id="T_c6ca4_row30_col5" class="data row30 col5" >10.705</td>
      <td id="T_c6ca4_row30_col6" class="data row30 col6" >0.0885115</td>
      <td id="T_c6ca4_row30_col7" class="data row30 col7" >0.996079</td>
      <td id="T_c6ca4_row30_col8" class="data row30 col8" >0.170654</td>
      <td id="T_c6ca4_row30_col9" class="data row30 col9" >1024.000000</td>
      <td id="T_c6ca4_row30_col10" class="data row30 col10" >torch.float32</td>
      <td id="T_c6ca4_row30_col11" class="data row30 col11" >(1024,)</td>
      <td id="T_c6ca4_row30_col12" class="data row30 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row31" class="row_heading level0 row31" >31</th>
      <td id="T_c6ca4_row31_col0" class="data row31 col0" >encoder.layers.1.self_attn.q_proj.weight</td>
      <td id="T_c6ca4_row31_col1" class="data row31 col1" >encoder.layers.1.self_attn.q_proj.weight</td>
      <td id="T_c6ca4_row31_col2" class="data row31 col2" >av_romanizer.w2v_model.encoder.layers.1.self_attn.q_proj.weight</td>
      <td id="T_c6ca4_row31_col3" class="data row31 col3" >18.8134</td>
      <td id="T_c6ca4_row31_col4" class="data row31 col4" >147.003</td>
      <td id="T_c6ca4_row31_col5" class="data row31 col5" >141.899</td>
      <td id="T_c6ca4_row31_col6" class="data row31 col6" >0.127979</td>
      <td id="T_c6ca4_row31_col7" class="data row31 col7" >0.992141</td>
      <td id="T_c6ca4_row31_col8" class="data row31 col8" >0.152954</td>
      <td id="T_c6ca4_row31_col9" class="data row31 col9" >1048576.000000</td>
      <td id="T_c6ca4_row31_col10" class="data row31 col10" >torch.float32</td>
      <td id="T_c6ca4_row31_col11" class="data row31 col11" >(1024, 1024)</td>
      <td id="T_c6ca4_row31_col12" class="data row31 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row32" class="row_heading level0 row32" >32</th>
      <td id="T_c6ca4_row32_col0" class="data row32 col0" >encoder.layers.1.self_attn.v_proj.bias</td>
      <td id="T_c6ca4_row32_col1" class="data row32 col1" >encoder.layers.1.self_attn.v_proj.bias</td>
      <td id="T_c6ca4_row32_col2" class="data row32 col2" >av_romanizer.w2v_model.encoder.layers.1.self_attn.v_proj.bias</td>
      <td id="T_c6ca4_row32_col3" class="data row32 col3" >0.294184</td>
      <td id="T_c6ca4_row32_col4" class="data row32 col4" >1.22376</td>
      <td id="T_c6ca4_row32_col5" class="data row32 col5" >1.08171</td>
      <td id="T_c6ca4_row32_col6" class="data row32 col6" >0.240394</td>
      <td id="T_c6ca4_row32_col7" class="data row32 col7" >0.974932</td>
      <td id="T_c6ca4_row32_col8" class="data row32 col8" >0.10498</td>
      <td id="T_c6ca4_row32_col9" class="data row32 col9" >1024.000000</td>
      <td id="T_c6ca4_row32_col10" class="data row32 col10" >torch.float32</td>
      <td id="T_c6ca4_row32_col11" class="data row32 col11" >(1024,)</td>
      <td id="T_c6ca4_row32_col12" class="data row32 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row33" class="row_heading level0 row33" >33</th>
      <td id="T_c6ca4_row33_col0" class="data row33 col0" >encoder.layers.1.self_attn.v_proj.weight</td>
      <td id="T_c6ca4_row33_col1" class="data row33 col1" >encoder.layers.1.self_attn.v_proj.weight</td>
      <td id="T_c6ca4_row33_col2" class="data row33 col2" >av_romanizer.w2v_model.encoder.layers.1.self_attn.v_proj.weight</td>
      <td id="T_c6ca4_row33_col3" class="data row33 col3" >17.6176</td>
      <td id="T_c6ca4_row33_col4" class="data row33 col4" >108.35</td>
      <td id="T_c6ca4_row33_col5" class="data row33 col5" >102.238</td>
      <td id="T_c6ca4_row33_col6" class="data row33 col6" >0.162599</td>
      <td id="T_c6ca4_row33_col7" class="data row33 col7" >0.987677</td>
      <td id="T_c6ca4_row33_col8" class="data row33 col8" >0.102173</td>
      <td id="T_c6ca4_row33_col9" class="data row33 col9" >1048576.000000</td>
      <td id="T_c6ca4_row33_col10" class="data row33 col10" >torch.float32</td>
      <td id="T_c6ca4_row33_col11" class="data row33 col11" >(1024, 1024)</td>
      <td id="T_c6ca4_row33_col12" class="data row33 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row34" class="row_heading level0 row34" >34</th>
      <td id="T_c6ca4_row34_col0" class="data row34 col0" >encoder.layers.1.self_attn_layer_norm.bias</td>
      <td id="T_c6ca4_row34_col1" class="data row34 col1" >encoder.layers.1.self_attn_layer_norm.bias</td>
      <td id="T_c6ca4_row34_col2" class="data row34 col2" >av_romanizer.w2v_model.encoder.layers.1.self_attn_layer_norm.bias</td>
      <td id="T_c6ca4_row34_col3" class="data row34 col3" >0.194414</td>
      <td id="T_c6ca4_row34_col4" class="data row34 col4" >1.10992</td>
      <td id="T_c6ca4_row34_col5" class="data row34 col5" >1.04378</td>
      <td id="T_c6ca4_row34_col6" class="data row34 col6" >0.17516</td>
      <td id="T_c6ca4_row34_col7" class="data row34 col7" >0.985575</td>
      <td id="T_c6ca4_row34_col8" class="data row34 col8" >0.0473633</td>
      <td id="T_c6ca4_row34_col9" class="data row34 col9" >1024.000000</td>
      <td id="T_c6ca4_row34_col10" class="data row34 col10" >torch.float32</td>
      <td id="T_c6ca4_row34_col11" class="data row34 col11" >(1024,)</td>
      <td id="T_c6ca4_row34_col12" class="data row34 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row35" class="row_heading level0 row35" >35</th>
      <td id="T_c6ca4_row35_col0" class="data row35 col0" >encoder.layers.1.self_attn_layer_norm.weight</td>
      <td id="T_c6ca4_row35_col1" class="data row35 col1" >encoder.layers.1.self_attn_layer_norm.weight</td>
      <td id="T_c6ca4_row35_col2" class="data row35 col2" >av_romanizer.w2v_model.encoder.layers.1.self_attn_layer_norm.weight</td>
      <td id="T_c6ca4_row35_col3" class="data row35 col3" >0.610429</td>
      <td id="T_c6ca4_row35_col4" class="data row35 col4" >4.23268</td>
      <td id="T_c6ca4_row35_col5" class="data row35 col5" >3.86834</td>
      <td id="T_c6ca4_row35_col6" class="data row35 col6" >0.144218</td>
      <td id="T_c6ca4_row35_col7" class="data row35 col7" >0.992675</td>
      <td id="T_c6ca4_row35_col8" class="data row35 col8" >0.117676</td>
      <td id="T_c6ca4_row35_col9" class="data row35 col9" >1024.000000</td>
      <td id="T_c6ca4_row35_col10" class="data row35 col10" >torch.float32</td>
      <td id="T_c6ca4_row35_col11" class="data row35 col11" >(1024,)</td>
      <td id="T_c6ca4_row35_col12" class="data row35 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row36" class="row_heading level0 row36" >36</th>
      <td id="T_c6ca4_row36_col0" class="data row36 col0" >encoder.layers.10.fc1.bias</td>
      <td id="T_c6ca4_row36_col1" class="data row36 col1" >encoder.layers.10.fc1.bias</td>
      <td id="T_c6ca4_row36_col2" class="data row36 col2" >av_romanizer.w2v_model.encoder.layers.10.fc1.bias</td>
      <td id="T_c6ca4_row36_col3" class="data row36 col3" >0.62671</td>
      <td id="T_c6ca4_row36_col4" class="data row36 col4" >3.12114</td>
      <td id="T_c6ca4_row36_col5" class="data row36 col5" >3.1269</td>
      <td id="T_c6ca4_row36_col6" class="data row36 col6" >0.200795</td>
      <td id="T_c6ca4_row36_col7" class="data row36 col7" >0.979879</td>
      <td id="T_c6ca4_row36_col8" class="data row36 col8" >0.0654602</td>
      <td id="T_c6ca4_row36_col9" class="data row36 col9" >4096.000000</td>
      <td id="T_c6ca4_row36_col10" class="data row36 col10" >torch.float32</td>
      <td id="T_c6ca4_row36_col11" class="data row36 col11" >(4096,)</td>
      <td id="T_c6ca4_row36_col12" class="data row36 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row37" class="row_heading level0 row37" >37</th>
      <td id="T_c6ca4_row37_col0" class="data row37 col0" >encoder.layers.10.fc1.weight</td>
      <td id="T_c6ca4_row37_col1" class="data row37 col1" >encoder.layers.10.fc1.weight</td>
      <td id="T_c6ca4_row37_col2" class="data row37 col2" >av_romanizer.w2v_model.encoder.layers.10.fc1.weight</td>
      <td id="T_c6ca4_row37_col3" class="data row37 col3" >53.3588</td>
      <td id="T_c6ca4_row37_col4" class="data row37 col4" >303.202</td>
      <td id="T_c6ca4_row37_col5" class="data row37 col5" >289.773</td>
      <td id="T_c6ca4_row37_col6" class="data row37 col6" >0.175985</td>
      <td id="T_c6ca4_row37_col7" class="data row37 col7" >0.984823</td>
      <td id="T_c6ca4_row37_col8" class="data row37 col8" >0.216309</td>
      <td id="T_c6ca4_row37_col9" class="data row37 col9" >4194304.000000</td>
      <td id="T_c6ca4_row37_col10" class="data row37 col10" >torch.float32</td>
      <td id="T_c6ca4_row37_col11" class="data row37 col11" >(4096, 1024)</td>
      <td id="T_c6ca4_row37_col12" class="data row37 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row38" class="row_heading level0 row38" >38</th>
      <td id="T_c6ca4_row38_col0" class="data row38 col0" >encoder.layers.10.fc2.bias</td>
      <td id="T_c6ca4_row38_col1" class="data row38 col1" >encoder.layers.10.fc2.bias</td>
      <td id="T_c6ca4_row38_col2" class="data row38 col2" >av_romanizer.w2v_model.encoder.layers.10.fc2.bias</td>
      <td id="T_c6ca4_row38_col3" class="data row38 col3" >0.197284</td>
      <td id="T_c6ca4_row38_col4" class="data row38 col4" >2.75618</td>
      <td id="T_c6ca4_row38_col5" class="data row38 col5" >2.61379</td>
      <td id="T_c6ca4_row38_col6" class="data row38 col6" >0.0715787</td>
      <td id="T_c6ca4_row38_col7" class="data row38 col7" >0.998706</td>
      <td id="T_c6ca4_row38_col8" class="data row38 col8" >0.0351562</td>
      <td id="T_c6ca4_row38_col9" class="data row38 col9" >1024.000000</td>
      <td id="T_c6ca4_row38_col10" class="data row38 col10" >torch.float32</td>
      <td id="T_c6ca4_row38_col11" class="data row38 col11" >(1024,)</td>
      <td id="T_c6ca4_row38_col12" class="data row38 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row39" class="row_heading level0 row39" >39</th>
      <td id="T_c6ca4_row39_col0" class="data row39 col0" >encoder.layers.10.fc2.weight</td>
      <td id="T_c6ca4_row39_col1" class="data row39 col1" >encoder.layers.10.fc2.weight</td>
      <td id="T_c6ca4_row39_col2" class="data row39 col2" >av_romanizer.w2v_model.encoder.layers.10.fc2.weight</td>
      <td id="T_c6ca4_row39_col3" class="data row39 col3" >50.8608</td>
      <td id="T_c6ca4_row39_col4" class="data row39 col4" >309.404</td>
      <td id="T_c6ca4_row39_col5" class="data row39 col5" >294.833</td>
      <td id="T_c6ca4_row39_col6" class="data row39 col6" >0.164384</td>
      <td id="T_c6ca4_row39_col7" class="data row39 col7" >0.986985</td>
      <td id="T_c6ca4_row39_col8" class="data row39 col8" >0.18103</td>
      <td id="T_c6ca4_row39_col9" class="data row39 col9" >4194304.000000</td>
      <td id="T_c6ca4_row39_col10" class="data row39 col10" >torch.float32</td>
      <td id="T_c6ca4_row39_col11" class="data row39 col11" >(1024, 4096)</td>
      <td id="T_c6ca4_row39_col12" class="data row39 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row40" class="row_heading level0 row40" >40</th>
      <td id="T_c6ca4_row40_col0" class="data row40 col0" >encoder.layers.10.final_layer_norm.bias</td>
      <td id="T_c6ca4_row40_col1" class="data row40 col1" >encoder.layers.10.final_layer_norm.bias</td>
      <td id="T_c6ca4_row40_col2" class="data row40 col2" >av_romanizer.w2v_model.encoder.layers.10.final_layer_norm.bias</td>
      <td id="T_c6ca4_row40_col3" class="data row40 col3" >0.228827</td>
      <td id="T_c6ca4_row40_col4" class="data row40 col4" >2.23653</td>
      <td id="T_c6ca4_row40_col5" class="data row40 col5" >2.12045</td>
      <td id="T_c6ca4_row40_col6" class="data row40 col6" >0.102314</td>
      <td id="T_c6ca4_row40_col7" class="data row40 col7" >0.9959</td>
      <td id="T_c6ca4_row40_col8" class="data row40 col8" >0.0292969</td>
      <td id="T_c6ca4_row40_col9" class="data row40 col9" >1024.000000</td>
      <td id="T_c6ca4_row40_col10" class="data row40 col10" >torch.float32</td>
      <td id="T_c6ca4_row40_col11" class="data row40 col11" >(1024,)</td>
      <td id="T_c6ca4_row40_col12" class="data row40 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row41" class="row_heading level0 row41" >41</th>
      <td id="T_c6ca4_row41_col0" class="data row41 col0" >encoder.layers.10.final_layer_norm.weight</td>
      <td id="T_c6ca4_row41_col1" class="data row41 col1" >encoder.layers.10.final_layer_norm.weight</td>
      <td id="T_c6ca4_row41_col2" class="data row41 col2" >av_romanizer.w2v_model.encoder.layers.10.final_layer_norm.weight</td>
      <td id="T_c6ca4_row41_col3" class="data row41 col3" >0.592174</td>
      <td id="T_c6ca4_row41_col4" class="data row41 col4" >7.14238</td>
      <td id="T_c6ca4_row41_col5" class="data row41 col5" >6.69188</td>
      <td id="T_c6ca4_row41_col6" class="data row41 col6" >0.0829099</td>
      <td id="T_c6ca4_row41_col7" class="data row41 col7" >0.998455</td>
      <td id="T_c6ca4_row41_col8" class="data row41 col8" >0.0583496</td>
      <td id="T_c6ca4_row41_col9" class="data row41 col9" >1024.000000</td>
      <td id="T_c6ca4_row41_col10" class="data row41 col10" >torch.float32</td>
      <td id="T_c6ca4_row41_col11" class="data row41 col11" >(1024,)</td>
      <td id="T_c6ca4_row41_col12" class="data row41 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row42" class="row_heading level0 row42" >42</th>
      <td id="T_c6ca4_row42_col0" class="data row42 col0" >encoder.layers.10.self_attn.k_proj.bias</td>
      <td id="T_c6ca4_row42_col1" class="data row42 col1" >encoder.layers.10.self_attn.k_proj.bias</td>
      <td id="T_c6ca4_row42_col2" class="data row42 col2" >av_romanizer.w2v_model.encoder.layers.10.self_attn.k_proj.bias</td>
      <td id="T_c6ca4_row42_col3" class="data row42 col3" >0.0137862</td>
      <td id="T_c6ca4_row42_col4" class="data row42 col4" >0.0737639</td>
      <td id="T_c6ca4_row42_col5" class="data row42 col5" >0.0706545</td>
      <td id="T_c6ca4_row42_col6" class="data row42 col6" >0.186897</td>
      <td id="T_c6ca4_row42_col7" class="data row42 col7" >0.982694</td>
      <td id="T_c6ca4_row42_col8" class="data row42 col8" >0.0030365</td>
      <td id="T_c6ca4_row42_col9" class="data row42 col9" >1024.000000</td>
      <td id="T_c6ca4_row42_col10" class="data row42 col10" >torch.float32</td>
      <td id="T_c6ca4_row42_col11" class="data row42 col11" >(1024,)</td>
      <td id="T_c6ca4_row42_col12" class="data row42 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row43" class="row_heading level0 row43" >43</th>
      <td id="T_c6ca4_row43_col0" class="data row43 col0" >encoder.layers.10.self_attn.k_proj.weight</td>
      <td id="T_c6ca4_row43_col1" class="data row43 col1" >encoder.layers.10.self_attn.k_proj.weight</td>
      <td id="T_c6ca4_row43_col2" class="data row43 col2" >av_romanizer.w2v_model.encoder.layers.10.self_attn.k_proj.weight</td>
      <td id="T_c6ca4_row43_col3" class="data row43 col3" >24.4768</td>
      <td id="T_c6ca4_row43_col4" class="data row43 col4" >143.693</td>
      <td id="T_c6ca4_row43_col5" class="data row43 col5" >137.906</td>
      <td id="T_c6ca4_row43_col6" class="data row43 col6" >0.170341</td>
      <td id="T_c6ca4_row43_col7" class="data row43 col7" >0.985728</td>
      <td id="T_c6ca4_row43_col8" class="data row43 col8" >0.153198</td>
      <td id="T_c6ca4_row43_col9" class="data row43 col9" >1048576.000000</td>
      <td id="T_c6ca4_row43_col10" class="data row43 col10" >torch.float32</td>
      <td id="T_c6ca4_row43_col11" class="data row43 col11" >(1024, 1024)</td>
      <td id="T_c6ca4_row43_col12" class="data row43 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row44" class="row_heading level0 row44" >44</th>
      <td id="T_c6ca4_row44_col0" class="data row44 col0" >encoder.layers.10.self_attn.out_proj.bias</td>
      <td id="T_c6ca4_row44_col1" class="data row44 col1" >encoder.layers.10.self_attn.out_proj.bias</td>
      <td id="T_c6ca4_row44_col2" class="data row44 col2" >av_romanizer.w2v_model.encoder.layers.10.self_attn.out_proj.bias</td>
      <td id="T_c6ca4_row44_col3" class="data row44 col3" >0.28438</td>
      <td id="T_c6ca4_row44_col4" class="data row44 col4" >3.73534</td>
      <td id="T_c6ca4_row44_col5" class="data row44 col5" >3.52541</td>
      <td id="T_c6ca4_row44_col6" class="data row44 col6" >0.0761323</td>
      <td id="T_c6ca4_row44_col7" class="data row44 col7" >0.998603</td>
      <td id="T_c6ca4_row44_col8" class="data row44 col8" >0.0351562</td>
      <td id="T_c6ca4_row44_col9" class="data row44 col9" >1024.000000</td>
      <td id="T_c6ca4_row44_col10" class="data row44 col10" >torch.float32</td>
      <td id="T_c6ca4_row44_col11" class="data row44 col11" >(1024,)</td>
      <td id="T_c6ca4_row44_col12" class="data row44 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row45" class="row_heading level0 row45" >45</th>
      <td id="T_c6ca4_row45_col0" class="data row45 col0" >encoder.layers.10.self_attn.out_proj.weight</td>
      <td id="T_c6ca4_row45_col1" class="data row45 col1" >encoder.layers.10.self_attn.out_proj.weight</td>
      <td id="T_c6ca4_row45_col2" class="data row45 col2" >av_romanizer.w2v_model.encoder.layers.10.self_attn.out_proj.weight</td>
      <td id="T_c6ca4_row45_col3" class="data row45 col3" >26.755</td>
      <td id="T_c6ca4_row45_col4" class="data row45 col4" >133.268</td>
      <td id="T_c6ca4_row45_col5" class="data row45 col5" >127.498</td>
      <td id="T_c6ca4_row45_col6" class="data row45 col6" >0.200761</td>
      <td id="T_c6ca4_row45_col7" class="data row45 col7" >0.979915</td>
      <td id="T_c6ca4_row45_col8" class="data row45 col8" >0.146484</td>
      <td id="T_c6ca4_row45_col9" class="data row45 col9" >1048576.000000</td>
      <td id="T_c6ca4_row45_col10" class="data row45 col10" >torch.float32</td>
      <td id="T_c6ca4_row45_col11" class="data row45 col11" >(1024, 1024)</td>
      <td id="T_c6ca4_row45_col12" class="data row45 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row46" class="row_heading level0 row46" >46</th>
      <td id="T_c6ca4_row46_col0" class="data row46 col0" >encoder.layers.10.self_attn.q_proj.bias</td>
      <td id="T_c6ca4_row46_col1" class="data row46 col1" >encoder.layers.10.self_attn.q_proj.bias</td>
      <td id="T_c6ca4_row46_col2" class="data row46 col2" >av_romanizer.w2v_model.encoder.layers.10.self_attn.q_proj.bias</td>
      <td id="T_c6ca4_row46_col3" class="data row46 col3" >0.910707</td>
      <td id="T_c6ca4_row46_col4" class="data row46 col4" >8.99433</td>
      <td id="T_c6ca4_row46_col5" class="data row46 col5" >8.74086</td>
      <td id="T_c6ca4_row46_col6" class="data row46 col6" >0.101253</td>
      <td id="T_c6ca4_row46_col7" class="data row46 col7" >0.995134</td>
      <td id="T_c6ca4_row46_col8" class="data row46 col8" >0.118408</td>
      <td id="T_c6ca4_row46_col9" class="data row46 col9" >1024.000000</td>
      <td id="T_c6ca4_row46_col10" class="data row46 col10" >torch.float32</td>
      <td id="T_c6ca4_row46_col11" class="data row46 col11" >(1024,)</td>
      <td id="T_c6ca4_row46_col12" class="data row46 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row47" class="row_heading level0 row47" >47</th>
      <td id="T_c6ca4_row47_col0" class="data row47 col0" >encoder.layers.10.self_attn.q_proj.weight</td>
      <td id="T_c6ca4_row47_col1" class="data row47 col1" >encoder.layers.10.self_attn.q_proj.weight</td>
      <td id="T_c6ca4_row47_col2" class="data row47 col2" >av_romanizer.w2v_model.encoder.layers.10.self_attn.q_proj.weight</td>
      <td id="T_c6ca4_row47_col3" class="data row47 col3" >24.9992</td>
      <td id="T_c6ca4_row47_col4" class="data row47 col4" >142.68</td>
      <td id="T_c6ca4_row47_col5" class="data row47 col5" >136.946</td>
      <td id="T_c6ca4_row47_col6" class="data row47 col6" >0.175212</td>
      <td id="T_c6ca4_row47_col7" class="data row47 col7" >0.984849</td>
      <td id="T_c6ca4_row47_col8" class="data row47 col8" >0.16217</td>
      <td id="T_c6ca4_row47_col9" class="data row47 col9" >1048576.000000</td>
      <td id="T_c6ca4_row47_col10" class="data row47 col10" >torch.float32</td>
      <td id="T_c6ca4_row47_col11" class="data row47 col11" >(1024, 1024)</td>
      <td id="T_c6ca4_row47_col12" class="data row47 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row48" class="row_heading level0 row48" >48</th>
      <td id="T_c6ca4_row48_col0" class="data row48 col0" >encoder.layers.10.self_attn.v_proj.bias</td>
      <td id="T_c6ca4_row48_col1" class="data row48 col1" >encoder.layers.10.self_attn.v_proj.bias</td>
      <td id="T_c6ca4_row48_col2" class="data row48 col2" >av_romanizer.w2v_model.encoder.layers.10.self_attn.v_proj.bias</td>
      <td id="T_c6ca4_row48_col3" class="data row48 col3" >0.226148</td>
      <td id="T_c6ca4_row48_col4" class="data row48 col4" >1.19369</td>
      <td id="T_c6ca4_row48_col5" class="data row48 col5" >1.16809</td>
      <td id="T_c6ca4_row48_col6" class="data row48 col6" >0.189452</td>
      <td id="T_c6ca4_row48_col7" class="data row48 col7" >0.981896</td>
      <td id="T_c6ca4_row48_col8" class="data row48 col8" >0.0300331</td>
      <td id="T_c6ca4_row48_col9" class="data row48 col9" >1024.000000</td>
      <td id="T_c6ca4_row48_col10" class="data row48 col10" >torch.float32</td>
      <td id="T_c6ca4_row48_col11" class="data row48 col11" >(1024,)</td>
      <td id="T_c6ca4_row48_col12" class="data row48 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row49" class="row_heading level0 row49" >49</th>
      <td id="T_c6ca4_row49_col0" class="data row49 col0" >encoder.layers.10.self_attn.v_proj.weight</td>
      <td id="T_c6ca4_row49_col1" class="data row49 col1" >encoder.layers.10.self_attn.v_proj.weight</td>
      <td id="T_c6ca4_row49_col2" class="data row49 col2" >av_romanizer.w2v_model.encoder.layers.10.self_attn.v_proj.weight</td>
      <td id="T_c6ca4_row49_col3" class="data row49 col3" >23.4478</td>
      <td id="T_c6ca4_row49_col4" class="data row49 col4" >137.595</td>
      <td id="T_c6ca4_row49_col5" class="data row49 col5" >131.666</td>
      <td id="T_c6ca4_row49_col6" class="data row49 col6" >0.170412</td>
      <td id="T_c6ca4_row49_col7" class="data row49 col7" >0.985796</td>
      <td id="T_c6ca4_row49_col8" class="data row49 col8" >0.134277</td>
      <td id="T_c6ca4_row49_col9" class="data row49 col9" >1048576.000000</td>
      <td id="T_c6ca4_row49_col10" class="data row49 col10" >torch.float32</td>
      <td id="T_c6ca4_row49_col11" class="data row49 col11" >(1024, 1024)</td>
      <td id="T_c6ca4_row49_col12" class="data row49 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row50" class="row_heading level0 row50" >50</th>
      <td id="T_c6ca4_row50_col0" class="data row50 col0" >encoder.layers.10.self_attn_layer_norm.bias</td>
      <td id="T_c6ca4_row50_col1" class="data row50 col1" >encoder.layers.10.self_attn_layer_norm.bias</td>
      <td id="T_c6ca4_row50_col2" class="data row50 col2" >av_romanizer.w2v_model.encoder.layers.10.self_attn_layer_norm.bias</td>
      <td id="T_c6ca4_row50_col3" class="data row50 col3" >0.180118</td>
      <td id="T_c6ca4_row50_col4" class="data row50 col4" >0.68438</td>
      <td id="T_c6ca4_row50_col5" class="data row50 col5" >0.669338</td>
      <td id="T_c6ca4_row50_col6" class="data row50 col6" >0.263184</td>
      <td id="T_c6ca4_row50_col7" class="data row50 col7" >0.964836</td>
      <td id="T_c6ca4_row50_col8" class="data row50 col8" >0.0354042</td>
      <td id="T_c6ca4_row50_col9" class="data row50 col9" >1024.000000</td>
      <td id="T_c6ca4_row50_col10" class="data row50 col10" >torch.float32</td>
      <td id="T_c6ca4_row50_col11" class="data row50 col11" >(1024,)</td>
      <td id="T_c6ca4_row50_col12" class="data row50 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row51" class="row_heading level0 row51" >51</th>
      <td id="T_c6ca4_row51_col0" class="data row51 col0" >encoder.layers.10.self_attn_layer_norm.weight</td>
      <td id="T_c6ca4_row51_col1" class="data row51 col1" >encoder.layers.10.self_attn_layer_norm.weight</td>
      <td id="T_c6ca4_row51_col2" class="data row51 col2" >av_romanizer.w2v_model.encoder.layers.10.self_attn_layer_norm.weight</td>
      <td id="T_c6ca4_row51_col3" class="data row51 col3" >0.378924</td>
      <td id="T_c6ca4_row51_col4" class="data row51 col4" >6.06237</td>
      <td id="T_c6ca4_row51_col5" class="data row51 col5" >6.05886</td>
      <td id="T_c6ca4_row51_col6" class="data row51 col6" >0.0625042</td>
      <td id="T_c6ca4_row51_col7" class="data row51 col7" >0.998046</td>
      <td id="T_c6ca4_row51_col8" class="data row51 col8" >0.0388184</td>
      <td id="T_c6ca4_row51_col9" class="data row51 col9" >1024.000000</td>
      <td id="T_c6ca4_row51_col10" class="data row51 col10" >torch.float32</td>
      <td id="T_c6ca4_row51_col11" class="data row51 col11" >(1024,)</td>
      <td id="T_c6ca4_row51_col12" class="data row51 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row52" class="row_heading level0 row52" >52</th>
      <td id="T_c6ca4_row52_col0" class="data row52 col0" >encoder.layers.11.fc1.bias</td>
      <td id="T_c6ca4_row52_col1" class="data row52 col1" >encoder.layers.11.fc1.bias</td>
      <td id="T_c6ca4_row52_col2" class="data row52 col2" >av_romanizer.w2v_model.encoder.layers.11.fc1.bias</td>
      <td id="T_c6ca4_row52_col3" class="data row52 col3" >0.645406</td>
      <td id="T_c6ca4_row52_col4" class="data row52 col4" >3.1651</td>
      <td id="T_c6ca4_row52_col5" class="data row52 col5" >3.15585</td>
      <td id="T_c6ca4_row52_col6" class="data row52 col6" >0.203913</td>
      <td id="T_c6ca4_row52_col7" class="data row52 col7" >0.979153</td>
      <td id="T_c6ca4_row52_col8" class="data row52 col8" >0.0600953</td>
      <td id="T_c6ca4_row52_col9" class="data row52 col9" >4096.000000</td>
      <td id="T_c6ca4_row52_col10" class="data row52 col10" >torch.float32</td>
      <td id="T_c6ca4_row52_col11" class="data row52 col11" >(4096,)</td>
      <td id="T_c6ca4_row52_col12" class="data row52 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row53" class="row_heading level0 row53" >53</th>
      <td id="T_c6ca4_row53_col0" class="data row53 col0" >encoder.layers.11.fc1.weight</td>
      <td id="T_c6ca4_row53_col1" class="data row53 col1" >encoder.layers.11.fc1.weight</td>
      <td id="T_c6ca4_row53_col2" class="data row53 col2" >av_romanizer.w2v_model.encoder.layers.11.fc1.weight</td>
      <td id="T_c6ca4_row53_col3" class="data row53 col3" >55.9926</td>
      <td id="T_c6ca4_row53_col4" class="data row53 col4" >300.495</td>
      <td id="T_c6ca4_row53_col5" class="data row53 col5" >287.31</td>
      <td id="T_c6ca4_row53_col6" class="data row53 col6" >0.186334</td>
      <td id="T_c6ca4_row53_col7" class="data row53 col7" >0.98285</td>
      <td id="T_c6ca4_row53_col8" class="data row53 col8" >0.215698</td>
      <td id="T_c6ca4_row53_col9" class="data row53 col9" >4194304.000000</td>
      <td id="T_c6ca4_row53_col10" class="data row53 col10" >torch.float32</td>
      <td id="T_c6ca4_row53_col11" class="data row53 col11" >(4096, 1024)</td>
      <td id="T_c6ca4_row53_col12" class="data row53 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row54" class="row_heading level0 row54" >54</th>
      <td id="T_c6ca4_row54_col0" class="data row54 col0" >encoder.layers.11.fc2.bias</td>
      <td id="T_c6ca4_row54_col1" class="data row54 col1" >encoder.layers.11.fc2.bias</td>
      <td id="T_c6ca4_row54_col2" class="data row54 col2" >av_romanizer.w2v_model.encoder.layers.11.fc2.bias</td>
      <td id="T_c6ca4_row54_col3" class="data row54 col3" >0.218354</td>
      <td id="T_c6ca4_row54_col4" class="data row54 col4" >2.69874</td>
      <td id="T_c6ca4_row54_col5" class="data row54 col5" >2.57068</td>
      <td id="T_c6ca4_row54_col6" class="data row54 col6" >0.0809096</td>
      <td id="T_c6ca4_row54_col7" class="data row54 col7" >0.997746</td>
      <td id="T_c6ca4_row54_col8" class="data row54 col8" >0.0305176</td>
      <td id="T_c6ca4_row54_col9" class="data row54 col9" >1024.000000</td>
      <td id="T_c6ca4_row54_col10" class="data row54 col10" >torch.float32</td>
      <td id="T_c6ca4_row54_col11" class="data row54 col11" >(1024,)</td>
      <td id="T_c6ca4_row54_col12" class="data row54 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row55" class="row_heading level0 row55" >55</th>
      <td id="T_c6ca4_row55_col0" class="data row55 col0" >encoder.layers.11.fc2.weight</td>
      <td id="T_c6ca4_row55_col1" class="data row55 col1" >encoder.layers.11.fc2.weight</td>
      <td id="T_c6ca4_row55_col2" class="data row55 col2" >av_romanizer.w2v_model.encoder.layers.11.fc2.weight</td>
      <td id="T_c6ca4_row55_col3" class="data row55 col3" >54.0971</td>
      <td id="T_c6ca4_row55_col4" class="data row55 col4" >306.624</td>
      <td id="T_c6ca4_row55_col5" class="data row55 col5" >292.737</td>
      <td id="T_c6ca4_row55_col6" class="data row55 col6" >0.176428</td>
      <td id="T_c6ca4_row55_col7" class="data row55 col7" >0.984773</td>
      <td id="T_c6ca4_row55_col8" class="data row55 col8" >0.186554</td>
      <td id="T_c6ca4_row55_col9" class="data row55 col9" >4194304.000000</td>
      <td id="T_c6ca4_row55_col10" class="data row55 col10" >torch.float32</td>
      <td id="T_c6ca4_row55_col11" class="data row55 col11" >(1024, 4096)</td>
      <td id="T_c6ca4_row55_col12" class="data row55 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row56" class="row_heading level0 row56" >56</th>
      <td id="T_c6ca4_row56_col0" class="data row56 col0" >encoder.layers.11.final_layer_norm.bias</td>
      <td id="T_c6ca4_row56_col1" class="data row56 col1" >encoder.layers.11.final_layer_norm.bias</td>
      <td id="T_c6ca4_row56_col2" class="data row56 col2" >av_romanizer.w2v_model.encoder.layers.11.final_layer_norm.bias</td>
      <td id="T_c6ca4_row56_col3" class="data row56 col3" >0.224777</td>
      <td id="T_c6ca4_row56_col4" class="data row56 col4" >2.00312</td>
      <td id="T_c6ca4_row56_col5" class="data row56 col5" >1.92604</td>
      <td id="T_c6ca4_row56_col6" class="data row56 col6" >0.112214</td>
      <td id="T_c6ca4_row56_col7" class="data row56 col7" >0.994222</td>
      <td id="T_c6ca4_row56_col8" class="data row56 col8" >0.026062</td>
      <td id="T_c6ca4_row56_col9" class="data row56 col9" >1024.000000</td>
      <td id="T_c6ca4_row56_col10" class="data row56 col10" >torch.float32</td>
      <td id="T_c6ca4_row56_col11" class="data row56 col11" >(1024,)</td>
      <td id="T_c6ca4_row56_col12" class="data row56 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row57" class="row_heading level0 row57" >57</th>
      <td id="T_c6ca4_row57_col0" class="data row57 col0" >encoder.layers.11.final_layer_norm.weight</td>
      <td id="T_c6ca4_row57_col1" class="data row57 col1" >encoder.layers.11.final_layer_norm.weight</td>
      <td id="T_c6ca4_row57_col2" class="data row57 col2" >av_romanizer.w2v_model.encoder.layers.11.final_layer_norm.weight</td>
      <td id="T_c6ca4_row57_col3" class="data row57 col3" >0.503181</td>
      <td id="T_c6ca4_row57_col4" class="data row57 col4" >7.29886</td>
      <td id="T_c6ca4_row57_col5" class="data row57 col5" >6.98785</td>
      <td id="T_c6ca4_row57_col6" class="data row57 col6" >0.0689396</td>
      <td id="T_c6ca4_row57_col7" class="data row57 col7" >0.998466</td>
      <td id="T_c6ca4_row57_col8" class="data row57 col8" >0.0456543</td>
      <td id="T_c6ca4_row57_col9" class="data row57 col9" >1024.000000</td>
      <td id="T_c6ca4_row57_col10" class="data row57 col10" >torch.float32</td>
      <td id="T_c6ca4_row57_col11" class="data row57 col11" >(1024,)</td>
      <td id="T_c6ca4_row57_col12" class="data row57 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row58" class="row_heading level0 row58" >58</th>
      <td id="T_c6ca4_row58_col0" class="data row58 col0" >encoder.layers.11.self_attn.k_proj.bias</td>
      <td id="T_c6ca4_row58_col1" class="data row58 col1" >encoder.layers.11.self_attn.k_proj.bias</td>
      <td id="T_c6ca4_row58_col2" class="data row58 col2" >av_romanizer.w2v_model.encoder.layers.11.self_attn.k_proj.bias</td>
      <td id="T_c6ca4_row58_col3" class="data row58 col3" >0.0154364</td>
      <td id="T_c6ca4_row58_col4" class="data row58 col4" >0.0812055</td>
      <td id="T_c6ca4_row58_col5" class="data row58 col5" >0.0789276</td>
      <td id="T_c6ca4_row58_col6" class="data row58 col6" >0.19009</td>
      <td id="T_c6ca4_row58_col7" class="data row58 col7" >0.981816</td>
      <td id="T_c6ca4_row58_col8" class="data row58 col8" >0.00263596</td>
      <td id="T_c6ca4_row58_col9" class="data row58 col9" >1024.000000</td>
      <td id="T_c6ca4_row58_col10" class="data row58 col10" >torch.float32</td>
      <td id="T_c6ca4_row58_col11" class="data row58 col11" >(1024,)</td>
      <td id="T_c6ca4_row58_col12" class="data row58 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row59" class="row_heading level0 row59" >59</th>
      <td id="T_c6ca4_row59_col0" class="data row59 col0" >encoder.layers.11.self_attn.k_proj.weight</td>
      <td id="T_c6ca4_row59_col1" class="data row59 col1" >encoder.layers.11.self_attn.k_proj.weight</td>
      <td id="T_c6ca4_row59_col2" class="data row59 col2" >av_romanizer.w2v_model.encoder.layers.11.self_attn.k_proj.weight</td>
      <td id="T_c6ca4_row59_col3" class="data row59 col3" >25.2052</td>
      <td id="T_c6ca4_row59_col4" class="data row59 col4" >147.957</td>
      <td id="T_c6ca4_row59_col5" class="data row59 col5" >141.843</td>
      <td id="T_c6ca4_row59_col6" class="data row59 col6" >0.170355</td>
      <td id="T_c6ca4_row59_col7" class="data row59 col7" >0.985755</td>
      <td id="T_c6ca4_row59_col8" class="data row59 col8" >0.171875</td>
      <td id="T_c6ca4_row59_col9" class="data row59 col9" >1048576.000000</td>
      <td id="T_c6ca4_row59_col10" class="data row59 col10" >torch.float32</td>
      <td id="T_c6ca4_row59_col11" class="data row59 col11" >(1024, 1024)</td>
      <td id="T_c6ca4_row59_col12" class="data row59 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row60" class="row_heading level0 row60" >60</th>
      <td id="T_c6ca4_row60_col0" class="data row60 col0" >encoder.layers.11.self_attn.out_proj.bias</td>
      <td id="T_c6ca4_row60_col1" class="data row60 col1" >encoder.layers.11.self_attn.out_proj.bias</td>
      <td id="T_c6ca4_row60_col2" class="data row60 col2" >av_romanizer.w2v_model.encoder.layers.11.self_attn.out_proj.bias</td>
      <td id="T_c6ca4_row60_col3" class="data row60 col3" >0.291032</td>
      <td id="T_c6ca4_row60_col4" class="data row60 col4" >3.74452</td>
      <td id="T_c6ca4_row60_col5" class="data row60 col5" >3.54166</td>
      <td id="T_c6ca4_row60_col6" class="data row60 col6" >0.077722</td>
      <td id="T_c6ca4_row60_col7" class="data row60 col7" >0.998358</td>
      <td id="T_c6ca4_row60_col8" class="data row60 col8" >0.0344238</td>
      <td id="T_c6ca4_row60_col9" class="data row60 col9" >1024.000000</td>
      <td id="T_c6ca4_row60_col10" class="data row60 col10" >torch.float32</td>
      <td id="T_c6ca4_row60_col11" class="data row60 col11" >(1024,)</td>
      <td id="T_c6ca4_row60_col12" class="data row60 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row61" class="row_heading level0 row61" >61</th>
      <td id="T_c6ca4_row61_col0" class="data row61 col0" >encoder.layers.11.self_attn.out_proj.weight</td>
      <td id="T_c6ca4_row61_col1" class="data row61 col1" >encoder.layers.11.self_attn.out_proj.weight</td>
      <td id="T_c6ca4_row61_col2" class="data row61 col2" >av_romanizer.w2v_model.encoder.layers.11.self_attn.out_proj.weight</td>
      <td id="T_c6ca4_row61_col3" class="data row61 col3" >26.7046</td>
      <td id="T_c6ca4_row61_col4" class="data row61 col4" >133.017</td>
      <td id="T_c6ca4_row61_col5" class="data row61 col5" >127.29</td>
      <td id="T_c6ca4_row61_col6" class="data row61 col6" >0.200761</td>
      <td id="T_c6ca4_row61_col7" class="data row61 col7" >0.979909</td>
      <td id="T_c6ca4_row61_col8" class="data row61 col8" >0.147705</td>
      <td id="T_c6ca4_row61_col9" class="data row61 col9" >1048576.000000</td>
      <td id="T_c6ca4_row61_col10" class="data row61 col10" >torch.float32</td>
      <td id="T_c6ca4_row61_col11" class="data row61 col11" >(1024, 1024)</td>
      <td id="T_c6ca4_row61_col12" class="data row61 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row62" class="row_heading level0 row62" >62</th>
      <td id="T_c6ca4_row62_col0" class="data row62 col0" >encoder.layers.11.self_attn.q_proj.bias</td>
      <td id="T_c6ca4_row62_col1" class="data row62 col1" >encoder.layers.11.self_attn.q_proj.bias</td>
      <td id="T_c6ca4_row62_col2" class="data row62 col2" >av_romanizer.w2v_model.encoder.layers.11.self_attn.q_proj.bias</td>
      <td id="T_c6ca4_row62_col3" class="data row62 col3" >0.857467</td>
      <td id="T_c6ca4_row62_col4" class="data row62 col4" >8.77676</td>
      <td id="T_c6ca4_row62_col5" class="data row62 col5" >8.40977</td>
      <td id="T_c6ca4_row62_col6" class="data row62 col6" >0.0976974</td>
      <td id="T_c6ca4_row62_col7" class="data row62 col7" >0.995932</td>
      <td id="T_c6ca4_row62_col8" class="data row62 col8" >0.106445</td>
      <td id="T_c6ca4_row62_col9" class="data row62 col9" >1024.000000</td>
      <td id="T_c6ca4_row62_col10" class="data row62 col10" >torch.float32</td>
      <td id="T_c6ca4_row62_col11" class="data row62 col11" >(1024,)</td>
      <td id="T_c6ca4_row62_col12" class="data row62 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row63" class="row_heading level0 row63" >63</th>
      <td id="T_c6ca4_row63_col0" class="data row63 col0" >encoder.layers.11.self_attn.q_proj.weight</td>
      <td id="T_c6ca4_row63_col1" class="data row63 col1" >encoder.layers.11.self_attn.q_proj.weight</td>
      <td id="T_c6ca4_row63_col2" class="data row63 col2" >av_romanizer.w2v_model.encoder.layers.11.self_attn.q_proj.weight</td>
      <td id="T_c6ca4_row63_col3" class="data row63 col3" >25.4476</td>
      <td id="T_c6ca4_row63_col4" class="data row63 col4" >147.156</td>
      <td id="T_c6ca4_row63_col5" class="data row63 col5" >141.135</td>
      <td id="T_c6ca4_row63_col6" class="data row63 col6" >0.172929</td>
      <td id="T_c6ca4_row63_col7" class="data row63 col7" >0.985283</td>
      <td id="T_c6ca4_row63_col8" class="data row63 col8" >0.160156</td>
      <td id="T_c6ca4_row63_col9" class="data row63 col9" >1048576.000000</td>
      <td id="T_c6ca4_row63_col10" class="data row63 col10" >torch.float32</td>
      <td id="T_c6ca4_row63_col11" class="data row63 col11" >(1024, 1024)</td>
      <td id="T_c6ca4_row63_col12" class="data row63 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row64" class="row_heading level0 row64" >64</th>
      <td id="T_c6ca4_row64_col0" class="data row64 col0" >encoder.layers.11.self_attn.v_proj.bias</td>
      <td id="T_c6ca4_row64_col1" class="data row64 col1" >encoder.layers.11.self_attn.v_proj.bias</td>
      <td id="T_c6ca4_row64_col2" class="data row64 col2" >av_romanizer.w2v_model.encoder.layers.11.self_attn.v_proj.bias</td>
      <td id="T_c6ca4_row64_col3" class="data row64 col3" >0.223442</td>
      <td id="T_c6ca4_row64_col4" class="data row64 col4" >1.38125</td>
      <td id="T_c6ca4_row64_col5" class="data row64 col5" >1.32887</td>
      <td id="T_c6ca4_row64_col6" class="data row64 col6" >0.161767</td>
      <td id="T_c6ca4_row64_col7" class="data row64 col7" >0.987147</td>
      <td id="T_c6ca4_row64_col8" class="data row64 col8" >0.0301056</td>
      <td id="T_c6ca4_row64_col9" class="data row64 col9" >1024.000000</td>
      <td id="T_c6ca4_row64_col10" class="data row64 col10" >torch.float32</td>
      <td id="T_c6ca4_row64_col11" class="data row64 col11" >(1024,)</td>
      <td id="T_c6ca4_row64_col12" class="data row64 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row65" class="row_heading level0 row65" >65</th>
      <td id="T_c6ca4_row65_col0" class="data row65 col0" >encoder.layers.11.self_attn.v_proj.weight</td>
      <td id="T_c6ca4_row65_col1" class="data row65 col1" >encoder.layers.11.self_attn.v_proj.weight</td>
      <td id="T_c6ca4_row65_col2" class="data row65 col2" >av_romanizer.w2v_model.encoder.layers.11.self_attn.v_proj.weight</td>
      <td id="T_c6ca4_row65_col3" class="data row65 col3" >23.6593</td>
      <td id="T_c6ca4_row65_col4" class="data row65 col4" >137.413</td>
      <td id="T_c6ca4_row65_col5" class="data row65 col5" >131.512</td>
      <td id="T_c6ca4_row65_col6" class="data row65 col6" >0.172176</td>
      <td id="T_c6ca4_row65_col7" class="data row65 col7" >0.985476</td>
      <td id="T_c6ca4_row65_col8" class="data row65 col8" >0.13501</td>
      <td id="T_c6ca4_row65_col9" class="data row65 col9" >1048576.000000</td>
      <td id="T_c6ca4_row65_col10" class="data row65 col10" >torch.float32</td>
      <td id="T_c6ca4_row65_col11" class="data row65 col11" >(1024, 1024)</td>
      <td id="T_c6ca4_row65_col12" class="data row65 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row66" class="row_heading level0 row66" >66</th>
      <td id="T_c6ca4_row66_col0" class="data row66 col0" >encoder.layers.11.self_attn_layer_norm.bias</td>
      <td id="T_c6ca4_row66_col1" class="data row66 col1" >encoder.layers.11.self_attn_layer_norm.bias</td>
      <td id="T_c6ca4_row66_col2" class="data row66 col2" >av_romanizer.w2v_model.encoder.layers.11.self_attn_layer_norm.bias</td>
      <td id="T_c6ca4_row66_col3" class="data row66 col3" >0.164483</td>
      <td id="T_c6ca4_row66_col4" class="data row66 col4" >0.75115</td>
      <td id="T_c6ca4_row66_col5" class="data row66 col5" >0.724463</td>
      <td id="T_c6ca4_row66_col6" class="data row66 col6" >0.218975</td>
      <td id="T_c6ca4_row66_col7" class="data row66 col7" >0.975796</td>
      <td id="T_c6ca4_row66_col8" class="data row66 col8" >0.0220947</td>
      <td id="T_c6ca4_row66_col9" class="data row66 col9" >1024.000000</td>
      <td id="T_c6ca4_row66_col10" class="data row66 col10" >torch.float32</td>
      <td id="T_c6ca4_row66_col11" class="data row66 col11" >(1024,)</td>
      <td id="T_c6ca4_row66_col12" class="data row66 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row67" class="row_heading level0 row67" >67</th>
      <td id="T_c6ca4_row67_col0" class="data row67 col0" >encoder.layers.11.self_attn_layer_norm.weight</td>
      <td id="T_c6ca4_row67_col1" class="data row67 col1" >encoder.layers.11.self_attn_layer_norm.weight</td>
      <td id="T_c6ca4_row67_col2" class="data row67 col2" >av_romanizer.w2v_model.encoder.layers.11.self_attn_layer_norm.weight</td>
      <td id="T_c6ca4_row67_col3" class="data row67 col3" >0.412263</td>
      <td id="T_c6ca4_row67_col4" class="data row67 col4" >6.15842</td>
      <td id="T_c6ca4_row67_col5" class="data row67 col5" >6.14308</td>
      <td id="T_c6ca4_row67_col6" class="data row67 col6" >0.066943</td>
      <td id="T_c6ca4_row67_col7" class="data row67 col7" >0.997757</td>
      <td id="T_c6ca4_row67_col8" class="data row67 col8" >0.064209</td>
      <td id="T_c6ca4_row67_col9" class="data row67 col9" >1024.000000</td>
      <td id="T_c6ca4_row67_col10" class="data row67 col10" >torch.float32</td>
      <td id="T_c6ca4_row67_col11" class="data row67 col11" >(1024,)</td>
      <td id="T_c6ca4_row67_col12" class="data row67 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row68" class="row_heading level0 row68" >68</th>
      <td id="T_c6ca4_row68_col0" class="data row68 col0" >encoder.layers.12.fc1.bias</td>
      <td id="T_c6ca4_row68_col1" class="data row68 col1" >encoder.layers.12.fc1.bias</td>
      <td id="T_c6ca4_row68_col2" class="data row68 col2" >av_romanizer.w2v_model.encoder.layers.12.fc1.bias</td>
      <td id="T_c6ca4_row68_col3" class="data row68 col3" >0.689205</td>
      <td id="T_c6ca4_row68_col4" class="data row68 col4" >3.55896</td>
      <td id="T_c6ca4_row68_col5" class="data row68 col5" >3.50668</td>
      <td id="T_c6ca4_row68_col6" class="data row68 col6" >0.193653</td>
      <td id="T_c6ca4_row68_col7" class="data row68 col7" >0.981079</td>
      <td id="T_c6ca4_row68_col8" class="data row68 col8" >0.0620728</td>
      <td id="T_c6ca4_row68_col9" class="data row68 col9" >4096.000000</td>
      <td id="T_c6ca4_row68_col10" class="data row68 col10" >torch.float32</td>
      <td id="T_c6ca4_row68_col11" class="data row68 col11" >(4096,)</td>
      <td id="T_c6ca4_row68_col12" class="data row68 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row69" class="row_heading level0 row69" >69</th>
      <td id="T_c6ca4_row69_col0" class="data row69 col0" >encoder.layers.12.fc1.weight</td>
      <td id="T_c6ca4_row69_col1" class="data row69 col1" >encoder.layers.12.fc1.weight</td>
      <td id="T_c6ca4_row69_col2" class="data row69 col2" >av_romanizer.w2v_model.encoder.layers.12.fc1.weight</td>
      <td id="T_c6ca4_row69_col3" class="data row69 col3" >58.1551</td>
      <td id="T_c6ca4_row69_col4" class="data row69 col4" >299.419</td>
      <td id="T_c6ca4_row69_col5" class="data row69 col5" >286.428</td>
      <td id="T_c6ca4_row69_col6" class="data row69 col6" >0.194227</td>
      <td id="T_c6ca4_row69_col7" class="data row69 col7" >0.981266</td>
      <td id="T_c6ca4_row69_col8" class="data row69 col8" >0.219482</td>
      <td id="T_c6ca4_row69_col9" class="data row69 col9" >4194304.000000</td>
      <td id="T_c6ca4_row69_col10" class="data row69 col10" >torch.float32</td>
      <td id="T_c6ca4_row69_col11" class="data row69 col11" >(4096, 1024)</td>
      <td id="T_c6ca4_row69_col12" class="data row69 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row70" class="row_heading level0 row70" >70</th>
      <td id="T_c6ca4_row70_col0" class="data row70 col0" >encoder.layers.12.fc2.bias</td>
      <td id="T_c6ca4_row70_col1" class="data row70 col1" >encoder.layers.12.fc2.bias</td>
      <td id="T_c6ca4_row70_col2" class="data row70 col2" >av_romanizer.w2v_model.encoder.layers.12.fc2.bias</td>
      <td id="T_c6ca4_row70_col3" class="data row70 col3" >0.265121</td>
      <td id="T_c6ca4_row70_col4" class="data row70 col4" >2.75139</td>
      <td id="T_c6ca4_row70_col5" class="data row70 col5" >2.59313</td>
      <td id="T_c6ca4_row70_col6" class="data row70 col6" >0.096359</td>
      <td id="T_c6ca4_row70_col7" class="data row70 col7" >0.996829</td>
      <td id="T_c6ca4_row70_col8" class="data row70 col8" >0.0256348</td>
      <td id="T_c6ca4_row70_col9" class="data row70 col9" >1024.000000</td>
      <td id="T_c6ca4_row70_col10" class="data row70 col10" >torch.float32</td>
      <td id="T_c6ca4_row70_col11" class="data row70 col11" >(1024,)</td>
      <td id="T_c6ca4_row70_col12" class="data row70 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row71" class="row_heading level0 row71" >71</th>
      <td id="T_c6ca4_row71_col0" class="data row71 col0" >encoder.layers.12.fc2.weight</td>
      <td id="T_c6ca4_row71_col1" class="data row71 col1" >encoder.layers.12.fc2.weight</td>
      <td id="T_c6ca4_row71_col2" class="data row71 col2" >av_romanizer.w2v_model.encoder.layers.12.fc2.weight</td>
      <td id="T_c6ca4_row71_col3" class="data row71 col3" >57.5986</td>
      <td id="T_c6ca4_row71_col4" class="data row71 col4" >309.384</td>
      <td id="T_c6ca4_row71_col5" class="data row71 col5" >295.758</td>
      <td id="T_c6ca4_row71_col6" class="data row71 col6" >0.186171</td>
      <td id="T_c6ca4_row71_col7" class="data row71 col7" >0.982886</td>
      <td id="T_c6ca4_row71_col8" class="data row71 col8" >0.188232</td>
      <td id="T_c6ca4_row71_col9" class="data row71 col9" >4194304.000000</td>
      <td id="T_c6ca4_row71_col10" class="data row71 col10" >torch.float32</td>
      <td id="T_c6ca4_row71_col11" class="data row71 col11" >(1024, 4096)</td>
      <td id="T_c6ca4_row71_col12" class="data row71 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row72" class="row_heading level0 row72" >72</th>
      <td id="T_c6ca4_row72_col0" class="data row72 col0" >encoder.layers.12.final_layer_norm.bias</td>
      <td id="T_c6ca4_row72_col1" class="data row72 col1" >encoder.layers.12.final_layer_norm.bias</td>
      <td id="T_c6ca4_row72_col2" class="data row72 col2" >av_romanizer.w2v_model.encoder.layers.12.final_layer_norm.bias</td>
      <td id="T_c6ca4_row72_col3" class="data row72 col3" >0.273097</td>
      <td id="T_c6ca4_row72_col4" class="data row72 col4" >2.0698</td>
      <td id="T_c6ca4_row72_col5" class="data row72 col5" >1.96632</td>
      <td id="T_c6ca4_row72_col6" class="data row72 col6" >0.131943</td>
      <td id="T_c6ca4_row72_col7" class="data row72 col7" >0.992153</td>
      <td id="T_c6ca4_row72_col8" class="data row72 col8" >0.0297394</td>
      <td id="T_c6ca4_row72_col9" class="data row72 col9" >1024.000000</td>
      <td id="T_c6ca4_row72_col10" class="data row72 col10" >torch.float32</td>
      <td id="T_c6ca4_row72_col11" class="data row72 col11" >(1024,)</td>
      <td id="T_c6ca4_row72_col12" class="data row72 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row73" class="row_heading level0 row73" >73</th>
      <td id="T_c6ca4_row73_col0" class="data row73 col0" >encoder.layers.12.final_layer_norm.weight</td>
      <td id="T_c6ca4_row73_col1" class="data row73 col1" >encoder.layers.12.final_layer_norm.weight</td>
      <td id="T_c6ca4_row73_col2" class="data row73 col2" >av_romanizer.w2v_model.encoder.layers.12.final_layer_norm.weight</td>
      <td id="T_c6ca4_row73_col3" class="data row73 col3" >0.490496</td>
      <td id="T_c6ca4_row73_col4" class="data row73 col4" >7.67552</td>
      <td id="T_c6ca4_row73_col5" class="data row73 col5" >7.45097</td>
      <td id="T_c6ca4_row73_col6" class="data row73 col6" >0.0639039</td>
      <td id="T_c6ca4_row73_col7" class="data row73 col7" >0.998337</td>
      <td id="T_c6ca4_row73_col8" class="data row73 col8" >0.0466309</td>
      <td id="T_c6ca4_row73_col9" class="data row73 col9" >1024.000000</td>
      <td id="T_c6ca4_row73_col10" class="data row73 col10" >torch.float32</td>
      <td id="T_c6ca4_row73_col11" class="data row73 col11" >(1024,)</td>
      <td id="T_c6ca4_row73_col12" class="data row73 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row74" class="row_heading level0 row74" >74</th>
      <td id="T_c6ca4_row74_col0" class="data row74 col0" >encoder.layers.12.self_attn.k_proj.bias</td>
      <td id="T_c6ca4_row74_col1" class="data row74 col1" >encoder.layers.12.self_attn.k_proj.bias</td>
      <td id="T_c6ca4_row74_col2" class="data row74 col2" >av_romanizer.w2v_model.encoder.layers.12.self_attn.k_proj.bias</td>
      <td id="T_c6ca4_row74_col3" class="data row74 col3" >0.017171</td>
      <td id="T_c6ca4_row74_col4" class="data row74 col4" >0.0787755</td>
      <td id="T_c6ca4_row74_col5" class="data row74 col5" >0.0776956</td>
      <td id="T_c6ca4_row74_col6" class="data row74 col6" >0.217974</td>
      <td id="T_c6ca4_row74_col7" class="data row74 col7" >0.976009</td>
      <td id="T_c6ca4_row74_col8" class="data row74 col8" >0.00306988</td>
      <td id="T_c6ca4_row74_col9" class="data row74 col9" >1024.000000</td>
      <td id="T_c6ca4_row74_col10" class="data row74 col10" >torch.float32</td>
      <td id="T_c6ca4_row74_col11" class="data row74 col11" >(1024,)</td>
      <td id="T_c6ca4_row74_col12" class="data row74 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row75" class="row_heading level0 row75" >75</th>
      <td id="T_c6ca4_row75_col0" class="data row75 col0" >encoder.layers.12.self_attn.k_proj.weight</td>
      <td id="T_c6ca4_row75_col1" class="data row75 col1" >encoder.layers.12.self_attn.k_proj.weight</td>
      <td id="T_c6ca4_row75_col2" class="data row75 col2" >av_romanizer.w2v_model.encoder.layers.12.self_attn.k_proj.weight</td>
      <td id="T_c6ca4_row75_col3" class="data row75 col3" >26.1364</td>
      <td id="T_c6ca4_row75_col4" class="data row75 col4" >147.672</td>
      <td id="T_c6ca4_row75_col5" class="data row75 col5" >141.925</td>
      <td id="T_c6ca4_row75_col6" class="data row75 col6" >0.176989</td>
      <td id="T_c6ca4_row75_col7" class="data row75 col7" >0.984491</td>
      <td id="T_c6ca4_row75_col8" class="data row75 col8" >0.152954</td>
      <td id="T_c6ca4_row75_col9" class="data row75 col9" >1048576.000000</td>
      <td id="T_c6ca4_row75_col10" class="data row75 col10" >torch.float32</td>
      <td id="T_c6ca4_row75_col11" class="data row75 col11" >(1024, 1024)</td>
      <td id="T_c6ca4_row75_col12" class="data row75 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row76" class="row_heading level0 row76" >76</th>
      <td id="T_c6ca4_row76_col0" class="data row76 col0" >encoder.layers.12.self_attn.out_proj.bias</td>
      <td id="T_c6ca4_row76_col1" class="data row76 col1" >encoder.layers.12.self_attn.out_proj.bias</td>
      <td id="T_c6ca4_row76_col2" class="data row76 col2" >av_romanizer.w2v_model.encoder.layers.12.self_attn.out_proj.bias</td>
      <td id="T_c6ca4_row76_col3" class="data row76 col3" >0.321745</td>
      <td id="T_c6ca4_row76_col4" class="data row76 col4" >4.35684</td>
      <td id="T_c6ca4_row76_col5" class="data row76 col5" >4.11367</td>
      <td id="T_c6ca4_row76_col6" class="data row76 col6" >0.0738483</td>
      <td id="T_c6ca4_row76_col7" class="data row76 col7" >0.998762</td>
      <td id="T_c6ca4_row76_col8" class="data row76 col8" >0.0427246</td>
      <td id="T_c6ca4_row76_col9" class="data row76 col9" >1024.000000</td>
      <td id="T_c6ca4_row76_col10" class="data row76 col10" >torch.float32</td>
      <td id="T_c6ca4_row76_col11" class="data row76 col11" >(1024,)</td>
      <td id="T_c6ca4_row76_col12" class="data row76 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row77" class="row_heading level0 row77" >77</th>
      <td id="T_c6ca4_row77_col0" class="data row77 col0" >encoder.layers.12.self_attn.out_proj.weight</td>
      <td id="T_c6ca4_row77_col1" class="data row77 col1" >encoder.layers.12.self_attn.out_proj.weight</td>
      <td id="T_c6ca4_row77_col2" class="data row77 col2" >av_romanizer.w2v_model.encoder.layers.12.self_attn.out_proj.weight</td>
      <td id="T_c6ca4_row77_col3" class="data row77 col3" >26.6244</td>
      <td id="T_c6ca4_row77_col4" class="data row77 col4" >133.979</td>
      <td id="T_c6ca4_row77_col5" class="data row77 col5" >128.025</td>
      <td id="T_c6ca4_row77_col6" class="data row77 col6" >0.198721</td>
      <td id="T_c6ca4_row77_col7" class="data row77 col7" >0.98037</td>
      <td id="T_c6ca4_row77_col8" class="data row77 col8" >0.138184</td>
      <td id="T_c6ca4_row77_col9" class="data row77 col9" >1048576.000000</td>
      <td id="T_c6ca4_row77_col10" class="data row77 col10" >torch.float32</td>
      <td id="T_c6ca4_row77_col11" class="data row77 col11" >(1024, 1024)</td>
      <td id="T_c6ca4_row77_col12" class="data row77 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row78" class="row_heading level0 row78" >78</th>
      <td id="T_c6ca4_row78_col0" class="data row78 col0" >encoder.layers.12.self_attn.q_proj.bias</td>
      <td id="T_c6ca4_row78_col1" class="data row78 col1" >encoder.layers.12.self_attn.q_proj.bias</td>
      <td id="T_c6ca4_row78_col2" class="data row78 col2" >av_romanizer.w2v_model.encoder.layers.12.self_attn.q_proj.bias</td>
      <td id="T_c6ca4_row78_col3" class="data row78 col3" >0.897232</td>
      <td id="T_c6ca4_row78_col4" class="data row78 col4" >9.35129</td>
      <td id="T_c6ca4_row78_col5" class="data row78 col5" >8.97557</td>
      <td id="T_c6ca4_row78_col6" class="data row78 col6" >0.0959475</td>
      <td id="T_c6ca4_row78_col7" class="data row78 col7" >0.996045</td>
      <td id="T_c6ca4_row78_col8" class="data row78 col8" >0.123047</td>
      <td id="T_c6ca4_row78_col9" class="data row78 col9" >1024.000000</td>
      <td id="T_c6ca4_row78_col10" class="data row78 col10" >torch.float32</td>
      <td id="T_c6ca4_row78_col11" class="data row78 col11" >(1024,)</td>
      <td id="T_c6ca4_row78_col12" class="data row78 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row79" class="row_heading level0 row79" >79</th>
      <td id="T_c6ca4_row79_col0" class="data row79 col0" >encoder.layers.12.self_attn.q_proj.weight</td>
      <td id="T_c6ca4_row79_col1" class="data row79 col1" >encoder.layers.12.self_attn.q_proj.weight</td>
      <td id="T_c6ca4_row79_col2" class="data row79 col2" >av_romanizer.w2v_model.encoder.layers.12.self_attn.q_proj.weight</td>
      <td id="T_c6ca4_row79_col3" class="data row79 col3" >26.5821</td>
      <td id="T_c6ca4_row79_col4" class="data row79 col4" >146.944</td>
      <td id="T_c6ca4_row79_col5" class="data row79 col5" >141.301</td>
      <td id="T_c6ca4_row79_col6" class="data row79 col6" >0.1809</td>
      <td id="T_c6ca4_row79_col7" class="data row79 col7" >0.983751</td>
      <td id="T_c6ca4_row79_col8" class="data row79 col8" >0.148438</td>
      <td id="T_c6ca4_row79_col9" class="data row79 col9" >1048576.000000</td>
      <td id="T_c6ca4_row79_col10" class="data row79 col10" >torch.float32</td>
      <td id="T_c6ca4_row79_col11" class="data row79 col11" >(1024, 1024)</td>
      <td id="T_c6ca4_row79_col12" class="data row79 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row80" class="row_heading level0 row80" >80</th>
      <td id="T_c6ca4_row80_col0" class="data row80 col0" >encoder.layers.12.self_attn.v_proj.bias</td>
      <td id="T_c6ca4_row80_col1" class="data row80 col1" >encoder.layers.12.self_attn.v_proj.bias</td>
      <td id="T_c6ca4_row80_col2" class="data row80 col2" >av_romanizer.w2v_model.encoder.layers.12.self_attn.v_proj.bias</td>
      <td id="T_c6ca4_row80_col3" class="data row80 col3" >0.226166</td>
      <td id="T_c6ca4_row80_col4" class="data row80 col4" >1.48299</td>
      <td id="T_c6ca4_row80_col5" class="data row80 col5" >1.41961</td>
      <td id="T_c6ca4_row80_col6" class="data row80 col6" >0.152507</td>
      <td id="T_c6ca4_row80_col7" class="data row80 col7" >0.988806</td>
      <td id="T_c6ca4_row80_col8" class="data row80 col8" >0.0239868</td>
      <td id="T_c6ca4_row80_col9" class="data row80 col9" >1024.000000</td>
      <td id="T_c6ca4_row80_col10" class="data row80 col10" >torch.float32</td>
      <td id="T_c6ca4_row80_col11" class="data row80 col11" >(1024,)</td>
      <td id="T_c6ca4_row80_col12" class="data row80 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row81" class="row_heading level0 row81" >81</th>
      <td id="T_c6ca4_row81_col0" class="data row81 col0" >encoder.layers.12.self_attn.v_proj.weight</td>
      <td id="T_c6ca4_row81_col1" class="data row81 col1" >encoder.layers.12.self_attn.v_proj.weight</td>
      <td id="T_c6ca4_row81_col2" class="data row81 col2" >av_romanizer.w2v_model.encoder.layers.12.self_attn.v_proj.weight</td>
      <td id="T_c6ca4_row81_col3" class="data row81 col3" >23.5554</td>
      <td id="T_c6ca4_row81_col4" class="data row81 col4" >138.275</td>
      <td id="T_c6ca4_row81_col5" class="data row81 col5" >132.17</td>
      <td id="T_c6ca4_row81_col6" class="data row81 col6" >0.170352</td>
      <td id="T_c6ca4_row81_col7" class="data row81 col7" >0.98584</td>
      <td id="T_c6ca4_row81_col8" class="data row81 col8" >0.128269</td>
      <td id="T_c6ca4_row81_col9" class="data row81 col9" >1048576.000000</td>
      <td id="T_c6ca4_row81_col10" class="data row81 col10" >torch.float32</td>
      <td id="T_c6ca4_row81_col11" class="data row81 col11" >(1024, 1024)</td>
      <td id="T_c6ca4_row81_col12" class="data row81 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row82" class="row_heading level0 row82" >82</th>
      <td id="T_c6ca4_row82_col0" class="data row82 col0" >encoder.layers.12.self_attn_layer_norm.bias</td>
      <td id="T_c6ca4_row82_col1" class="data row82 col1" >encoder.layers.12.self_attn_layer_norm.bias</td>
      <td id="T_c6ca4_row82_col2" class="data row82 col2" >av_romanizer.w2v_model.encoder.layers.12.self_attn_layer_norm.bias</td>
      <td id="T_c6ca4_row82_col3" class="data row82 col3" >0.189834</td>
      <td id="T_c6ca4_row82_col4" class="data row82 col4" >0.803943</td>
      <td id="T_c6ca4_row82_col5" class="data row82 col5" >0.781503</td>
      <td id="T_c6ca4_row82_col6" class="data row82 col6" >0.236128</td>
      <td id="T_c6ca4_row82_col7" class="data row82 col7" >0.971722</td>
      <td id="T_c6ca4_row82_col8" class="data row82 col8" >0.0437012</td>
      <td id="T_c6ca4_row82_col9" class="data row82 col9" >1024.000000</td>
      <td id="T_c6ca4_row82_col10" class="data row82 col10" >torch.float32</td>
      <td id="T_c6ca4_row82_col11" class="data row82 col11" >(1024,)</td>
      <td id="T_c6ca4_row82_col12" class="data row82 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row83" class="row_heading level0 row83" >83</th>
      <td id="T_c6ca4_row83_col0" class="data row83 col0" >encoder.layers.12.self_attn_layer_norm.weight</td>
      <td id="T_c6ca4_row83_col1" class="data row83 col1" >encoder.layers.12.self_attn_layer_norm.weight</td>
      <td id="T_c6ca4_row83_col2" class="data row83 col2" >av_romanizer.w2v_model.encoder.layers.12.self_attn_layer_norm.weight</td>
      <td id="T_c6ca4_row83_col3" class="data row83 col3" >0.40711</td>
      <td id="T_c6ca4_row83_col4" class="data row83 col4" >6.13626</td>
      <td id="T_c6ca4_row83_col5" class="data row83 col5" >6.24438</td>
      <td id="T_c6ca4_row83_col6" class="data row83 col6" >0.0663451</td>
      <td id="T_c6ca4_row83_col7" class="data row83 col7" >0.99799</td>
      <td id="T_c6ca4_row83_col8" class="data row83 col8" >0.0482178</td>
      <td id="T_c6ca4_row83_col9" class="data row83 col9" >1024.000000</td>
      <td id="T_c6ca4_row83_col10" class="data row83 col10" >torch.float32</td>
      <td id="T_c6ca4_row83_col11" class="data row83 col11" >(1024,)</td>
      <td id="T_c6ca4_row83_col12" class="data row83 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row84" class="row_heading level0 row84" >84</th>
      <td id="T_c6ca4_row84_col0" class="data row84 col0" >encoder.layers.13.fc1.bias</td>
      <td id="T_c6ca4_row84_col1" class="data row84 col1" >encoder.layers.13.fc1.bias</td>
      <td id="T_c6ca4_row84_col2" class="data row84 col2" >av_romanizer.w2v_model.encoder.layers.13.fc1.bias</td>
      <td id="T_c6ca4_row84_col3" class="data row84 col3" >0.710107</td>
      <td id="T_c6ca4_row84_col4" class="data row84 col4" >3.50088</td>
      <td id="T_c6ca4_row84_col5" class="data row84 col5" >3.45665</td>
      <td id="T_c6ca4_row84_col6" class="data row84 col6" >0.202836</td>
      <td id="T_c6ca4_row84_col7" class="data row84 col7" >0.979246</td>
      <td id="T_c6ca4_row84_col8" class="data row84 col8" >0.0669556</td>
      <td id="T_c6ca4_row84_col9" class="data row84 col9" >4096.000000</td>
      <td id="T_c6ca4_row84_col10" class="data row84 col10" >torch.float32</td>
      <td id="T_c6ca4_row84_col11" class="data row84 col11" >(4096,)</td>
      <td id="T_c6ca4_row84_col12" class="data row84 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row85" class="row_heading level0 row85" >85</th>
      <td id="T_c6ca4_row85_col0" class="data row85 col0" >encoder.layers.13.fc1.weight</td>
      <td id="T_c6ca4_row85_col1" class="data row85 col1" >encoder.layers.13.fc1.weight</td>
      <td id="T_c6ca4_row85_col2" class="data row85 col2" >av_romanizer.w2v_model.encoder.layers.13.fc1.weight</td>
      <td id="T_c6ca4_row85_col3" class="data row85 col3" >60.4571</td>
      <td id="T_c6ca4_row85_col4" class="data row85 col4" >301.499</td>
      <td id="T_c6ca4_row85_col5" class="data row85 col5" >288.631</td>
      <td id="T_c6ca4_row85_col6" class="data row85 col6" >0.200522</td>
      <td id="T_c6ca4_row85_col7" class="data row85 col7" >0.97995</td>
      <td id="T_c6ca4_row85_col8" class="data row85 col8" >0.210327</td>
      <td id="T_c6ca4_row85_col9" class="data row85 col9" >4194304.000000</td>
      <td id="T_c6ca4_row85_col10" class="data row85 col10" >torch.float32</td>
      <td id="T_c6ca4_row85_col11" class="data row85 col11" >(4096, 1024)</td>
      <td id="T_c6ca4_row85_col12" class="data row85 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row86" class="row_heading level0 row86" >86</th>
      <td id="T_c6ca4_row86_col0" class="data row86 col0" >encoder.layers.13.fc2.bias</td>
      <td id="T_c6ca4_row86_col1" class="data row86 col1" >encoder.layers.13.fc2.bias</td>
      <td id="T_c6ca4_row86_col2" class="data row86 col2" >av_romanizer.w2v_model.encoder.layers.13.fc2.bias</td>
      <td id="T_c6ca4_row86_col3" class="data row86 col3" >0.299228</td>
      <td id="T_c6ca4_row86_col4" class="data row86 col4" >2.80802</td>
      <td id="T_c6ca4_row86_col5" class="data row86 col5" >2.65965</td>
      <td id="T_c6ca4_row86_col6" class="data row86 col6" >0.106562</td>
      <td id="T_c6ca4_row86_col7" class="data row86 col7" >0.995479</td>
      <td id="T_c6ca4_row86_col8" class="data row86 col8" >0.0316162</td>
      <td id="T_c6ca4_row86_col9" class="data row86 col9" >1024.000000</td>
      <td id="T_c6ca4_row86_col10" class="data row86 col10" >torch.float32</td>
      <td id="T_c6ca4_row86_col11" class="data row86 col11" >(1024,)</td>
      <td id="T_c6ca4_row86_col12" class="data row86 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row87" class="row_heading level0 row87" >87</th>
      <td id="T_c6ca4_row87_col0" class="data row87 col0" >encoder.layers.13.fc2.weight</td>
      <td id="T_c6ca4_row87_col1" class="data row87 col1" >encoder.layers.13.fc2.weight</td>
      <td id="T_c6ca4_row87_col2" class="data row87 col2" >av_romanizer.w2v_model.encoder.layers.13.fc2.weight</td>
      <td id="T_c6ca4_row87_col3" class="data row87 col3" >60.6067</td>
      <td id="T_c6ca4_row87_col4" class="data row87 col4" >314.502</td>
      <td id="T_c6ca4_row87_col5" class="data row87 col5" >301.057</td>
      <td id="T_c6ca4_row87_col6" class="data row87 col6" >0.192707</td>
      <td id="T_c6ca4_row87_col7" class="data row87 col7" >0.981557</td>
      <td id="T_c6ca4_row87_col8" class="data row87 col8" >0.198303</td>
      <td id="T_c6ca4_row87_col9" class="data row87 col9" >4194304.000000</td>
      <td id="T_c6ca4_row87_col10" class="data row87 col10" >torch.float32</td>
      <td id="T_c6ca4_row87_col11" class="data row87 col11" >(1024, 4096)</td>
      <td id="T_c6ca4_row87_col12" class="data row87 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row88" class="row_heading level0 row88" >88</th>
      <td id="T_c6ca4_row88_col0" class="data row88 col0" >encoder.layers.13.final_layer_norm.bias</td>
      <td id="T_c6ca4_row88_col1" class="data row88 col1" >encoder.layers.13.final_layer_norm.bias</td>
      <td id="T_c6ca4_row88_col2" class="data row88 col2" >av_romanizer.w2v_model.encoder.layers.13.final_layer_norm.bias</td>
      <td id="T_c6ca4_row88_col3" class="data row88 col3" >0.25246</td>
      <td id="T_c6ca4_row88_col4" class="data row88 col4" >2.28514</td>
      <td id="T_c6ca4_row88_col5" class="data row88 col5" >2.19959</td>
      <td id="T_c6ca4_row88_col6" class="data row88 col6" >0.110479</td>
      <td id="T_c6ca4_row88_col7" class="data row88 col7" >0.994388</td>
      <td id="T_c6ca4_row88_col8" class="data row88 col8" >0.0318727</td>
      <td id="T_c6ca4_row88_col9" class="data row88 col9" >1024.000000</td>
      <td id="T_c6ca4_row88_col10" class="data row88 col10" >torch.float32</td>
      <td id="T_c6ca4_row88_col11" class="data row88 col11" >(1024,)</td>
      <td id="T_c6ca4_row88_col12" class="data row88 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row89" class="row_heading level0 row89" >89</th>
      <td id="T_c6ca4_row89_col0" class="data row89 col0" >encoder.layers.13.final_layer_norm.weight</td>
      <td id="T_c6ca4_row89_col1" class="data row89 col1" >encoder.layers.13.final_layer_norm.weight</td>
      <td id="T_c6ca4_row89_col2" class="data row89 col2" >av_romanizer.w2v_model.encoder.layers.13.final_layer_norm.weight</td>
      <td id="T_c6ca4_row89_col3" class="data row89 col3" >0.502591</td>
      <td id="T_c6ca4_row89_col4" class="data row89 col4" >8.44529</td>
      <td id="T_c6ca4_row89_col5" class="data row89 col5" >8.36109</td>
      <td id="T_c6ca4_row89_col6" class="data row89 col6" >0.0595114</td>
      <td id="T_c6ca4_row89_col7" class="data row89 col7" >0.998262</td>
      <td id="T_c6ca4_row89_col8" class="data row89 col8" >0.0537109</td>
      <td id="T_c6ca4_row89_col9" class="data row89 col9" >1024.000000</td>
      <td id="T_c6ca4_row89_col10" class="data row89 col10" >torch.float32</td>
      <td id="T_c6ca4_row89_col11" class="data row89 col11" >(1024,)</td>
      <td id="T_c6ca4_row89_col12" class="data row89 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row90" class="row_heading level0 row90" >90</th>
      <td id="T_c6ca4_row90_col0" class="data row90 col0" >encoder.layers.13.self_attn.k_proj.bias</td>
      <td id="T_c6ca4_row90_col1" class="data row90 col1" >encoder.layers.13.self_attn.k_proj.bias</td>
      <td id="T_c6ca4_row90_col2" class="data row90 col2" >av_romanizer.w2v_model.encoder.layers.13.self_attn.k_proj.bias</td>
      <td id="T_c6ca4_row90_col3" class="data row90 col3" >0.017252</td>
      <td id="T_c6ca4_row90_col4" class="data row90 col4" >0.0838876</td>
      <td id="T_c6ca4_row90_col5" class="data row90 col5" >0.0817587</td>
      <td id="T_c6ca4_row90_col6" class="data row90 col6" >0.205657</td>
      <td id="T_c6ca4_row90_col7" class="data row90 col7" >0.978632</td>
      <td id="T_c6ca4_row90_col8" class="data row90 col8" >0.00316811</td>
      <td id="T_c6ca4_row90_col9" class="data row90 col9" >1024.000000</td>
      <td id="T_c6ca4_row90_col10" class="data row90 col10" >torch.float32</td>
      <td id="T_c6ca4_row90_col11" class="data row90 col11" >(1024,)</td>
      <td id="T_c6ca4_row90_col12" class="data row90 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row91" class="row_heading level0 row91" >91</th>
      <td id="T_c6ca4_row91_col0" class="data row91 col0" >encoder.layers.13.self_attn.k_proj.weight</td>
      <td id="T_c6ca4_row91_col1" class="data row91 col1" >encoder.layers.13.self_attn.k_proj.weight</td>
      <td id="T_c6ca4_row91_col2" class="data row91 col2" >av_romanizer.w2v_model.encoder.layers.13.self_attn.k_proj.weight</td>
      <td id="T_c6ca4_row91_col3" class="data row91 col3" >26.5376</td>
      <td id="T_c6ca4_row91_col4" class="data row91 col4" >147.547</td>
      <td id="T_c6ca4_row91_col5" class="data row91 col5" >141.573</td>
      <td id="T_c6ca4_row91_col6" class="data row91 col6" >0.179859</td>
      <td id="T_c6ca4_row91_col7" class="data row91 col7" >0.983997</td>
      <td id="T_c6ca4_row91_col8" class="data row91 col8" >0.149902</td>
      <td id="T_c6ca4_row91_col9" class="data row91 col9" >1048576.000000</td>
      <td id="T_c6ca4_row91_col10" class="data row91 col10" >torch.float32</td>
      <td id="T_c6ca4_row91_col11" class="data row91 col11" >(1024, 1024)</td>
      <td id="T_c6ca4_row91_col12" class="data row91 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row92" class="row_heading level0 row92" >92</th>
      <td id="T_c6ca4_row92_col0" class="data row92 col0" >encoder.layers.13.self_attn.out_proj.bias</td>
      <td id="T_c6ca4_row92_col1" class="data row92 col1" >encoder.layers.13.self_attn.out_proj.bias</td>
      <td id="T_c6ca4_row92_col2" class="data row92 col2" >av_romanizer.w2v_model.encoder.layers.13.self_attn.out_proj.bias</td>
      <td id="T_c6ca4_row92_col3" class="data row92 col3" >0.316337</td>
      <td id="T_c6ca4_row92_col4" class="data row92 col4" >4.33553</td>
      <td id="T_c6ca4_row92_col5" class="data row92 col5" >4.10535</td>
      <td id="T_c6ca4_row92_col6" class="data row92 col6" >0.0729638</td>
      <td id="T_c6ca4_row92_col7" class="data row92 col7" >0.998677</td>
      <td id="T_c6ca4_row92_col8" class="data row92 col8" >0.0424805</td>
      <td id="T_c6ca4_row92_col9" class="data row92 col9" >1024.000000</td>
      <td id="T_c6ca4_row92_col10" class="data row92 col10" >torch.float32</td>
      <td id="T_c6ca4_row92_col11" class="data row92 col11" >(1024,)</td>
      <td id="T_c6ca4_row92_col12" class="data row92 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row93" class="row_heading level0 row93" >93</th>
      <td id="T_c6ca4_row93_col0" class="data row93 col0" >encoder.layers.13.self_attn.out_proj.weight</td>
      <td id="T_c6ca4_row93_col1" class="data row93 col1" >encoder.layers.13.self_attn.out_proj.weight</td>
      <td id="T_c6ca4_row93_col2" class="data row93 col2" >av_romanizer.w2v_model.encoder.layers.13.self_attn.out_proj.weight</td>
      <td id="T_c6ca4_row93_col3" class="data row93 col3" >25.4583</td>
      <td id="T_c6ca4_row93_col4" class="data row93 col4" >134.598</td>
      <td id="T_c6ca4_row93_col5" class="data row93 col5" >128.646</td>
      <td id="T_c6ca4_row93_col6" class="data row93 col6" >0.189144</td>
      <td id="T_c6ca4_row93_col7" class="data row93 col7" >0.982308</td>
      <td id="T_c6ca4_row93_col8" class="data row93 col8" >0.14563</td>
      <td id="T_c6ca4_row93_col9" class="data row93 col9" >1048576.000000</td>
      <td id="T_c6ca4_row93_col10" class="data row93 col10" >torch.float32</td>
      <td id="T_c6ca4_row93_col11" class="data row93 col11" >(1024, 1024)</td>
      <td id="T_c6ca4_row93_col12" class="data row93 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row94" class="row_heading level0 row94" >94</th>
      <td id="T_c6ca4_row94_col0" class="data row94 col0" >encoder.layers.13.self_attn.q_proj.bias</td>
      <td id="T_c6ca4_row94_col1" class="data row94 col1" >encoder.layers.13.self_attn.q_proj.bias</td>
      <td id="T_c6ca4_row94_col2" class="data row94 col2" >av_romanizer.w2v_model.encoder.layers.13.self_attn.q_proj.bias</td>
      <td id="T_c6ca4_row94_col3" class="data row94 col3" >0.908703</td>
      <td id="T_c6ca4_row94_col4" class="data row94 col4" >9.05959</td>
      <td id="T_c6ca4_row94_col5" class="data row94 col5" >8.61004</td>
      <td id="T_c6ca4_row94_col6" class="data row94 col6" >0.100303</td>
      <td id="T_c6ca4_row94_col7" class="data row94 col7" >0.996002</td>
      <td id="T_c6ca4_row94_col8" class="data row94 col8" >0.124023</td>
      <td id="T_c6ca4_row94_col9" class="data row94 col9" >1024.000000</td>
      <td id="T_c6ca4_row94_col10" class="data row94 col10" >torch.float32</td>
      <td id="T_c6ca4_row94_col11" class="data row94 col11" >(1024,)</td>
      <td id="T_c6ca4_row94_col12" class="data row94 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row95" class="row_heading level0 row95" >95</th>
      <td id="T_c6ca4_row95_col0" class="data row95 col0" >encoder.layers.13.self_attn.q_proj.weight</td>
      <td id="T_c6ca4_row95_col1" class="data row95 col1" >encoder.layers.13.self_attn.q_proj.weight</td>
      <td id="T_c6ca4_row95_col2" class="data row95 col2" >av_romanizer.w2v_model.encoder.layers.13.self_attn.q_proj.weight</td>
      <td id="T_c6ca4_row95_col3" class="data row95 col3" >26.831</td>
      <td id="T_c6ca4_row95_col4" class="data row95 col4" >146.285</td>
      <td id="T_c6ca4_row95_col5" class="data row95 col5" >140.485</td>
      <td id="T_c6ca4_row95_col6" class="data row95 col6" >0.183416</td>
      <td id="T_c6ca4_row95_col7" class="data row95 col7" >0.983303</td>
      <td id="T_c6ca4_row95_col8" class="data row95 col8" >0.151123</td>
      <td id="T_c6ca4_row95_col9" class="data row95 col9" >1048576.000000</td>
      <td id="T_c6ca4_row95_col10" class="data row95 col10" >torch.float32</td>
      <td id="T_c6ca4_row95_col11" class="data row95 col11" >(1024, 1024)</td>
      <td id="T_c6ca4_row95_col12" class="data row95 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row96" class="row_heading level0 row96" >96</th>
      <td id="T_c6ca4_row96_col0" class="data row96 col0" >encoder.layers.13.self_attn.v_proj.bias</td>
      <td id="T_c6ca4_row96_col1" class="data row96 col1" >encoder.layers.13.self_attn.v_proj.bias</td>
      <td id="T_c6ca4_row96_col2" class="data row96 col2" >av_romanizer.w2v_model.encoder.layers.13.self_attn.v_proj.bias</td>
      <td id="T_c6ca4_row96_col3" class="data row96 col3" >0.230535</td>
      <td id="T_c6ca4_row96_col4" class="data row96 col4" >1.60915</td>
      <td id="T_c6ca4_row96_col5" class="data row96 col5" >1.54673</td>
      <td id="T_c6ca4_row96_col6" class="data row96 col6" >0.143265</td>
      <td id="T_c6ca4_row96_col7" class="data row96 col7" >0.990106</td>
      <td id="T_c6ca4_row96_col8" class="data row96 col8" >0.0290833</td>
      <td id="T_c6ca4_row96_col9" class="data row96 col9" >1024.000000</td>
      <td id="T_c6ca4_row96_col10" class="data row96 col10" >torch.float32</td>
      <td id="T_c6ca4_row96_col11" class="data row96 col11" >(1024,)</td>
      <td id="T_c6ca4_row96_col12" class="data row96 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row97" class="row_heading level0 row97" >97</th>
      <td id="T_c6ca4_row97_col0" class="data row97 col0" >encoder.layers.13.self_attn.v_proj.weight</td>
      <td id="T_c6ca4_row97_col1" class="data row97 col1" >encoder.layers.13.self_attn.v_proj.weight</td>
      <td id="T_c6ca4_row97_col2" class="data row97 col2" >av_romanizer.w2v_model.encoder.layers.13.self_attn.v_proj.weight</td>
      <td id="T_c6ca4_row97_col3" class="data row97 col3" >22.8016</td>
      <td id="T_c6ca4_row97_col4" class="data row97 col4" >138.664</td>
      <td id="T_c6ca4_row97_col5" class="data row97 col5" >132.565</td>
      <td id="T_c6ca4_row97_col6" class="data row97 col6" >0.164437</td>
      <td id="T_c6ca4_row97_col7" class="data row97 col7" >0.98687</td>
      <td id="T_c6ca4_row97_col8" class="data row97 col8" >0.130005</td>
      <td id="T_c6ca4_row97_col9" class="data row97 col9" >1048576.000000</td>
      <td id="T_c6ca4_row97_col10" class="data row97 col10" >torch.float32</td>
      <td id="T_c6ca4_row97_col11" class="data row97 col11" >(1024, 1024)</td>
      <td id="T_c6ca4_row97_col12" class="data row97 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row98" class="row_heading level0 row98" >98</th>
      <td id="T_c6ca4_row98_col0" class="data row98 col0" >encoder.layers.13.self_attn_layer_norm.bias</td>
      <td id="T_c6ca4_row98_col1" class="data row98 col1" >encoder.layers.13.self_attn_layer_norm.bias</td>
      <td id="T_c6ca4_row98_col2" class="data row98 col2" >av_romanizer.w2v_model.encoder.layers.13.self_attn_layer_norm.bias</td>
      <td id="T_c6ca4_row98_col3" class="data row98 col3" >0.216855</td>
      <td id="T_c6ca4_row98_col4" class="data row98 col4" >0.845777</td>
      <td id="T_c6ca4_row98_col5" class="data row98 col5" >0.819174</td>
      <td id="T_c6ca4_row98_col6" class="data row98 col6" >0.256397</td>
      <td id="T_c6ca4_row98_col7" class="data row98 col7" >0.966573</td>
      <td id="T_c6ca4_row98_col8" class="data row98 col8" >0.0373535</td>
      <td id="T_c6ca4_row98_col9" class="data row98 col9" >1024.000000</td>
      <td id="T_c6ca4_row98_col10" class="data row98 col10" >torch.float32</td>
      <td id="T_c6ca4_row98_col11" class="data row98 col11" >(1024,)</td>
      <td id="T_c6ca4_row98_col12" class="data row98 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row99" class="row_heading level0 row99" >99</th>
      <td id="T_c6ca4_row99_col0" class="data row99 col0" >encoder.layers.13.self_attn_layer_norm.weight</td>
      <td id="T_c6ca4_row99_col1" class="data row99 col1" >encoder.layers.13.self_attn_layer_norm.weight</td>
      <td id="T_c6ca4_row99_col2" class="data row99 col2" >av_romanizer.w2v_model.encoder.layers.13.self_attn_layer_norm.weight</td>
      <td id="T_c6ca4_row99_col3" class="data row99 col3" >0.426237</td>
      <td id="T_c6ca4_row99_col4" class="data row99 col4" >6.19923</td>
      <td id="T_c6ca4_row99_col5" class="data row99 col5" >6.33407</td>
      <td id="T_c6ca4_row99_col6" class="data row99 col6" >0.0687564</td>
      <td id="T_c6ca4_row99_col7" class="data row99 col7" >0.997918</td>
      <td id="T_c6ca4_row99_col8" class="data row99 col8" >0.0430908</td>
      <td id="T_c6ca4_row99_col9" class="data row99 col9" >1024.000000</td>
      <td id="T_c6ca4_row99_col10" class="data row99 col10" >torch.float32</td>
      <td id="T_c6ca4_row99_col11" class="data row99 col11" >(1024,)</td>
      <td id="T_c6ca4_row99_col12" class="data row99 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row100" class="row_heading level0 row100" >100</th>
      <td id="T_c6ca4_row100_col0" class="data row100 col0" >encoder.layers.14.fc1.bias</td>
      <td id="T_c6ca4_row100_col1" class="data row100 col1" >encoder.layers.14.fc1.bias</td>
      <td id="T_c6ca4_row100_col2" class="data row100 col2" >av_romanizer.w2v_model.encoder.layers.14.fc1.bias</td>
      <td id="T_c6ca4_row100_col3" class="data row100 col3" >0.704621</td>
      <td id="T_c6ca4_row100_col4" class="data row100 col4" >2.78177</td>
      <td id="T_c6ca4_row100_col5" class="data row100 col5" >2.77197</td>
      <td id="T_c6ca4_row100_col6" class="data row100 col6" >0.253299</td>
      <td id="T_c6ca4_row100_col7" class="data row100 col7" >0.967812</td>
      <td id="T_c6ca4_row100_col8" class="data row100 col8" >0.0556641</td>
      <td id="T_c6ca4_row100_col9" class="data row100 col9" >4096.000000</td>
      <td id="T_c6ca4_row100_col10" class="data row100 col10" >torch.float32</td>
      <td id="T_c6ca4_row100_col11" class="data row100 col11" >(4096,)</td>
      <td id="T_c6ca4_row100_col12" class="data row100 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row101" class="row_heading level0 row101" >101</th>
      <td id="T_c6ca4_row101_col0" class="data row101 col0" >encoder.layers.14.fc1.weight</td>
      <td id="T_c6ca4_row101_col1" class="data row101 col1" >encoder.layers.14.fc1.weight</td>
      <td id="T_c6ca4_row101_col2" class="data row101 col2" >av_romanizer.w2v_model.encoder.layers.14.fc1.weight</td>
      <td id="T_c6ca4_row101_col3" class="data row101 col3" >60.9741</td>
      <td id="T_c6ca4_row101_col4" class="data row101 col4" >302.663</td>
      <td id="T_c6ca4_row101_col5" class="data row101 col5" >289.801</td>
      <td id="T_c6ca4_row101_col6" class="data row101 col6" >0.201459</td>
      <td id="T_c6ca4_row101_col7" class="data row101 col7" >0.97975</td>
      <td id="T_c6ca4_row101_col8" class="data row101 col8" >0.207901</td>
      <td id="T_c6ca4_row101_col9" class="data row101 col9" >4194304.000000</td>
      <td id="T_c6ca4_row101_col10" class="data row101 col10" >torch.float32</td>
      <td id="T_c6ca4_row101_col11" class="data row101 col11" >(4096, 1024)</td>
      <td id="T_c6ca4_row101_col12" class="data row101 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row102" class="row_heading level0 row102" >102</th>
      <td id="T_c6ca4_row102_col0" class="data row102 col0" >encoder.layers.14.fc2.bias</td>
      <td id="T_c6ca4_row102_col1" class="data row102 col1" >encoder.layers.14.fc2.bias</td>
      <td id="T_c6ca4_row102_col2" class="data row102 col2" >av_romanizer.w2v_model.encoder.layers.14.fc2.bias</td>
      <td id="T_c6ca4_row102_col3" class="data row102 col3" >0.340703</td>
      <td id="T_c6ca4_row102_col4" class="data row102 col4" >3.39179</td>
      <td id="T_c6ca4_row102_col5" class="data row102 col5" >3.18827</td>
      <td id="T_c6ca4_row102_col6" class="data row102 col6" >0.100449</td>
      <td id="T_c6ca4_row102_col7" class="data row102 col7" >0.996548</td>
      <td id="T_c6ca4_row102_col8" class="data row102 col8" >0.0535889</td>
      <td id="T_c6ca4_row102_col9" class="data row102 col9" >1024.000000</td>
      <td id="T_c6ca4_row102_col10" class="data row102 col10" >torch.float32</td>
      <td id="T_c6ca4_row102_col11" class="data row102 col11" >(1024,)</td>
      <td id="T_c6ca4_row102_col12" class="data row102 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row103" class="row_heading level0 row103" >103</th>
      <td id="T_c6ca4_row103_col0" class="data row103 col0" >encoder.layers.14.fc2.weight</td>
      <td id="T_c6ca4_row103_col1" class="data row103 col1" >encoder.layers.14.fc2.weight</td>
      <td id="T_c6ca4_row103_col2" class="data row103 col2" >av_romanizer.w2v_model.encoder.layers.14.fc2.weight</td>
      <td id="T_c6ca4_row103_col3" class="data row103 col3" >64.5754</td>
      <td id="T_c6ca4_row103_col4" class="data row103 col4" >314.411</td>
      <td id="T_c6ca4_row103_col5" class="data row103 col5" >301.367</td>
      <td id="T_c6ca4_row103_col6" class="data row103 col6" >0.205386</td>
      <td id="T_c6ca4_row103_col7" class="data row103 col7" >0.978893</td>
      <td id="T_c6ca4_row103_col8" class="data row103 col8" >0.238281</td>
      <td id="T_c6ca4_row103_col9" class="data row103 col9" >4194304.000000</td>
      <td id="T_c6ca4_row103_col10" class="data row103 col10" >torch.float32</td>
      <td id="T_c6ca4_row103_col11" class="data row103 col11" >(1024, 4096)</td>
      <td id="T_c6ca4_row103_col12" class="data row103 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row104" class="row_heading level0 row104" >104</th>
      <td id="T_c6ca4_row104_col0" class="data row104 col0" >encoder.layers.14.final_layer_norm.bias</td>
      <td id="T_c6ca4_row104_col1" class="data row104 col1" >encoder.layers.14.final_layer_norm.bias</td>
      <td id="T_c6ca4_row104_col2" class="data row104 col2" >av_romanizer.w2v_model.encoder.layers.14.final_layer_norm.bias</td>
      <td id="T_c6ca4_row104_col3" class="data row104 col3" >0.246175</td>
      <td id="T_c6ca4_row104_col4" class="data row104 col4" >2.70913</td>
      <td id="T_c6ca4_row104_col5" class="data row104 col5" >2.62598</td>
      <td id="T_c6ca4_row104_col6" class="data row104 col6" >0.0908685</td>
      <td id="T_c6ca4_row104_col7" class="data row104 col7" >0.996227</td>
      <td id="T_c6ca4_row104_col8" class="data row104 col8" >0.0322266</td>
      <td id="T_c6ca4_row104_col9" class="data row104 col9" >1024.000000</td>
      <td id="T_c6ca4_row104_col10" class="data row104 col10" >torch.float32</td>
      <td id="T_c6ca4_row104_col11" class="data row104 col11" >(1024,)</td>
      <td id="T_c6ca4_row104_col12" class="data row104 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row105" class="row_heading level0 row105" >105</th>
      <td id="T_c6ca4_row105_col0" class="data row105 col0" >encoder.layers.14.final_layer_norm.weight</td>
      <td id="T_c6ca4_row105_col1" class="data row105 col1" >encoder.layers.14.final_layer_norm.weight</td>
      <td id="T_c6ca4_row105_col2" class="data row105 col2" >av_romanizer.w2v_model.encoder.layers.14.final_layer_norm.weight</td>
      <td id="T_c6ca4_row105_col3" class="data row105 col3" >0.581267</td>
      <td id="T_c6ca4_row105_col4" class="data row105 col4" >9.91966</td>
      <td id="T_c6ca4_row105_col5" class="data row105 col5" >9.76859</td>
      <td id="T_c6ca4_row105_col6" class="data row105 col6" >0.0585974</td>
      <td id="T_c6ca4_row105_col7" class="data row105 col7" >0.998374</td>
      <td id="T_c6ca4_row105_col8" class="data row105 col8" >0.0720215</td>
      <td id="T_c6ca4_row105_col9" class="data row105 col9" >1024.000000</td>
      <td id="T_c6ca4_row105_col10" class="data row105 col10" >torch.float32</td>
      <td id="T_c6ca4_row105_col11" class="data row105 col11" >(1024,)</td>
      <td id="T_c6ca4_row105_col12" class="data row105 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row106" class="row_heading level0 row106" >106</th>
      <td id="T_c6ca4_row106_col0" class="data row106 col0" >encoder.layers.14.self_attn.k_proj.bias</td>
      <td id="T_c6ca4_row106_col1" class="data row106 col1" >encoder.layers.14.self_attn.k_proj.bias</td>
      <td id="T_c6ca4_row106_col2" class="data row106 col2" >av_romanizer.w2v_model.encoder.layers.14.self_attn.k_proj.bias</td>
      <td id="T_c6ca4_row106_col3" class="data row106 col3" >0.0189156</td>
      <td id="T_c6ca4_row106_col4" class="data row106 col4" >0.0888604</td>
      <td id="T_c6ca4_row106_col5" class="data row106 col5" >0.0867714</td>
      <td id="T_c6ca4_row106_col6" class="data row106 col6" >0.212869</td>
      <td id="T_c6ca4_row106_col7" class="data row106 col7" >0.977081</td>
      <td id="T_c6ca4_row106_col8" class="data row106 col8" >0.00281644</td>
      <td id="T_c6ca4_row106_col9" class="data row106 col9" >1024.000000</td>
      <td id="T_c6ca4_row106_col10" class="data row106 col10" >torch.float32</td>
      <td id="T_c6ca4_row106_col11" class="data row106 col11" >(1024,)</td>
      <td id="T_c6ca4_row106_col12" class="data row106 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row107" class="row_heading level0 row107" >107</th>
      <td id="T_c6ca4_row107_col0" class="data row107 col0" >encoder.layers.14.self_attn.k_proj.weight</td>
      <td id="T_c6ca4_row107_col1" class="data row107 col1" >encoder.layers.14.self_attn.k_proj.weight</td>
      <td id="T_c6ca4_row107_col2" class="data row107 col2" >av_romanizer.w2v_model.encoder.layers.14.self_attn.k_proj.weight</td>
      <td id="T_c6ca4_row107_col3" class="data row107 col3" >26.9951</td>
      <td id="T_c6ca4_row107_col4" class="data row107 col4" >144.784</td>
      <td id="T_c6ca4_row107_col5" class="data row107 col5" >139.11</td>
      <td id="T_c6ca4_row107_col6" class="data row107 col6" >0.186451</td>
      <td id="T_c6ca4_row107_col7" class="data row107 col7" >0.982708</td>
      <td id="T_c6ca4_row107_col8" class="data row107 col8" >0.131233</td>
      <td id="T_c6ca4_row107_col9" class="data row107 col9" >1048576.000000</td>
      <td id="T_c6ca4_row107_col10" class="data row107 col10" >torch.float32</td>
      <td id="T_c6ca4_row107_col11" class="data row107 col11" >(1024, 1024)</td>
      <td id="T_c6ca4_row107_col12" class="data row107 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row108" class="row_heading level0 row108" >108</th>
      <td id="T_c6ca4_row108_col0" class="data row108 col0" >encoder.layers.14.self_attn.out_proj.bias</td>
      <td id="T_c6ca4_row108_col1" class="data row108 col1" >encoder.layers.14.self_attn.out_proj.bias</td>
      <td id="T_c6ca4_row108_col2" class="data row108 col2" >av_romanizer.w2v_model.encoder.layers.14.self_attn.out_proj.bias</td>
      <td id="T_c6ca4_row108_col3" class="data row108 col3" >0.314449</td>
      <td id="T_c6ca4_row108_col4" class="data row108 col4" >4.50498</td>
      <td id="T_c6ca4_row108_col5" class="data row108 col5" >4.28654</td>
      <td id="T_c6ca4_row108_col6" class="data row108 col6" >0.0698003</td>
      <td id="T_c6ca4_row108_col7" class="data row108 col7" >0.998675</td>
      <td id="T_c6ca4_row108_col8" class="data row108 col8" >0.0380859</td>
      <td id="T_c6ca4_row108_col9" class="data row108 col9" >1024.000000</td>
      <td id="T_c6ca4_row108_col10" class="data row108 col10" >torch.float32</td>
      <td id="T_c6ca4_row108_col11" class="data row108 col11" >(1024,)</td>
      <td id="T_c6ca4_row108_col12" class="data row108 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row109" class="row_heading level0 row109" >109</th>
      <td id="T_c6ca4_row109_col0" class="data row109 col0" >encoder.layers.14.self_attn.out_proj.weight</td>
      <td id="T_c6ca4_row109_col1" class="data row109 col1" >encoder.layers.14.self_attn.out_proj.weight</td>
      <td id="T_c6ca4_row109_col2" class="data row109 col2" >av_romanizer.w2v_model.encoder.layers.14.self_attn.out_proj.weight</td>
      <td id="T_c6ca4_row109_col3" class="data row109 col3" >25.6552</td>
      <td id="T_c6ca4_row109_col4" class="data row109 col4" >140.613</td>
      <td id="T_c6ca4_row109_col5" class="data row109 col5" >134.241</td>
      <td id="T_c6ca4_row109_col6" class="data row109 col6" >0.182453</td>
      <td id="T_c6ca4_row109_col7" class="data row109 col7" >0.983641</td>
      <td id="T_c6ca4_row109_col8" class="data row109 col8" >0.138</td>
      <td id="T_c6ca4_row109_col9" class="data row109 col9" >1048576.000000</td>
      <td id="T_c6ca4_row109_col10" class="data row109 col10" >torch.float32</td>
      <td id="T_c6ca4_row109_col11" class="data row109 col11" >(1024, 1024)</td>
      <td id="T_c6ca4_row109_col12" class="data row109 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row110" class="row_heading level0 row110" >110</th>
      <td id="T_c6ca4_row110_col0" class="data row110 col0" >encoder.layers.14.self_attn.q_proj.bias</td>
      <td id="T_c6ca4_row110_col1" class="data row110 col1" >encoder.layers.14.self_attn.q_proj.bias</td>
      <td id="T_c6ca4_row110_col2" class="data row110 col2" >av_romanizer.w2v_model.encoder.layers.14.self_attn.q_proj.bias</td>
      <td id="T_c6ca4_row110_col3" class="data row110 col3" >0.837105</td>
      <td id="T_c6ca4_row110_col4" class="data row110 col4" >8.38965</td>
      <td id="T_c6ca4_row110_col5" class="data row110 col5" >8.13349</td>
      <td id="T_c6ca4_row110_col6" class="data row110 col6" >0.0997784</td>
      <td id="T_c6ca4_row110_col7" class="data row110 col7" >0.995346</td>
      <td id="T_c6ca4_row110_col8" class="data row110 col8" >0.145508</td>
      <td id="T_c6ca4_row110_col9" class="data row110 col9" >1024.000000</td>
      <td id="T_c6ca4_row110_col10" class="data row110 col10" >torch.float32</td>
      <td id="T_c6ca4_row110_col11" class="data row110 col11" >(1024,)</td>
      <td id="T_c6ca4_row110_col12" class="data row110 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row111" class="row_heading level0 row111" >111</th>
      <td id="T_c6ca4_row111_col0" class="data row111 col0" >encoder.layers.14.self_attn.q_proj.weight</td>
      <td id="T_c6ca4_row111_col1" class="data row111 col1" >encoder.layers.14.self_attn.q_proj.weight</td>
      <td id="T_c6ca4_row111_col2" class="data row111 col2" >av_romanizer.w2v_model.encoder.layers.14.self_attn.q_proj.weight</td>
      <td id="T_c6ca4_row111_col3" class="data row111 col3" >27.1165</td>
      <td id="T_c6ca4_row111_col4" class="data row111 col4" >143.616</td>
      <td id="T_c6ca4_row111_col5" class="data row111 col5" >138.123</td>
      <td id="T_c6ca4_row111_col6" class="data row111 col6" >0.188813</td>
      <td id="T_c6ca4_row111_col7" class="data row111 col7" >0.982226</td>
      <td id="T_c6ca4_row111_col8" class="data row111 col8" >0.138672</td>
      <td id="T_c6ca4_row111_col9" class="data row111 col9" >1048576.000000</td>
      <td id="T_c6ca4_row111_col10" class="data row111 col10" >torch.float32</td>
      <td id="T_c6ca4_row111_col11" class="data row111 col11" >(1024, 1024)</td>
      <td id="T_c6ca4_row111_col12" class="data row111 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row112" class="row_heading level0 row112" >112</th>
      <td id="T_c6ca4_row112_col0" class="data row112 col0" >encoder.layers.14.self_attn.v_proj.bias</td>
      <td id="T_c6ca4_row112_col1" class="data row112 col1" >encoder.layers.14.self_attn.v_proj.bias</td>
      <td id="T_c6ca4_row112_col2" class="data row112 col2" >av_romanizer.w2v_model.encoder.layers.14.self_attn.v_proj.bias</td>
      <td id="T_c6ca4_row112_col3" class="data row112 col3" >0.251808</td>
      <td id="T_c6ca4_row112_col4" class="data row112 col4" >1.7867</td>
      <td id="T_c6ca4_row112_col5" class="data row112 col5" >1.70195</td>
      <td id="T_c6ca4_row112_col6" class="data row112 col6" >0.140935</td>
      <td id="T_c6ca4_row112_col7" class="data row112 col7" >0.990755</td>
      <td id="T_c6ca4_row112_col8" class="data row112 col8" >0.0303955</td>
      <td id="T_c6ca4_row112_col9" class="data row112 col9" >1024.000000</td>
      <td id="T_c6ca4_row112_col10" class="data row112 col10" >torch.float32</td>
      <td id="T_c6ca4_row112_col11" class="data row112 col11" >(1024,)</td>
      <td id="T_c6ca4_row112_col12" class="data row112 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row113" class="row_heading level0 row113" >113</th>
      <td id="T_c6ca4_row113_col0" class="data row113 col0" >encoder.layers.14.self_attn.v_proj.weight</td>
      <td id="T_c6ca4_row113_col1" class="data row113 col1" >encoder.layers.14.self_attn.v_proj.weight</td>
      <td id="T_c6ca4_row113_col2" class="data row113 col2" >av_romanizer.w2v_model.encoder.layers.14.self_attn.v_proj.weight</td>
      <td id="T_c6ca4_row113_col3" class="data row113 col3" >22.6775</td>
      <td id="T_c6ca4_row113_col4" class="data row113 col4" >143.642</td>
      <td id="T_c6ca4_row113_col5" class="data row113 col5" >137.201</td>
      <td id="T_c6ca4_row113_col6" class="data row113 col6" >0.157874</td>
      <td id="T_c6ca4_row113_col7" class="data row113 col7" >0.988005</td>
      <td id="T_c6ca4_row113_col8" class="data row113 col8" >0.11731</td>
      <td id="T_c6ca4_row113_col9" class="data row113 col9" >1048576.000000</td>
      <td id="T_c6ca4_row113_col10" class="data row113 col10" >torch.float32</td>
      <td id="T_c6ca4_row113_col11" class="data row113 col11" >(1024, 1024)</td>
      <td id="T_c6ca4_row113_col12" class="data row113 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row114" class="row_heading level0 row114" >114</th>
      <td id="T_c6ca4_row114_col0" class="data row114 col0" >encoder.layers.14.self_attn_layer_norm.bias</td>
      <td id="T_c6ca4_row114_col1" class="data row114 col1" >encoder.layers.14.self_attn_layer_norm.bias</td>
      <td id="T_c6ca4_row114_col2" class="data row114 col2" >av_romanizer.w2v_model.encoder.layers.14.self_attn_layer_norm.bias</td>
      <td id="T_c6ca4_row114_col3" class="data row114 col3" >0.224216</td>
      <td id="T_c6ca4_row114_col4" class="data row114 col4" >0.988572</td>
      <td id="T_c6ca4_row114_col5" class="data row114 col5" >0.952484</td>
      <td id="T_c6ca4_row114_col6" class="data row114 col6" >0.226808</td>
      <td id="T_c6ca4_row114_col7" class="data row114 col7" >0.973996</td>
      <td id="T_c6ca4_row114_col8" class="data row114 col8" >0.0267229</td>
      <td id="T_c6ca4_row114_col9" class="data row114 col9" >1024.000000</td>
      <td id="T_c6ca4_row114_col10" class="data row114 col10" >torch.float32</td>
      <td id="T_c6ca4_row114_col11" class="data row114 col11" >(1024,)</td>
      <td id="T_c6ca4_row114_col12" class="data row114 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row115" class="row_heading level0 row115" >115</th>
      <td id="T_c6ca4_row115_col0" class="data row115 col0" >encoder.layers.14.self_attn_layer_norm.weight</td>
      <td id="T_c6ca4_row115_col1" class="data row115 col1" >encoder.layers.14.self_attn_layer_norm.weight</td>
      <td id="T_c6ca4_row115_col2" class="data row115 col2" >av_romanizer.w2v_model.encoder.layers.14.self_attn_layer_norm.weight</td>
      <td id="T_c6ca4_row115_col3" class="data row115 col3" >0.423377</td>
      <td id="T_c6ca4_row115_col4" class="data row115 col4" >6.59015</td>
      <td id="T_c6ca4_row115_col5" class="data row115 col5" >6.67874</td>
      <td id="T_c6ca4_row115_col6" class="data row115 col6" >0.0642439</td>
      <td id="T_c6ca4_row115_col7" class="data row115 col7" >0.998053</td>
      <td id="T_c6ca4_row115_col8" class="data row115 col8" >0.038208</td>
      <td id="T_c6ca4_row115_col9" class="data row115 col9" >1024.000000</td>
      <td id="T_c6ca4_row115_col10" class="data row115 col10" >torch.float32</td>
      <td id="T_c6ca4_row115_col11" class="data row115 col11" >(1024,)</td>
      <td id="T_c6ca4_row115_col12" class="data row115 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row116" class="row_heading level0 row116" >116</th>
      <td id="T_c6ca4_row116_col0" class="data row116 col0" >encoder.layers.15.fc1.bias</td>
      <td id="T_c6ca4_row116_col1" class="data row116 col1" >encoder.layers.15.fc1.bias</td>
      <td id="T_c6ca4_row116_col2" class="data row116 col2" >av_romanizer.w2v_model.encoder.layers.15.fc1.bias</td>
      <td id="T_c6ca4_row116_col3" class="data row116 col3" >0.743217</td>
      <td id="T_c6ca4_row116_col4" class="data row116 col4" >2.8033</td>
      <td id="T_c6ca4_row116_col5" class="data row116 col5" >2.74254</td>
      <td id="T_c6ca4_row116_col6" class="data row116 col6" >0.265123</td>
      <td id="T_c6ca4_row116_col7" class="data row116 col7" >0.964317</td>
      <td id="T_c6ca4_row116_col8" class="data row116 col8" >0.0742188</td>
      <td id="T_c6ca4_row116_col9" class="data row116 col9" >4096.000000</td>
      <td id="T_c6ca4_row116_col10" class="data row116 col10" >torch.float32</td>
      <td id="T_c6ca4_row116_col11" class="data row116 col11" >(4096,)</td>
      <td id="T_c6ca4_row116_col12" class="data row116 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row117" class="row_heading level0 row117" >117</th>
      <td id="T_c6ca4_row117_col0" class="data row117 col0" >encoder.layers.15.fc1.weight</td>
      <td id="T_c6ca4_row117_col1" class="data row117 col1" >encoder.layers.15.fc1.weight</td>
      <td id="T_c6ca4_row117_col2" class="data row117 col2" >av_romanizer.w2v_model.encoder.layers.15.fc1.weight</td>
      <td id="T_c6ca4_row117_col3" class="data row117 col3" >63.2817</td>
      <td id="T_c6ca4_row117_col4" class="data row117 col4" >306.45</td>
      <td id="T_c6ca4_row117_col5" class="data row117 col5" >293.258</td>
      <td id="T_c6ca4_row117_col6" class="data row117 col6" >0.206499</td>
      <td id="T_c6ca4_row117_col7" class="data row117 col7" >0.978688</td>
      <td id="T_c6ca4_row117_col8" class="data row117 col8" >0.185059</td>
      <td id="T_c6ca4_row117_col9" class="data row117 col9" >4194304.000000</td>
      <td id="T_c6ca4_row117_col10" class="data row117 col10" >torch.float32</td>
      <td id="T_c6ca4_row117_col11" class="data row117 col11" >(4096, 1024)</td>
      <td id="T_c6ca4_row117_col12" class="data row117 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row118" class="row_heading level0 row118" >118</th>
      <td id="T_c6ca4_row118_col0" class="data row118 col0" >encoder.layers.15.fc2.bias</td>
      <td id="T_c6ca4_row118_col1" class="data row118 col1" >encoder.layers.15.fc2.bias</td>
      <td id="T_c6ca4_row118_col2" class="data row118 col2" >av_romanizer.w2v_model.encoder.layers.15.fc2.bias</td>
      <td id="T_c6ca4_row118_col3" class="data row118 col3" >0.376344</td>
      <td id="T_c6ca4_row118_col4" class="data row118 col4" >5.21447</td>
      <td id="T_c6ca4_row118_col5" class="data row118 col5" >4.96194</td>
      <td id="T_c6ca4_row118_col6" class="data row118 col6" >0.0721731</td>
      <td id="T_c6ca4_row118_col7" class="data row118 col7" >0.998495</td>
      <td id="T_c6ca4_row118_col8" class="data row118 col8" >0.0529785</td>
      <td id="T_c6ca4_row118_col9" class="data row118 col9" >1024.000000</td>
      <td id="T_c6ca4_row118_col10" class="data row118 col10" >torch.float32</td>
      <td id="T_c6ca4_row118_col11" class="data row118 col11" >(1024,)</td>
      <td id="T_c6ca4_row118_col12" class="data row118 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row119" class="row_heading level0 row119" >119</th>
      <td id="T_c6ca4_row119_col0" class="data row119 col0" >encoder.layers.15.fc2.weight</td>
      <td id="T_c6ca4_row119_col1" class="data row119 col1" >encoder.layers.15.fc2.weight</td>
      <td id="T_c6ca4_row119_col2" class="data row119 col2" >av_romanizer.w2v_model.encoder.layers.15.fc2.weight</td>
      <td id="T_c6ca4_row119_col3" class="data row119 col3" >66.902</td>
      <td id="T_c6ca4_row119_col4" class="data row119 col4" >315.966</td>
      <td id="T_c6ca4_row119_col5" class="data row119 col5" >302.889</td>
      <td id="T_c6ca4_row119_col6" class="data row119 col6" >0.211738</td>
      <td id="T_c6ca4_row119_col7" class="data row119 col7" >0.977509</td>
      <td id="T_c6ca4_row119_col8" class="data row119 col8" >0.232056</td>
      <td id="T_c6ca4_row119_col9" class="data row119 col9" >4194304.000000</td>
      <td id="T_c6ca4_row119_col10" class="data row119 col10" >torch.float32</td>
      <td id="T_c6ca4_row119_col11" class="data row119 col11" >(1024, 4096)</td>
      <td id="T_c6ca4_row119_col12" class="data row119 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row120" class="row_heading level0 row120" >120</th>
      <td id="T_c6ca4_row120_col0" class="data row120 col0" >encoder.layers.15.final_layer_norm.bias</td>
      <td id="T_c6ca4_row120_col1" class="data row120 col1" >encoder.layers.15.final_layer_norm.bias</td>
      <td id="T_c6ca4_row120_col2" class="data row120 col2" >av_romanizer.w2v_model.encoder.layers.15.final_layer_norm.bias</td>
      <td id="T_c6ca4_row120_col3" class="data row120 col3" >0.337947</td>
      <td id="T_c6ca4_row120_col4" class="data row120 col4" >3.24761</td>
      <td id="T_c6ca4_row120_col5" class="data row120 col5" >3.06037</td>
      <td id="T_c6ca4_row120_col6" class="data row120 col6" >0.10406</td>
      <td id="T_c6ca4_row120_col7" class="data row120 col7" >0.996018</td>
      <td id="T_c6ca4_row120_col8" class="data row120 col8" >0.03479</td>
      <td id="T_c6ca4_row120_col9" class="data row120 col9" >1024.000000</td>
      <td id="T_c6ca4_row120_col10" class="data row120 col10" >torch.float32</td>
      <td id="T_c6ca4_row120_col11" class="data row120 col11" >(1024,)</td>
      <td id="T_c6ca4_row120_col12" class="data row120 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row121" class="row_heading level0 row121" >121</th>
      <td id="T_c6ca4_row121_col0" class="data row121 col0" >encoder.layers.15.final_layer_norm.weight</td>
      <td id="T_c6ca4_row121_col1" class="data row121 col1" >encoder.layers.15.final_layer_norm.weight</td>
      <td id="T_c6ca4_row121_col2" class="data row121 col2" >av_romanizer.w2v_model.encoder.layers.15.final_layer_norm.weight</td>
      <td id="T_c6ca4_row121_col3" class="data row121 col3" >0.713044</td>
      <td id="T_c6ca4_row121_col4" class="data row121 col4" >12.0699</td>
      <td id="T_c6ca4_row121_col5" class="data row121 col5" >11.7635</td>
      <td id="T_c6ca4_row121_col6" class="data row121 col6" >0.0590765</td>
      <td id="T_c6ca4_row121_col7" class="data row121 col7" >0.99854</td>
      <td id="T_c6ca4_row121_col8" class="data row121 col8" >0.102783</td>
      <td id="T_c6ca4_row121_col9" class="data row121 col9" >1024.000000</td>
      <td id="T_c6ca4_row121_col10" class="data row121 col10" >torch.float32</td>
      <td id="T_c6ca4_row121_col11" class="data row121 col11" >(1024,)</td>
      <td id="T_c6ca4_row121_col12" class="data row121 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row122" class="row_heading level0 row122" >122</th>
      <td id="T_c6ca4_row122_col0" class="data row122 col0" >encoder.layers.15.self_attn.k_proj.bias</td>
      <td id="T_c6ca4_row122_col1" class="data row122 col1" >encoder.layers.15.self_attn.k_proj.bias</td>
      <td id="T_c6ca4_row122_col2" class="data row122 col2" >av_romanizer.w2v_model.encoder.layers.15.self_attn.k_proj.bias</td>
      <td id="T_c6ca4_row122_col3" class="data row122 col3" >0.0147289</td>
      <td id="T_c6ca4_row122_col4" class="data row122 col4" >0.070726</td>
      <td id="T_c6ca4_row122_col5" class="data row122 col5" >0.0689379</td>
      <td id="T_c6ca4_row122_col6" class="data row122 col6" >0.208254</td>
      <td id="T_c6ca4_row122_col7" class="data row122 col7" >0.978081</td>
      <td id="T_c6ca4_row122_col8" class="data row122 col8" >0.00232029</td>
      <td id="T_c6ca4_row122_col9" class="data row122 col9" >1024.000000</td>
      <td id="T_c6ca4_row122_col10" class="data row122 col10" >torch.float32</td>
      <td id="T_c6ca4_row122_col11" class="data row122 col11" >(1024,)</td>
      <td id="T_c6ca4_row122_col12" class="data row122 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row123" class="row_heading level0 row123" >123</th>
      <td id="T_c6ca4_row123_col0" class="data row123 col0" >encoder.layers.15.self_attn.k_proj.weight</td>
      <td id="T_c6ca4_row123_col1" class="data row123 col1" >encoder.layers.15.self_attn.k_proj.weight</td>
      <td id="T_c6ca4_row123_col2" class="data row123 col2" >av_romanizer.w2v_model.encoder.layers.15.self_attn.k_proj.weight</td>
      <td id="T_c6ca4_row123_col3" class="data row123 col3" >27.0998</td>
      <td id="T_c6ca4_row123_col4" class="data row123 col4" >139.855</td>
      <td id="T_c6ca4_row123_col5" class="data row123 col5" >134.466</td>
      <td id="T_c6ca4_row123_col6" class="data row123 col6" >0.193771</td>
      <td id="T_c6ca4_row123_col7" class="data row123 col7" >0.981246</td>
      <td id="T_c6ca4_row123_col8" class="data row123 col8" >0.132202</td>
      <td id="T_c6ca4_row123_col9" class="data row123 col9" >1048576.000000</td>
      <td id="T_c6ca4_row123_col10" class="data row123 col10" >torch.float32</td>
      <td id="T_c6ca4_row123_col11" class="data row123 col11" >(1024, 1024)</td>
      <td id="T_c6ca4_row123_col12" class="data row123 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row124" class="row_heading level0 row124" >124</th>
      <td id="T_c6ca4_row124_col0" class="data row124 col0" >encoder.layers.15.self_attn.out_proj.bias</td>
      <td id="T_c6ca4_row124_col1" class="data row124 col1" >encoder.layers.15.self_attn.out_proj.bias</td>
      <td id="T_c6ca4_row124_col2" class="data row124 col2" >av_romanizer.w2v_model.encoder.layers.15.self_attn.out_proj.bias</td>
      <td id="T_c6ca4_row124_col3" class="data row124 col3" >0.316987</td>
      <td id="T_c6ca4_row124_col4" class="data row124 col4" >4.85214</td>
      <td id="T_c6ca4_row124_col5" class="data row124 col5" >4.64473</td>
      <td id="T_c6ca4_row124_col6" class="data row124 col6" >0.0653294</td>
      <td id="T_c6ca4_row124_col7" class="data row124 col7" >0.998725</td>
      <td id="T_c6ca4_row124_col8" class="data row124 col8" >0.0410156</td>
      <td id="T_c6ca4_row124_col9" class="data row124 col9" >1024.000000</td>
      <td id="T_c6ca4_row124_col10" class="data row124 col10" >torch.float32</td>
      <td id="T_c6ca4_row124_col11" class="data row124 col11" >(1024,)</td>
      <td id="T_c6ca4_row124_col12" class="data row124 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row125" class="row_heading level0 row125" >125</th>
      <td id="T_c6ca4_row125_col0" class="data row125 col0" >encoder.layers.15.self_attn.out_proj.weight</td>
      <td id="T_c6ca4_row125_col1" class="data row125 col1" >encoder.layers.15.self_attn.out_proj.weight</td>
      <td id="T_c6ca4_row125_col2" class="data row125 col2" >av_romanizer.w2v_model.encoder.layers.15.self_attn.out_proj.weight</td>
      <td id="T_c6ca4_row125_col3" class="data row125 col3" >26.5156</td>
      <td id="T_c6ca4_row125_col4" class="data row125 col4" >143.142</td>
      <td id="T_c6ca4_row125_col5" class="data row125 col5" >136.537</td>
      <td id="T_c6ca4_row125_col6" class="data row125 col6" >0.18524</td>
      <td id="T_c6ca4_row125_col7" class="data row125 col7" >0.983129</td>
      <td id="T_c6ca4_row125_col8" class="data row125 col8" >0.141602</td>
      <td id="T_c6ca4_row125_col9" class="data row125 col9" >1048576.000000</td>
      <td id="T_c6ca4_row125_col10" class="data row125 col10" >torch.float32</td>
      <td id="T_c6ca4_row125_col11" class="data row125 col11" >(1024, 1024)</td>
      <td id="T_c6ca4_row125_col12" class="data row125 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row126" class="row_heading level0 row126" >126</th>
      <td id="T_c6ca4_row126_col0" class="data row126 col0" >encoder.layers.15.self_attn.q_proj.bias</td>
      <td id="T_c6ca4_row126_col1" class="data row126 col1" >encoder.layers.15.self_attn.q_proj.bias</td>
      <td id="T_c6ca4_row126_col2" class="data row126 col2" >av_romanizer.w2v_model.encoder.layers.15.self_attn.q_proj.bias</td>
      <td id="T_c6ca4_row126_col3" class="data row126 col3" >0.867146</td>
      <td id="T_c6ca4_row126_col4" class="data row126 col4" >8.48764</td>
      <td id="T_c6ca4_row126_col5" class="data row126 col5" >8.23026</td>
      <td id="T_c6ca4_row126_col6" class="data row126 col6" >0.102166</td>
      <td id="T_c6ca4_row126_col7" class="data row126 col7" >0.995092</td>
      <td id="T_c6ca4_row126_col8" class="data row126 col8" >0.139648</td>
      <td id="T_c6ca4_row126_col9" class="data row126 col9" >1024.000000</td>
      <td id="T_c6ca4_row126_col10" class="data row126 col10" >torch.float32</td>
      <td id="T_c6ca4_row126_col11" class="data row126 col11" >(1024,)</td>
      <td id="T_c6ca4_row126_col12" class="data row126 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row127" class="row_heading level0 row127" >127</th>
      <td id="T_c6ca4_row127_col0" class="data row127 col0" >encoder.layers.15.self_attn.q_proj.weight</td>
      <td id="T_c6ca4_row127_col1" class="data row127 col1" >encoder.layers.15.self_attn.q_proj.weight</td>
      <td id="T_c6ca4_row127_col2" class="data row127 col2" >av_romanizer.w2v_model.encoder.layers.15.self_attn.q_proj.weight</td>
      <td id="T_c6ca4_row127_col3" class="data row127 col3" >27.194</td>
      <td id="T_c6ca4_row127_col4" class="data row127 col4" >138.732</td>
      <td id="T_c6ca4_row127_col5" class="data row127 col5" >133.465</td>
      <td id="T_c6ca4_row127_col6" class="data row127 col6" >0.196019</td>
      <td id="T_c6ca4_row127_col7" class="data row127 col7" >0.980779</td>
      <td id="T_c6ca4_row127_col8" class="data row127 col8" >0.133423</td>
      <td id="T_c6ca4_row127_col9" class="data row127 col9" >1048576.000000</td>
      <td id="T_c6ca4_row127_col10" class="data row127 col10" >torch.float32</td>
      <td id="T_c6ca4_row127_col11" class="data row127 col11" >(1024, 1024)</td>
      <td id="T_c6ca4_row127_col12" class="data row127 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row128" class="row_heading level0 row128" >128</th>
      <td id="T_c6ca4_row128_col0" class="data row128 col0" >encoder.layers.15.self_attn.v_proj.bias</td>
      <td id="T_c6ca4_row128_col1" class="data row128 col1" >encoder.layers.15.self_attn.v_proj.bias</td>
      <td id="T_c6ca4_row128_col2" class="data row128 col2" >av_romanizer.w2v_model.encoder.layers.15.self_attn.v_proj.bias</td>
      <td id="T_c6ca4_row128_col3" class="data row128 col3" >0.243258</td>
      <td id="T_c6ca4_row128_col4" class="data row128 col4" >1.66213</td>
      <td id="T_c6ca4_row128_col5" class="data row128 col5" >1.59521</td>
      <td id="T_c6ca4_row128_col6" class="data row128 col6" >0.146353</td>
      <td id="T_c6ca4_row128_col7" class="data row128 col7" >0.989686</td>
      <td id="T_c6ca4_row128_col8" class="data row128 col8" >0.0275269</td>
      <td id="T_c6ca4_row128_col9" class="data row128 col9" >1024.000000</td>
      <td id="T_c6ca4_row128_col10" class="data row128 col10" >torch.float32</td>
      <td id="T_c6ca4_row128_col11" class="data row128 col11" >(1024,)</td>
      <td id="T_c6ca4_row128_col12" class="data row128 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row129" class="row_heading level0 row129" >129</th>
      <td id="T_c6ca4_row129_col0" class="data row129 col0" >encoder.layers.15.self_attn.v_proj.weight</td>
      <td id="T_c6ca4_row129_col1" class="data row129 col1" >encoder.layers.15.self_attn.v_proj.weight</td>
      <td id="T_c6ca4_row129_col2" class="data row129 col2" >av_romanizer.w2v_model.encoder.layers.15.self_attn.v_proj.weight</td>
      <td id="T_c6ca4_row129_col3" class="data row129 col3" >22.6328</td>
      <td id="T_c6ca4_row129_col4" class="data row129 col4" >146.116</td>
      <td id="T_c6ca4_row129_col5" class="data row129 col5" >139.451</td>
      <td id="T_c6ca4_row129_col6" class="data row129 col6" >0.154896</td>
      <td id="T_c6ca4_row129_col7" class="data row129 col7" >0.98852</td>
      <td id="T_c6ca4_row129_col8" class="data row129 col8" >0.121918</td>
      <td id="T_c6ca4_row129_col9" class="data row129 col9" >1048576.000000</td>
      <td id="T_c6ca4_row129_col10" class="data row129 col10" >torch.float32</td>
      <td id="T_c6ca4_row129_col11" class="data row129 col11" >(1024, 1024)</td>
      <td id="T_c6ca4_row129_col12" class="data row129 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row130" class="row_heading level0 row130" >130</th>
      <td id="T_c6ca4_row130_col0" class="data row130 col0" >encoder.layers.15.self_attn_layer_norm.bias</td>
      <td id="T_c6ca4_row130_col1" class="data row130 col1" >encoder.layers.15.self_attn_layer_norm.bias</td>
      <td id="T_c6ca4_row130_col2" class="data row130 col2" >av_romanizer.w2v_model.encoder.layers.15.self_attn_layer_norm.bias</td>
      <td id="T_c6ca4_row130_col3" class="data row130 col3" >0.201449</td>
      <td id="T_c6ca4_row130_col4" class="data row130 col4" >1.00634</td>
      <td id="T_c6ca4_row130_col5" class="data row130 col5" >0.978211</td>
      <td id="T_c6ca4_row130_col6" class="data row130 col6" >0.20018</td>
      <td id="T_c6ca4_row130_col7" class="data row130 col7" >0.97979</td>
      <td id="T_c6ca4_row130_col8" class="data row130 col8" >0.0217285</td>
      <td id="T_c6ca4_row130_col9" class="data row130 col9" >1024.000000</td>
      <td id="T_c6ca4_row130_col10" class="data row130 col10" >torch.float32</td>
      <td id="T_c6ca4_row130_col11" class="data row130 col11" >(1024,)</td>
      <td id="T_c6ca4_row130_col12" class="data row130 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row131" class="row_heading level0 row131" >131</th>
      <td id="T_c6ca4_row131_col0" class="data row131 col0" >encoder.layers.15.self_attn_layer_norm.weight</td>
      <td id="T_c6ca4_row131_col1" class="data row131 col1" >encoder.layers.15.self_attn_layer_norm.weight</td>
      <td id="T_c6ca4_row131_col2" class="data row131 col2" >av_romanizer.w2v_model.encoder.layers.15.self_attn_layer_norm.weight</td>
      <td id="T_c6ca4_row131_col3" class="data row131 col3" >0.414952</td>
      <td id="T_c6ca4_row131_col4" class="data row131 col4" >6.93085</td>
      <td id="T_c6ca4_row131_col5" class="data row131 col5" >6.98964</td>
      <td id="T_c6ca4_row131_col6" class="data row131 col6" >0.0598702</td>
      <td id="T_c6ca4_row131_col7" class="data row131 col7" >0.998259</td>
      <td id="T_c6ca4_row131_col8" class="data row131 col8" >0.0563965</td>
      <td id="T_c6ca4_row131_col9" class="data row131 col9" >1024.000000</td>
      <td id="T_c6ca4_row131_col10" class="data row131 col10" >torch.float32</td>
      <td id="T_c6ca4_row131_col11" class="data row131 col11" >(1024,)</td>
      <td id="T_c6ca4_row131_col12" class="data row131 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row132" class="row_heading level0 row132" >132</th>
      <td id="T_c6ca4_row132_col0" class="data row132 col0" >encoder.layers.16.fc1.bias</td>
      <td id="T_c6ca4_row132_col1" class="data row132 col1" >encoder.layers.16.fc1.bias</td>
      <td id="T_c6ca4_row132_col2" class="data row132 col2" >av_romanizer.w2v_model.encoder.layers.16.fc1.bias</td>
      <td id="T_c6ca4_row132_col3" class="data row132 col3" >0.716511</td>
      <td id="T_c6ca4_row132_col4" class="data row132 col4" >3.40478</td>
      <td id="T_c6ca4_row132_col5" class="data row132 col5" >3.23221</td>
      <td id="T_c6ca4_row132_col6" class="data row132 col6" >0.210442</td>
      <td id="T_c6ca4_row132_col7" class="data row132 col7" >0.978028</td>
      <td id="T_c6ca4_row132_col8" class="data row132 col8" >0.0390701</td>
      <td id="T_c6ca4_row132_col9" class="data row132 col9" >4096.000000</td>
      <td id="T_c6ca4_row132_col10" class="data row132 col10" >torch.float32</td>
      <td id="T_c6ca4_row132_col11" class="data row132 col11" >(4096,)</td>
      <td id="T_c6ca4_row132_col12" class="data row132 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row133" class="row_heading level0 row133" >133</th>
      <td id="T_c6ca4_row133_col0" class="data row133 col0" >encoder.layers.16.fc1.weight</td>
      <td id="T_c6ca4_row133_col1" class="data row133 col1" >encoder.layers.16.fc1.weight</td>
      <td id="T_c6ca4_row133_col2" class="data row133 col2" >av_romanizer.w2v_model.encoder.layers.16.fc1.weight</td>
      <td id="T_c6ca4_row133_col3" class="data row133 col3" >66.5024</td>
      <td id="T_c6ca4_row133_col4" class="data row133 col4" >316.388</td>
      <td id="T_c6ca4_row133_col5" class="data row133 col5" >303.181</td>
      <td id="T_c6ca4_row133_col6" class="data row133 col6" >0.210193</td>
      <td id="T_c6ca4_row133_col7" class="data row133 col7" >0.977856</td>
      <td id="T_c6ca4_row133_col8" class="data row133 col8" >0.223999</td>
      <td id="T_c6ca4_row133_col9" class="data row133 col9" >4194304.000000</td>
      <td id="T_c6ca4_row133_col10" class="data row133 col10" >torch.float32</td>
      <td id="T_c6ca4_row133_col11" class="data row133 col11" >(4096, 1024)</td>
      <td id="T_c6ca4_row133_col12" class="data row133 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row134" class="row_heading level0 row134" >134</th>
      <td id="T_c6ca4_row134_col0" class="data row134 col0" >encoder.layers.16.fc2.bias</td>
      <td id="T_c6ca4_row134_col1" class="data row134 col1" >encoder.layers.16.fc2.bias</td>
      <td id="T_c6ca4_row134_col2" class="data row134 col2" >av_romanizer.w2v_model.encoder.layers.16.fc2.bias</td>
      <td id="T_c6ca4_row134_col3" class="data row134 col3" >0.410237</td>
      <td id="T_c6ca4_row134_col4" class="data row134 col4" >7.63082</td>
      <td id="T_c6ca4_row134_col5" class="data row134 col5" >7.32942</td>
      <td id="T_c6ca4_row134_col6" class="data row134 col6" >0.0537605</td>
      <td id="T_c6ca4_row134_col7" class="data row134 col7" >0.999308</td>
      <td id="T_c6ca4_row134_col8" class="data row134 col8" >0.0463867</td>
      <td id="T_c6ca4_row134_col9" class="data row134 col9" >1024.000000</td>
      <td id="T_c6ca4_row134_col10" class="data row134 col10" >torch.float32</td>
      <td id="T_c6ca4_row134_col11" class="data row134 col11" >(1024,)</td>
      <td id="T_c6ca4_row134_col12" class="data row134 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row135" class="row_heading level0 row135" >135</th>
      <td id="T_c6ca4_row135_col0" class="data row135 col0" >encoder.layers.16.fc2.weight</td>
      <td id="T_c6ca4_row135_col1" class="data row135 col1" >encoder.layers.16.fc2.weight</td>
      <td id="T_c6ca4_row135_col2" class="data row135 col2" >av_romanizer.w2v_model.encoder.layers.16.fc2.weight</td>
      <td id="T_c6ca4_row135_col3" class="data row135 col3" >71.1636</td>
      <td id="T_c6ca4_row135_col4" class="data row135 col4" >325.159</td>
      <td id="T_c6ca4_row135_col5" class="data row135 col5" >311.955</td>
      <td id="T_c6ca4_row135_col6" class="data row135 col6" >0.218858</td>
      <td id="T_c6ca4_row135_col7" class="data row135 col7" >0.975896</td>
      <td id="T_c6ca4_row135_col8" class="data row135 col8" >0.228516</td>
      <td id="T_c6ca4_row135_col9" class="data row135 col9" >4194304.000000</td>
      <td id="T_c6ca4_row135_col10" class="data row135 col10" >torch.float32</td>
      <td id="T_c6ca4_row135_col11" class="data row135 col11" >(1024, 4096)</td>
      <td id="T_c6ca4_row135_col12" class="data row135 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row136" class="row_heading level0 row136" >136</th>
      <td id="T_c6ca4_row136_col0" class="data row136 col0" >encoder.layers.16.final_layer_norm.bias</td>
      <td id="T_c6ca4_row136_col1" class="data row136 col1" >encoder.layers.16.final_layer_norm.bias</td>
      <td id="T_c6ca4_row136_col2" class="data row136 col2" >av_romanizer.w2v_model.encoder.layers.16.final_layer_norm.bias</td>
      <td id="T_c6ca4_row136_col3" class="data row136 col3" >0.398839</td>
      <td id="T_c6ca4_row136_col4" class="data row136 col4" >5.821</td>
      <td id="T_c6ca4_row136_col5" class="data row136 col5" >5.63211</td>
      <td id="T_c6ca4_row136_col6" class="data row136 col6" >0.0685174</td>
      <td id="T_c6ca4_row136_col7" class="data row136 col7" >0.998118</td>
      <td id="T_c6ca4_row136_col8" class="data row136 col8" >0.0498047</td>
      <td id="T_c6ca4_row136_col9" class="data row136 col9" >1024.000000</td>
      <td id="T_c6ca4_row136_col10" class="data row136 col10" >torch.float32</td>
      <td id="T_c6ca4_row136_col11" class="data row136 col11" >(1024,)</td>
      <td id="T_c6ca4_row136_col12" class="data row136 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row137" class="row_heading level0 row137" >137</th>
      <td id="T_c6ca4_row137_col0" class="data row137 col0" >encoder.layers.16.final_layer_norm.weight</td>
      <td id="T_c6ca4_row137_col1" class="data row137 col1" >encoder.layers.16.final_layer_norm.weight</td>
      <td id="T_c6ca4_row137_col2" class="data row137 col2" >av_romanizer.w2v_model.encoder.layers.16.final_layer_norm.weight</td>
      <td id="T_c6ca4_row137_col3" class="data row137 col3" >0.933115</td>
      <td id="T_c6ca4_row137_col4" class="data row137 col4" >21.112</td>
      <td id="T_c6ca4_row137_col5" class="data row137 col5" >20.649</td>
      <td id="T_c6ca4_row137_col6" class="data row137 col6" >0.0441983</td>
      <td id="T_c6ca4_row137_col7" class="data row137 col7" >0.999247</td>
      <td id="T_c6ca4_row137_col8" class="data row137 col8" >0.115234</td>
      <td id="T_c6ca4_row137_col9" class="data row137 col9" >1024.000000</td>
      <td id="T_c6ca4_row137_col10" class="data row137 col10" >torch.float32</td>
      <td id="T_c6ca4_row137_col11" class="data row137 col11" >(1024,)</td>
      <td id="T_c6ca4_row137_col12" class="data row137 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row138" class="row_heading level0 row138" >138</th>
      <td id="T_c6ca4_row138_col0" class="data row138 col0" >encoder.layers.16.self_attn.k_proj.bias</td>
      <td id="T_c6ca4_row138_col1" class="data row138 col1" >encoder.layers.16.self_attn.k_proj.bias</td>
      <td id="T_c6ca4_row138_col2" class="data row138 col2" >av_romanizer.w2v_model.encoder.layers.16.self_attn.k_proj.bias</td>
      <td id="T_c6ca4_row138_col3" class="data row138 col3" >0.0156533</td>
      <td id="T_c6ca4_row138_col4" class="data row138 col4" >0.082463</td>
      <td id="T_c6ca4_row138_col5" class="data row138 col5" >0.0792897</td>
      <td id="T_c6ca4_row138_col6" class="data row138 col6" >0.189822</td>
      <td id="T_c6ca4_row138_col7" class="data row138 col7" >0.982033</td>
      <td id="T_c6ca4_row138_col8" class="data row138 col8" >0.00229549</td>
      <td id="T_c6ca4_row138_col9" class="data row138 col9" >1024.000000</td>
      <td id="T_c6ca4_row138_col10" class="data row138 col10" >torch.float32</td>
      <td id="T_c6ca4_row138_col11" class="data row138 col11" >(1024,)</td>
      <td id="T_c6ca4_row138_col12" class="data row138 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row139" class="row_heading level0 row139" >139</th>
      <td id="T_c6ca4_row139_col0" class="data row139 col0" >encoder.layers.16.self_attn.k_proj.weight</td>
      <td id="T_c6ca4_row139_col1" class="data row139 col1" >encoder.layers.16.self_attn.k_proj.weight</td>
      <td id="T_c6ca4_row139_col2" class="data row139 col2" >av_romanizer.w2v_model.encoder.layers.16.self_attn.k_proj.weight</td>
      <td id="T_c6ca4_row139_col3" class="data row139 col3" >27.1727</td>
      <td id="T_c6ca4_row139_col4" class="data row139 col4" >138.07</td>
      <td id="T_c6ca4_row139_col5" class="data row139 col5" >132.839</td>
      <td id="T_c6ca4_row139_col6" class="data row139 col6" >0.196804</td>
      <td id="T_c6ca4_row139_col7" class="data row139 col7" >0.980617</td>
      <td id="T_c6ca4_row139_col8" class="data row139 col8" >0.149017</td>
      <td id="T_c6ca4_row139_col9" class="data row139 col9" >1048576.000000</td>
      <td id="T_c6ca4_row139_col10" class="data row139 col10" >torch.float32</td>
      <td id="T_c6ca4_row139_col11" class="data row139 col11" >(1024, 1024)</td>
      <td id="T_c6ca4_row139_col12" class="data row139 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row140" class="row_heading level0 row140" >140</th>
      <td id="T_c6ca4_row140_col0" class="data row140 col0" >encoder.layers.16.self_attn.out_proj.bias</td>
      <td id="T_c6ca4_row140_col1" class="data row140 col1" >encoder.layers.16.self_attn.out_proj.bias</td>
      <td id="T_c6ca4_row140_col2" class="data row140 col2" >av_romanizer.w2v_model.encoder.layers.16.self_attn.out_proj.bias</td>
      <td id="T_c6ca4_row140_col3" class="data row140 col3" >0.330608</td>
      <td id="T_c6ca4_row140_col4" class="data row140 col4" >5.10086</td>
      <td id="T_c6ca4_row140_col5" class="data row140 col5" >4.89437</td>
      <td id="T_c6ca4_row140_col6" class="data row140 col6" >0.0648143</td>
      <td id="T_c6ca4_row140_col7" class="data row140 col7" >0.998665</td>
      <td id="T_c6ca4_row140_col8" class="data row140 col8" >0.0419922</td>
      <td id="T_c6ca4_row140_col9" class="data row140 col9" >1024.000000</td>
      <td id="T_c6ca4_row140_col10" class="data row140 col10" >torch.float32</td>
      <td id="T_c6ca4_row140_col11" class="data row140 col11" >(1024,)</td>
      <td id="T_c6ca4_row140_col12" class="data row140 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row141" class="row_heading level0 row141" >141</th>
      <td id="T_c6ca4_row141_col0" class="data row141 col0" >encoder.layers.16.self_attn.out_proj.weight</td>
      <td id="T_c6ca4_row141_col1" class="data row141 col1" >encoder.layers.16.self_attn.out_proj.weight</td>
      <td id="T_c6ca4_row141_col2" class="data row141 col2" >av_romanizer.w2v_model.encoder.layers.16.self_attn.out_proj.weight</td>
      <td id="T_c6ca4_row141_col3" class="data row141 col3" >27.559</td>
      <td id="T_c6ca4_row141_col4" class="data row141 col4" >148.48</td>
      <td id="T_c6ca4_row141_col5" class="data row141 col5" >141.589</td>
      <td id="T_c6ca4_row141_col6" class="data row141 col6" >0.185607</td>
      <td id="T_c6ca4_row141_col7" class="data row141 col7" >0.983066</td>
      <td id="T_c6ca4_row141_col8" class="data row141 col8" >0.146088</td>
      <td id="T_c6ca4_row141_col9" class="data row141 col9" >1048576.000000</td>
      <td id="T_c6ca4_row141_col10" class="data row141 col10" >torch.float32</td>
      <td id="T_c6ca4_row141_col11" class="data row141 col11" >(1024, 1024)</td>
      <td id="T_c6ca4_row141_col12" class="data row141 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row142" class="row_heading level0 row142" >142</th>
      <td id="T_c6ca4_row142_col0" class="data row142 col0" >encoder.layers.16.self_attn.q_proj.bias</td>
      <td id="T_c6ca4_row142_col1" class="data row142 col1" >encoder.layers.16.self_attn.q_proj.bias</td>
      <td id="T_c6ca4_row142_col2" class="data row142 col2" >av_romanizer.w2v_model.encoder.layers.16.self_attn.q_proj.bias</td>
      <td id="T_c6ca4_row142_col3" class="data row142 col3" >0.907399</td>
      <td id="T_c6ca4_row142_col4" class="data row142 col4" >8.52441</td>
      <td id="T_c6ca4_row142_col5" class="data row142 col5" >8.23966</td>
      <td id="T_c6ca4_row142_col6" class="data row142 col6" >0.106447</td>
      <td id="T_c6ca4_row142_col7" class="data row142 col7" >0.994716</td>
      <td id="T_c6ca4_row142_col8" class="data row142 col8" >0.158203</td>
      <td id="T_c6ca4_row142_col9" class="data row142 col9" >1024.000000</td>
      <td id="T_c6ca4_row142_col10" class="data row142 col10" >torch.float32</td>
      <td id="T_c6ca4_row142_col11" class="data row142 col11" >(1024,)</td>
      <td id="T_c6ca4_row142_col12" class="data row142 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row143" class="row_heading level0 row143" >143</th>
      <td id="T_c6ca4_row143_col0" class="data row143 col0" >encoder.layers.16.self_attn.q_proj.weight</td>
      <td id="T_c6ca4_row143_col1" class="data row143 col1" >encoder.layers.16.self_attn.q_proj.weight</td>
      <td id="T_c6ca4_row143_col2" class="data row143 col2" >av_romanizer.w2v_model.encoder.layers.16.self_attn.q_proj.weight</td>
      <td id="T_c6ca4_row143_col3" class="data row143 col3" >27.2213</td>
      <td id="T_c6ca4_row143_col4" class="data row143 col4" >136.278</td>
      <td id="T_c6ca4_row143_col5" class="data row143 col5" >131.22</td>
      <td id="T_c6ca4_row143_col6" class="data row143 col6" >0.199747</td>
      <td id="T_c6ca4_row143_col7" class="data row143 col7" >0.979997</td>
      <td id="T_c6ca4_row143_col8" class="data row143 col8" >0.149292</td>
      <td id="T_c6ca4_row143_col9" class="data row143 col9" >1048576.000000</td>
      <td id="T_c6ca4_row143_col10" class="data row143 col10" >torch.float32</td>
      <td id="T_c6ca4_row143_col11" class="data row143 col11" >(1024, 1024)</td>
      <td id="T_c6ca4_row143_col12" class="data row143 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row144" class="row_heading level0 row144" >144</th>
      <td id="T_c6ca4_row144_col0" class="data row144 col0" >encoder.layers.16.self_attn.v_proj.bias</td>
      <td id="T_c6ca4_row144_col1" class="data row144 col1" >encoder.layers.16.self_attn.v_proj.bias</td>
      <td id="T_c6ca4_row144_col2" class="data row144 col2" >av_romanizer.w2v_model.encoder.layers.16.self_attn.v_proj.bias</td>
      <td id="T_c6ca4_row144_col3" class="data row144 col3" >0.255727</td>
      <td id="T_c6ca4_row144_col4" class="data row144 col4" >1.98481</td>
      <td id="T_c6ca4_row144_col5" class="data row144 col5" >1.88158</td>
      <td id="T_c6ca4_row144_col6" class="data row144 col6" >0.128842</td>
      <td id="T_c6ca4_row144_col7" class="data row144 col7" >0.992671</td>
      <td id="T_c6ca4_row144_col8" class="data row144 col8" >0.0288086</td>
      <td id="T_c6ca4_row144_col9" class="data row144 col9" >1024.000000</td>
      <td id="T_c6ca4_row144_col10" class="data row144 col10" >torch.float32</td>
      <td id="T_c6ca4_row144_col11" class="data row144 col11" >(1024,)</td>
      <td id="T_c6ca4_row144_col12" class="data row144 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row145" class="row_heading level0 row145" >145</th>
      <td id="T_c6ca4_row145_col0" class="data row145 col0" >encoder.layers.16.self_attn.v_proj.weight</td>
      <td id="T_c6ca4_row145_col1" class="data row145 col1" >encoder.layers.16.self_attn.v_proj.weight</td>
      <td id="T_c6ca4_row145_col2" class="data row145 col2" >av_romanizer.w2v_model.encoder.layers.16.self_attn.v_proj.weight</td>
      <td id="T_c6ca4_row145_col3" class="data row145 col3" >23.6551</td>
      <td id="T_c6ca4_row145_col4" class="data row145 col4" >151.845</td>
      <td id="T_c6ca4_row145_col5" class="data row145 col5" >144.839</td>
      <td id="T_c6ca4_row145_col6" class="data row145 col6" >0.155785</td>
      <td id="T_c6ca4_row145_col7" class="data row145 col7" >0.988394</td>
      <td id="T_c6ca4_row145_col8" class="data row145 col8" >0.143829</td>
      <td id="T_c6ca4_row145_col9" class="data row145 col9" >1048576.000000</td>
      <td id="T_c6ca4_row145_col10" class="data row145 col10" >torch.float32</td>
      <td id="T_c6ca4_row145_col11" class="data row145 col11" >(1024, 1024)</td>
      <td id="T_c6ca4_row145_col12" class="data row145 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row146" class="row_heading level0 row146" >146</th>
      <td id="T_c6ca4_row146_col0" class="data row146 col0" >encoder.layers.16.self_attn_layer_norm.bias</td>
      <td id="T_c6ca4_row146_col1" class="data row146 col1" >encoder.layers.16.self_attn_layer_norm.bias</td>
      <td id="T_c6ca4_row146_col2" class="data row146 col2" >av_romanizer.w2v_model.encoder.layers.16.self_attn_layer_norm.bias</td>
      <td id="T_c6ca4_row146_col3" class="data row146 col3" >0.219869</td>
      <td id="T_c6ca4_row146_col4" class="data row146 col4" >1.08828</td>
      <td id="T_c6ca4_row146_col5" class="data row146 col5" >1.0703</td>
      <td id="T_c6ca4_row146_col6" class="data row146 col6" >0.202033</td>
      <td id="T_c6ca4_row146_col7" class="data row146 col7" >0.979387</td>
      <td id="T_c6ca4_row146_col8" class="data row146 col8" >0.0225296</td>
      <td id="T_c6ca4_row146_col9" class="data row146 col9" >1024.000000</td>
      <td id="T_c6ca4_row146_col10" class="data row146 col10" >torch.float32</td>
      <td id="T_c6ca4_row146_col11" class="data row146 col11" >(1024,)</td>
      <td id="T_c6ca4_row146_col12" class="data row146 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row147" class="row_heading level0 row147" >147</th>
      <td id="T_c6ca4_row147_col0" class="data row147 col0" >encoder.layers.16.self_attn_layer_norm.weight</td>
      <td id="T_c6ca4_row147_col1" class="data row147 col1" >encoder.layers.16.self_attn_layer_norm.weight</td>
      <td id="T_c6ca4_row147_col2" class="data row147 col2" >av_romanizer.w2v_model.encoder.layers.16.self_attn_layer_norm.weight</td>
      <td id="T_c6ca4_row147_col3" class="data row147 col3" >0.462912</td>
      <td id="T_c6ca4_row147_col4" class="data row147 col4" >7.13903</td>
      <td id="T_c6ca4_row147_col5" class="data row147 col5" >7.1261</td>
      <td id="T_c6ca4_row147_col6" class="data row147 col6" >0.0648425</td>
      <td id="T_c6ca4_row147_col7" class="data row147 col7" >0.997896</td>
      <td id="T_c6ca4_row147_col8" class="data row147 col8" >0.0418701</td>
      <td id="T_c6ca4_row147_col9" class="data row147 col9" >1024.000000</td>
      <td id="T_c6ca4_row147_col10" class="data row147 col10" >torch.float32</td>
      <td id="T_c6ca4_row147_col11" class="data row147 col11" >(1024,)</td>
      <td id="T_c6ca4_row147_col12" class="data row147 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row148" class="row_heading level0 row148" >148</th>
      <td id="T_c6ca4_row148_col0" class="data row148 col0" >encoder.layers.17.fc1.bias</td>
      <td id="T_c6ca4_row148_col1" class="data row148 col1" >encoder.layers.17.fc1.bias</td>
      <td id="T_c6ca4_row148_col2" class="data row148 col2" >av_romanizer.w2v_model.encoder.layers.17.fc1.bias</td>
      <td id="T_c6ca4_row148_col3" class="data row148 col3" >0.694658</td>
      <td id="T_c6ca4_row148_col4" class="data row148 col4" >4.11644</td>
      <td id="T_c6ca4_row148_col5" class="data row148 col5" >3.87116</td>
      <td id="T_c6ca4_row148_col6" class="data row148 col6" >0.168752</td>
      <td id="T_c6ca4_row148_col7" class="data row148 col7" >0.986747</td>
      <td id="T_c6ca4_row148_col8" class="data row148 col8" >0.0411377</td>
      <td id="T_c6ca4_row148_col9" class="data row148 col9" >4096.000000</td>
      <td id="T_c6ca4_row148_col10" class="data row148 col10" >torch.float32</td>
      <td id="T_c6ca4_row148_col11" class="data row148 col11" >(4096,)</td>
      <td id="T_c6ca4_row148_col12" class="data row148 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row149" class="row_heading level0 row149" >149</th>
      <td id="T_c6ca4_row149_col0" class="data row149 col0" >encoder.layers.17.fc1.weight</td>
      <td id="T_c6ca4_row149_col1" class="data row149 col1" >encoder.layers.17.fc1.weight</td>
      <td id="T_c6ca4_row149_col2" class="data row149 col2" >av_romanizer.w2v_model.encoder.layers.17.fc1.weight</td>
      <td id="T_c6ca4_row149_col3" class="data row149 col3" >67.6004</td>
      <td id="T_c6ca4_row149_col4" class="data row149 col4" >325.56</td>
      <td id="T_c6ca4_row149_col5" class="data row149 col5" >312.314</td>
      <td id="T_c6ca4_row149_col6" class="data row149 col6" >0.207643</td>
      <td id="T_c6ca4_row149_col7" class="data row149 col7" >0.978391</td>
      <td id="T_c6ca4_row149_col8" class="data row149 col8" >0.244751</td>
      <td id="T_c6ca4_row149_col9" class="data row149 col9" >4194304.000000</td>
      <td id="T_c6ca4_row149_col10" class="data row149 col10" >torch.float32</td>
      <td id="T_c6ca4_row149_col11" class="data row149 col11" >(4096, 1024)</td>
      <td id="T_c6ca4_row149_col12" class="data row149 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row150" class="row_heading level0 row150" >150</th>
      <td id="T_c6ca4_row150_col0" class="data row150 col0" >encoder.layers.17.fc2.bias</td>
      <td id="T_c6ca4_row150_col1" class="data row150 col1" >encoder.layers.17.fc2.bias</td>
      <td id="T_c6ca4_row150_col2" class="data row150 col2" >av_romanizer.w2v_model.encoder.layers.17.fc2.bias</td>
      <td id="T_c6ca4_row150_col3" class="data row150 col3" >0.535614</td>
      <td id="T_c6ca4_row150_col4" class="data row150 col4" >9.6322</td>
      <td id="T_c6ca4_row150_col5" class="data row150 col5" >9.21359</td>
      <td id="T_c6ca4_row150_col6" class="data row150 col6" >0.0556066</td>
      <td id="T_c6ca4_row150_col7" class="data row150 col7" >0.999371</td>
      <td id="T_c6ca4_row150_col8" class="data row150 col8" >0.0600586</td>
      <td id="T_c6ca4_row150_col9" class="data row150 col9" >1024.000000</td>
      <td id="T_c6ca4_row150_col10" class="data row150 col10" >torch.float32</td>
      <td id="T_c6ca4_row150_col11" class="data row150 col11" >(1024,)</td>
      <td id="T_c6ca4_row150_col12" class="data row150 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row151" class="row_heading level0 row151" >151</th>
      <td id="T_c6ca4_row151_col0" class="data row151 col0" >encoder.layers.17.fc2.weight</td>
      <td id="T_c6ca4_row151_col1" class="data row151 col1" >encoder.layers.17.fc2.weight</td>
      <td id="T_c6ca4_row151_col2" class="data row151 col2" >av_romanizer.w2v_model.encoder.layers.17.fc2.weight</td>
      <td id="T_c6ca4_row151_col3" class="data row151 col3" >73.5452</td>
      <td id="T_c6ca4_row151_col4" class="data row151 col4" >351.911</td>
      <td id="T_c6ca4_row151_col5" class="data row151 col5" >337.929</td>
      <td id="T_c6ca4_row151_col6" class="data row151 col6" >0.208988</td>
      <td id="T_c6ca4_row151_col7" class="data row151 col7" >0.97808</td>
      <td id="T_c6ca4_row151_col8" class="data row151 col8" >0.302246</td>
      <td id="T_c6ca4_row151_col9" class="data row151 col9" >4194304.000000</td>
      <td id="T_c6ca4_row151_col10" class="data row151 col10" >torch.float32</td>
      <td id="T_c6ca4_row151_col11" class="data row151 col11" >(1024, 4096)</td>
      <td id="T_c6ca4_row151_col12" class="data row151 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row152" class="row_heading level0 row152" >152</th>
      <td id="T_c6ca4_row152_col0" class="data row152 col0" >encoder.layers.17.final_layer_norm.bias</td>
      <td id="T_c6ca4_row152_col1" class="data row152 col1" >encoder.layers.17.final_layer_norm.bias</td>
      <td id="T_c6ca4_row152_col2" class="data row152 col2" >av_romanizer.w2v_model.encoder.layers.17.final_layer_norm.bias</td>
      <td id="T_c6ca4_row152_col3" class="data row152 col3" >0.450619</td>
      <td id="T_c6ca4_row152_col4" class="data row152 col4" >9.21097</td>
      <td id="T_c6ca4_row152_col5" class="data row152 col5" >9.06198</td>
      <td id="T_c6ca4_row152_col6" class="data row152 col6" >0.048922</td>
      <td id="T_c6ca4_row152_col7" class="data row152 col7" >0.998917</td>
      <td id="T_c6ca4_row152_col8" class="data row152 col8" >0.0537109</td>
      <td id="T_c6ca4_row152_col9" class="data row152 col9" >1024.000000</td>
      <td id="T_c6ca4_row152_col10" class="data row152 col10" >torch.float32</td>
      <td id="T_c6ca4_row152_col11" class="data row152 col11" >(1024,)</td>
      <td id="T_c6ca4_row152_col12" class="data row152 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row153" class="row_heading level0 row153" >153</th>
      <td id="T_c6ca4_row153_col0" class="data row153 col0" >encoder.layers.17.final_layer_norm.weight</td>
      <td id="T_c6ca4_row153_col1" class="data row153 col1" >encoder.layers.17.final_layer_norm.weight</td>
      <td id="T_c6ca4_row153_col2" class="data row153 col2" >av_romanizer.w2v_model.encoder.layers.17.final_layer_norm.weight</td>
      <td id="T_c6ca4_row153_col3" class="data row153 col3" >1.29255</td>
      <td id="T_c6ca4_row153_col4" class="data row153 col4" >38.2847</td>
      <td id="T_c6ca4_row153_col5" class="data row153 col5" >37.3791</td>
      <td id="T_c6ca4_row153_col6" class="data row153 col6" >0.0337616</td>
      <td id="T_c6ca4_row153_col7" class="data row153 col7" >0.999703</td>
      <td id="T_c6ca4_row153_col8" class="data row153 col8" >0.12207</td>
      <td id="T_c6ca4_row153_col9" class="data row153 col9" >1024.000000</td>
      <td id="T_c6ca4_row153_col10" class="data row153 col10" >torch.float32</td>
      <td id="T_c6ca4_row153_col11" class="data row153 col11" >(1024,)</td>
      <td id="T_c6ca4_row153_col12" class="data row153 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row154" class="row_heading level0 row154" >154</th>
      <td id="T_c6ca4_row154_col0" class="data row154 col0" >encoder.layers.17.self_attn.k_proj.bias</td>
      <td id="T_c6ca4_row154_col1" class="data row154 col1" >encoder.layers.17.self_attn.k_proj.bias</td>
      <td id="T_c6ca4_row154_col2" class="data row154 col2" >av_romanizer.w2v_model.encoder.layers.17.self_attn.k_proj.bias</td>
      <td id="T_c6ca4_row154_col3" class="data row154 col3" >0.0151394</td>
      <td id="T_c6ca4_row154_col4" class="data row154 col4" >0.188097</td>
      <td id="T_c6ca4_row154_col5" class="data row154 col5" >0.187953</td>
      <td id="T_c6ca4_row154_col6" class="data row154 col6" >0.0804872</td>
      <td id="T_c6ca4_row154_col7" class="data row154 col7" >0.996759</td>
      <td id="T_c6ca4_row154_col8" class="data row154 col8" >0.00223875</td>
      <td id="T_c6ca4_row154_col9" class="data row154 col9" >1024.000000</td>
      <td id="T_c6ca4_row154_col10" class="data row154 col10" >torch.float32</td>
      <td id="T_c6ca4_row154_col11" class="data row154 col11" >(1024,)</td>
      <td id="T_c6ca4_row154_col12" class="data row154 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row155" class="row_heading level0 row155" >155</th>
      <td id="T_c6ca4_row155_col0" class="data row155 col0" >encoder.layers.17.self_attn.k_proj.weight</td>
      <td id="T_c6ca4_row155_col1" class="data row155 col1" >encoder.layers.17.self_attn.k_proj.weight</td>
      <td id="T_c6ca4_row155_col2" class="data row155 col2" >av_romanizer.w2v_model.encoder.layers.17.self_attn.k_proj.weight</td>
      <td id="T_c6ca4_row155_col3" class="data row155 col3" >27.5671</td>
      <td id="T_c6ca4_row155_col4" class="data row155 col4" >139.505</td>
      <td id="T_c6ca4_row155_col5" class="data row155 col5" >134.515</td>
      <td id="T_c6ca4_row155_col6" class="data row155 col6" >0.197606</td>
      <td id="T_c6ca4_row155_col7" class="data row155 col7" >0.980415</td>
      <td id="T_c6ca4_row155_col8" class="data row155 col8" >0.186523</td>
      <td id="T_c6ca4_row155_col9" class="data row155 col9" >1048576.000000</td>
      <td id="T_c6ca4_row155_col10" class="data row155 col10" >torch.float32</td>
      <td id="T_c6ca4_row155_col11" class="data row155 col11" >(1024, 1024)</td>
      <td id="T_c6ca4_row155_col12" class="data row155 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row156" class="row_heading level0 row156" >156</th>
      <td id="T_c6ca4_row156_col0" class="data row156 col0" >encoder.layers.17.self_attn.out_proj.bias</td>
      <td id="T_c6ca4_row156_col1" class="data row156 col1" >encoder.layers.17.self_attn.out_proj.bias</td>
      <td id="T_c6ca4_row156_col2" class="data row156 col2" >av_romanizer.w2v_model.encoder.layers.17.self_attn.out_proj.bias</td>
      <td id="T_c6ca4_row156_col3" class="data row156 col3" >0.330326</td>
      <td id="T_c6ca4_row156_col4" class="data row156 col4" >5.64344</td>
      <td id="T_c6ca4_row156_col5" class="data row156 col5" >5.4516</td>
      <td id="T_c6ca4_row156_col6" class="data row156 col6" >0.0585328</td>
      <td id="T_c6ca4_row156_col7" class="data row156 col7" >0.998825</td>
      <td id="T_c6ca4_row156_col8" class="data row156 col8" >0.0394287</td>
      <td id="T_c6ca4_row156_col9" class="data row156 col9" >1024.000000</td>
      <td id="T_c6ca4_row156_col10" class="data row156 col10" >torch.float32</td>
      <td id="T_c6ca4_row156_col11" class="data row156 col11" >(1024,)</td>
      <td id="T_c6ca4_row156_col12" class="data row156 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row157" class="row_heading level0 row157" >157</th>
      <td id="T_c6ca4_row157_col0" class="data row157 col0" >encoder.layers.17.self_attn.out_proj.weight</td>
      <td id="T_c6ca4_row157_col1" class="data row157 col1" >encoder.layers.17.self_attn.out_proj.weight</td>
      <td id="T_c6ca4_row157_col2" class="data row157 col2" >av_romanizer.w2v_model.encoder.layers.17.self_attn.out_proj.weight</td>
      <td id="T_c6ca4_row157_col3" class="data row157 col3" >29.7823</td>
      <td id="T_c6ca4_row157_col4" class="data row157 col4" >155.944</td>
      <td id="T_c6ca4_row157_col5" class="data row157 col5" >148.565</td>
      <td id="T_c6ca4_row157_col6" class="data row157 col6" >0.19098</td>
      <td id="T_c6ca4_row157_col7" class="data row157 col7" >0.982033</td>
      <td id="T_c6ca4_row157_col8" class="data row157 col8" >0.157227</td>
      <td id="T_c6ca4_row157_col9" class="data row157 col9" >1048576.000000</td>
      <td id="T_c6ca4_row157_col10" class="data row157 col10" >torch.float32</td>
      <td id="T_c6ca4_row157_col11" class="data row157 col11" >(1024, 1024)</td>
      <td id="T_c6ca4_row157_col12" class="data row157 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row158" class="row_heading level0 row158" >158</th>
      <td id="T_c6ca4_row158_col0" class="data row158 col0" >encoder.layers.17.self_attn.q_proj.bias</td>
      <td id="T_c6ca4_row158_col1" class="data row158 col1" >encoder.layers.17.self_attn.q_proj.bias</td>
      <td id="T_c6ca4_row158_col2" class="data row158 col2" >av_romanizer.w2v_model.encoder.layers.17.self_attn.q_proj.bias</td>
      <td id="T_c6ca4_row158_col3" class="data row158 col3" >0.887896</td>
      <td id="T_c6ca4_row158_col4" class="data row158 col4" >8.73061</td>
      <td id="T_c6ca4_row158_col5" class="data row158 col5" >8.52131</td>
      <td id="T_c6ca4_row158_col6" class="data row158 col6" >0.101699</td>
      <td id="T_c6ca4_row158_col7" class="data row158 col7" >0.994996</td>
      <td id="T_c6ca4_row158_col8" class="data row158 col8" >0.154297</td>
      <td id="T_c6ca4_row158_col9" class="data row158 col9" >1024.000000</td>
      <td id="T_c6ca4_row158_col10" class="data row158 col10" >torch.float32</td>
      <td id="T_c6ca4_row158_col11" class="data row158 col11" >(1024,)</td>
      <td id="T_c6ca4_row158_col12" class="data row158 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row159" class="row_heading level0 row159" >159</th>
      <td id="T_c6ca4_row159_col0" class="data row159 col0" >encoder.layers.17.self_attn.q_proj.weight</td>
      <td id="T_c6ca4_row159_col1" class="data row159 col1" >encoder.layers.17.self_attn.q_proj.weight</td>
      <td id="T_c6ca4_row159_col2" class="data row159 col2" >av_romanizer.w2v_model.encoder.layers.17.self_attn.q_proj.weight</td>
      <td id="T_c6ca4_row159_col3" class="data row159 col3" >27.9809</td>
      <td id="T_c6ca4_row159_col4" class="data row159 col4" >137.103</td>
      <td id="T_c6ca4_row159_col5" class="data row159 col5" >132.25</td>
      <td id="T_c6ca4_row159_col6" class="data row159 col6" >0.204087</td>
      <td id="T_c6ca4_row159_col7" class="data row159 col7" >0.979059</td>
      <td id="T_c6ca4_row159_col8" class="data row159 col8" >0.176697</td>
      <td id="T_c6ca4_row159_col9" class="data row159 col9" >1048576.000000</td>
      <td id="T_c6ca4_row159_col10" class="data row159 col10" >torch.float32</td>
      <td id="T_c6ca4_row159_col11" class="data row159 col11" >(1024, 1024)</td>
      <td id="T_c6ca4_row159_col12" class="data row159 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row160" class="row_heading level0 row160" >160</th>
      <td id="T_c6ca4_row160_col0" class="data row160 col0" >encoder.layers.17.self_attn.v_proj.bias</td>
      <td id="T_c6ca4_row160_col1" class="data row160 col1" >encoder.layers.17.self_attn.v_proj.bias</td>
      <td id="T_c6ca4_row160_col2" class="data row160 col2" >av_romanizer.w2v_model.encoder.layers.17.self_attn.v_proj.bias</td>
      <td id="T_c6ca4_row160_col3" class="data row160 col3" >0.279612</td>
      <td id="T_c6ca4_row160_col4" class="data row160 col4" >2.43945</td>
      <td id="T_c6ca4_row160_col5" class="data row160 col5" >2.33943</td>
      <td id="T_c6ca4_row160_col6" class="data row160 col6" >0.114621</td>
      <td id="T_c6ca4_row160_col7" class="data row160 col7" >0.994027</td>
      <td id="T_c6ca4_row160_col8" class="data row160 col8" >0.0364685</td>
      <td id="T_c6ca4_row160_col9" class="data row160 col9" >1024.000000</td>
      <td id="T_c6ca4_row160_col10" class="data row160 col10" >torch.float32</td>
      <td id="T_c6ca4_row160_col11" class="data row160 col11" >(1024,)</td>
      <td id="T_c6ca4_row160_col12" class="data row160 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row161" class="row_heading level0 row161" >161</th>
      <td id="T_c6ca4_row161_col0" class="data row161 col0" >encoder.layers.17.self_attn.v_proj.weight</td>
      <td id="T_c6ca4_row161_col1" class="data row161 col1" >encoder.layers.17.self_attn.v_proj.weight</td>
      <td id="T_c6ca4_row161_col2" class="data row161 col2" >av_romanizer.w2v_model.encoder.layers.17.self_attn.v_proj.weight</td>
      <td id="T_c6ca4_row161_col3" class="data row161 col3" >25.5187</td>
      <td id="T_c6ca4_row161_col4" class="data row161 col4" >159.359</td>
      <td id="T_c6ca4_row161_col5" class="data row161 col5" >151.904</td>
      <td id="T_c6ca4_row161_col6" class="data row161 col6" >0.160133</td>
      <td id="T_c6ca4_row161_col7" class="data row161 col7" >0.987697</td>
      <td id="T_c6ca4_row161_col8" class="data row161 col8" >0.139038</td>
      <td id="T_c6ca4_row161_col9" class="data row161 col9" >1048576.000000</td>
      <td id="T_c6ca4_row161_col10" class="data row161 col10" >torch.float32</td>
      <td id="T_c6ca4_row161_col11" class="data row161 col11" >(1024, 1024)</td>
      <td id="T_c6ca4_row161_col12" class="data row161 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row162" class="row_heading level0 row162" >162</th>
      <td id="T_c6ca4_row162_col0" class="data row162 col0" >encoder.layers.17.self_attn_layer_norm.bias</td>
      <td id="T_c6ca4_row162_col1" class="data row162 col1" >encoder.layers.17.self_attn_layer_norm.bias</td>
      <td id="T_c6ca4_row162_col2" class="data row162 col2" >av_romanizer.w2v_model.encoder.layers.17.self_attn_layer_norm.bias</td>
      <td id="T_c6ca4_row162_col3" class="data row162 col3" >0.237474</td>
      <td id="T_c6ca4_row162_col4" class="data row162 col4" >1.26533</td>
      <td id="T_c6ca4_row162_col5" class="data row162 col5" >1.24643</td>
      <td id="T_c6ca4_row162_col6" class="data row162 col6" >0.187678</td>
      <td id="T_c6ca4_row162_col7" class="data row162 col7" >0.982235</td>
      <td id="T_c6ca4_row162_col8" class="data row162 col8" >0.022583</td>
      <td id="T_c6ca4_row162_col9" class="data row162 col9" >1024.000000</td>
      <td id="T_c6ca4_row162_col10" class="data row162 col10" >torch.float32</td>
      <td id="T_c6ca4_row162_col11" class="data row162 col11" >(1024,)</td>
      <td id="T_c6ca4_row162_col12" class="data row162 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row163" class="row_heading level0 row163" >163</th>
      <td id="T_c6ca4_row163_col0" class="data row163 col0" >encoder.layers.17.self_attn_layer_norm.weight</td>
      <td id="T_c6ca4_row163_col1" class="data row163 col1" >encoder.layers.17.self_attn_layer_norm.weight</td>
      <td id="T_c6ca4_row163_col2" class="data row163 col2" >av_romanizer.w2v_model.encoder.layers.17.self_attn_layer_norm.weight</td>
      <td id="T_c6ca4_row163_col3" class="data row163 col3" >0.486189</td>
      <td id="T_c6ca4_row163_col4" class="data row163 col4" >7.55239</td>
      <td id="T_c6ca4_row163_col5" class="data row163 col5" >7.53698</td>
      <td id="T_c6ca4_row163_col6" class="data row163 col6" >0.0643755</td>
      <td id="T_c6ca4_row163_col7" class="data row163 col7" >0.997926</td>
      <td id="T_c6ca4_row163_col8" class="data row163 col8" >0.0635986</td>
      <td id="T_c6ca4_row163_col9" class="data row163 col9" >1024.000000</td>
      <td id="T_c6ca4_row163_col10" class="data row163 col10" >torch.float32</td>
      <td id="T_c6ca4_row163_col11" class="data row163 col11" >(1024,)</td>
      <td id="T_c6ca4_row163_col12" class="data row163 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row164" class="row_heading level0 row164" >164</th>
      <td id="T_c6ca4_row164_col0" class="data row164 col0" >encoder.layers.18.fc1.bias</td>
      <td id="T_c6ca4_row164_col1" class="data row164 col1" >encoder.layers.18.fc1.bias</td>
      <td id="T_c6ca4_row164_col2" class="data row164 col2" >av_romanizer.w2v_model.encoder.layers.18.fc1.bias</td>
      <td id="T_c6ca4_row164_col3" class="data row164 col3" >0.67792</td>
      <td id="T_c6ca4_row164_col4" class="data row164 col4" >3.92645</td>
      <td id="T_c6ca4_row164_col5" class="data row164 col5" >3.73217</td>
      <td id="T_c6ca4_row164_col6" class="data row164 col6" >0.172654</td>
      <td id="T_c6ca4_row164_col7" class="data row164 col7" >0.985607</td>
      <td id="T_c6ca4_row164_col8" class="data row164 col8" >0.0480042</td>
      <td id="T_c6ca4_row164_col9" class="data row164 col9" >4096.000000</td>
      <td id="T_c6ca4_row164_col10" class="data row164 col10" >torch.float32</td>
      <td id="T_c6ca4_row164_col11" class="data row164 col11" >(4096,)</td>
      <td id="T_c6ca4_row164_col12" class="data row164 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row165" class="row_heading level0 row165" >165</th>
      <td id="T_c6ca4_row165_col0" class="data row165 col0" >encoder.layers.18.fc1.weight</td>
      <td id="T_c6ca4_row165_col1" class="data row165 col1" >encoder.layers.18.fc1.weight</td>
      <td id="T_c6ca4_row165_col2" class="data row165 col2" >av_romanizer.w2v_model.encoder.layers.18.fc1.weight</td>
      <td id="T_c6ca4_row165_col3" class="data row165 col3" >69.5189</td>
      <td id="T_c6ca4_row165_col4" class="data row165 col4" >322.68</td>
      <td id="T_c6ca4_row165_col5" class="data row165 col5" >309.683</td>
      <td id="T_c6ca4_row165_col6" class="data row165 col6" >0.215442</td>
      <td id="T_c6ca4_row165_col7" class="data row165 col7" >0.976664</td>
      <td id="T_c6ca4_row165_col8" class="data row165 col8" >0.245697</td>
      <td id="T_c6ca4_row165_col9" class="data row165 col9" >4194304.000000</td>
      <td id="T_c6ca4_row165_col10" class="data row165 col10" >torch.float32</td>
      <td id="T_c6ca4_row165_col11" class="data row165 col11" >(4096, 1024)</td>
      <td id="T_c6ca4_row165_col12" class="data row165 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row166" class="row_heading level0 row166" >166</th>
      <td id="T_c6ca4_row166_col0" class="data row166 col0" >encoder.layers.18.fc2.bias</td>
      <td id="T_c6ca4_row166_col1" class="data row166 col1" >encoder.layers.18.fc2.bias</td>
      <td id="T_c6ca4_row166_col2" class="data row166 col2" >av_romanizer.w2v_model.encoder.layers.18.fc2.bias</td>
      <td id="T_c6ca4_row166_col3" class="data row166 col3" >0.401458</td>
      <td id="T_c6ca4_row166_col4" class="data row166 col4" >7.20773</td>
      <td id="T_c6ca4_row166_col5" class="data row166 col5" >6.92999</td>
      <td id="T_c6ca4_row166_col6" class="data row166 col6" >0.0556982</td>
      <td id="T_c6ca4_row166_col7" class="data row166 col7" >0.999159</td>
      <td id="T_c6ca4_row166_col8" class="data row166 col8" >0.0410156</td>
      <td id="T_c6ca4_row166_col9" class="data row166 col9" >1024.000000</td>
      <td id="T_c6ca4_row166_col10" class="data row166 col10" >torch.float32</td>
      <td id="T_c6ca4_row166_col11" class="data row166 col11" >(1024,)</td>
      <td id="T_c6ca4_row166_col12" class="data row166 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row167" class="row_heading level0 row167" >167</th>
      <td id="T_c6ca4_row167_col0" class="data row167 col0" >encoder.layers.18.fc2.weight</td>
      <td id="T_c6ca4_row167_col1" class="data row167 col1" >encoder.layers.18.fc2.weight</td>
      <td id="T_c6ca4_row167_col2" class="data row167 col2" >av_romanizer.w2v_model.encoder.layers.18.fc2.weight</td>
      <td id="T_c6ca4_row167_col3" class="data row167 col3" >74.381</td>
      <td id="T_c6ca4_row167_col4" class="data row167 col4" >347.295</td>
      <td id="T_c6ca4_row167_col5" class="data row167 col5" >333.742</td>
      <td id="T_c6ca4_row167_col6" class="data row167 col6" >0.214172</td>
      <td id="T_c6ca4_row167_col7" class="data row167 col7" >0.976926</td>
      <td id="T_c6ca4_row167_col8" class="data row167 col8" >0.328735</td>
      <td id="T_c6ca4_row167_col9" class="data row167 col9" >4194304.000000</td>
      <td id="T_c6ca4_row167_col10" class="data row167 col10" >torch.float32</td>
      <td id="T_c6ca4_row167_col11" class="data row167 col11" >(1024, 4096)</td>
      <td id="T_c6ca4_row167_col12" class="data row167 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row168" class="row_heading level0 row168" >168</th>
      <td id="T_c6ca4_row168_col0" class="data row168 col0" >encoder.layers.18.final_layer_norm.bias</td>
      <td id="T_c6ca4_row168_col1" class="data row168 col1" >encoder.layers.18.final_layer_norm.bias</td>
      <td id="T_c6ca4_row168_col2" class="data row168 col2" >av_romanizer.w2v_model.encoder.layers.18.final_layer_norm.bias</td>
      <td id="T_c6ca4_row168_col3" class="data row168 col3" >0.44065</td>
      <td id="T_c6ca4_row168_col4" class="data row168 col4" >8.78102</td>
      <td id="T_c6ca4_row168_col5" class="data row168 col5" >8.57933</td>
      <td id="T_c6ca4_row168_col6" class="data row168 col6" >0.0501821</td>
      <td id="T_c6ca4_row168_col7" class="data row168 col7" >0.998981</td>
      <td id="T_c6ca4_row168_col8" class="data row168 col8" >0.0532227</td>
      <td id="T_c6ca4_row168_col9" class="data row168 col9" >1024.000000</td>
      <td id="T_c6ca4_row168_col10" class="data row168 col10" >torch.float32</td>
      <td id="T_c6ca4_row168_col11" class="data row168 col11" >(1024,)</td>
      <td id="T_c6ca4_row168_col12" class="data row168 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row169" class="row_heading level0 row169" >169</th>
      <td id="T_c6ca4_row169_col0" class="data row169 col0" >encoder.layers.18.final_layer_norm.weight</td>
      <td id="T_c6ca4_row169_col1" class="data row169 col1" >encoder.layers.18.final_layer_norm.weight</td>
      <td id="T_c6ca4_row169_col2" class="data row169 col2" >av_romanizer.w2v_model.encoder.layers.18.final_layer_norm.weight</td>
      <td id="T_c6ca4_row169_col3" class="data row169 col3" >1.31651</td>
      <td id="T_c6ca4_row169_col4" class="data row169 col4" >36.551</td>
      <td id="T_c6ca4_row169_col5" class="data row169 col5" >35.5534</td>
      <td id="T_c6ca4_row169_col6" class="data row169 col6" >0.0360185</td>
      <td id="T_c6ca4_row169_col7" class="data row169 col7" >0.999716</td>
      <td id="T_c6ca4_row169_col8" class="data row169 col8" >0.131897</td>
      <td id="T_c6ca4_row169_col9" class="data row169 col9" >1024.000000</td>
      <td id="T_c6ca4_row169_col10" class="data row169 col10" >torch.float32</td>
      <td id="T_c6ca4_row169_col11" class="data row169 col11" >(1024,)</td>
      <td id="T_c6ca4_row169_col12" class="data row169 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row170" class="row_heading level0 row170" >170</th>
      <td id="T_c6ca4_row170_col0" class="data row170 col0" >encoder.layers.18.self_attn.k_proj.bias</td>
      <td id="T_c6ca4_row170_col1" class="data row170 col1" >encoder.layers.18.self_attn.k_proj.bias</td>
      <td id="T_c6ca4_row170_col2" class="data row170 col2" >av_romanizer.w2v_model.encoder.layers.18.self_attn.k_proj.bias</td>
      <td id="T_c6ca4_row170_col3" class="data row170 col3" >0.0199122</td>
      <td id="T_c6ca4_row170_col4" class="data row170 col4" >0.211174</td>
      <td id="T_c6ca4_row170_col5" class="data row170 col5" >0.205634</td>
      <td id="T_c6ca4_row170_col6" class="data row170 col6" >0.0942929</td>
      <td id="T_c6ca4_row170_col7" class="data row170 col7" >0.995788</td>
      <td id="T_c6ca4_row170_col8" class="data row170 col8" >0.00422382</td>
      <td id="T_c6ca4_row170_col9" class="data row170 col9" >1024.000000</td>
      <td id="T_c6ca4_row170_col10" class="data row170 col10" >torch.float32</td>
      <td id="T_c6ca4_row170_col11" class="data row170 col11" >(1024,)</td>
      <td id="T_c6ca4_row170_col12" class="data row170 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row171" class="row_heading level0 row171" >171</th>
      <td id="T_c6ca4_row171_col0" class="data row171 col0" >encoder.layers.18.self_attn.k_proj.weight</td>
      <td id="T_c6ca4_row171_col1" class="data row171 col1" >encoder.layers.18.self_attn.k_proj.weight</td>
      <td id="T_c6ca4_row171_col2" class="data row171 col2" >av_romanizer.w2v_model.encoder.layers.18.self_attn.k_proj.weight</td>
      <td id="T_c6ca4_row171_col3" class="data row171 col3" >28.3238</td>
      <td id="T_c6ca4_row171_col4" class="data row171 col4" >133.971</td>
      <td id="T_c6ca4_row171_col5" class="data row171 col5" >129.224</td>
      <td id="T_c6ca4_row171_col6" class="data row171 col6" >0.211418</td>
      <td id="T_c6ca4_row171_col7" class="data row171 col7" >0.977481</td>
      <td id="T_c6ca4_row171_col8" class="data row171 col8" >0.19873</td>
      <td id="T_c6ca4_row171_col9" class="data row171 col9" >1048576.000000</td>
      <td id="T_c6ca4_row171_col10" class="data row171 col10" >torch.float32</td>
      <td id="T_c6ca4_row171_col11" class="data row171 col11" >(1024, 1024)</td>
      <td id="T_c6ca4_row171_col12" class="data row171 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row172" class="row_heading level0 row172" >172</th>
      <td id="T_c6ca4_row172_col0" class="data row172 col0" >encoder.layers.18.self_attn.out_proj.bias</td>
      <td id="T_c6ca4_row172_col1" class="data row172 col1" >encoder.layers.18.self_attn.out_proj.bias</td>
      <td id="T_c6ca4_row172_col2" class="data row172 col2" >av_romanizer.w2v_model.encoder.layers.18.self_attn.out_proj.bias</td>
      <td id="T_c6ca4_row172_col3" class="data row172 col3" >0.321</td>
      <td id="T_c6ca4_row172_col4" class="data row172 col4" >5.65779</td>
      <td id="T_c6ca4_row172_col5" class="data row172 col5" >5.49028</td>
      <td id="T_c6ca4_row172_col6" class="data row172 col6" >0.056736</td>
      <td id="T_c6ca4_row172_col7" class="data row172 col7" >0.998793</td>
      <td id="T_c6ca4_row172_col8" class="data row172 col8" >0.0385742</td>
      <td id="T_c6ca4_row172_col9" class="data row172 col9" >1024.000000</td>
      <td id="T_c6ca4_row172_col10" class="data row172 col10" >torch.float32</td>
      <td id="T_c6ca4_row172_col11" class="data row172 col11" >(1024,)</td>
      <td id="T_c6ca4_row172_col12" class="data row172 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row173" class="row_heading level0 row173" >173</th>
      <td id="T_c6ca4_row173_col0" class="data row173 col0" >encoder.layers.18.self_attn.out_proj.weight</td>
      <td id="T_c6ca4_row173_col1" class="data row173 col1" >encoder.layers.18.self_attn.out_proj.weight</td>
      <td id="T_c6ca4_row173_col2" class="data row173 col2" >av_romanizer.w2v_model.encoder.layers.18.self_attn.out_proj.weight</td>
      <td id="T_c6ca4_row173_col3" class="data row173 col3" >30.9073</td>
      <td id="T_c6ca4_row173_col4" class="data row173 col4" >168.699</td>
      <td id="T_c6ca4_row173_col5" class="data row173 col5" >160.717</td>
      <td id="T_c6ca4_row173_col6" class="data row173 col6" >0.18321</td>
      <td id="T_c6ca4_row173_col7" class="data row173 col7" >0.983559</td>
      <td id="T_c6ca4_row173_col8" class="data row173 col8" >0.166504</td>
      <td id="T_c6ca4_row173_col9" class="data row173 col9" >1048576.000000</td>
      <td id="T_c6ca4_row173_col10" class="data row173 col10" >torch.float32</td>
      <td id="T_c6ca4_row173_col11" class="data row173 col11" >(1024, 1024)</td>
      <td id="T_c6ca4_row173_col12" class="data row173 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row174" class="row_heading level0 row174" >174</th>
      <td id="T_c6ca4_row174_col0" class="data row174 col0" >encoder.layers.18.self_attn.q_proj.bias</td>
      <td id="T_c6ca4_row174_col1" class="data row174 col1" >encoder.layers.18.self_attn.q_proj.bias</td>
      <td id="T_c6ca4_row174_col2" class="data row174 col2" >av_romanizer.w2v_model.encoder.layers.18.self_attn.q_proj.bias</td>
      <td id="T_c6ca4_row174_col3" class="data row174 col3" >0.853632</td>
      <td id="T_c6ca4_row174_col4" class="data row174 col4" >8.66697</td>
      <td id="T_c6ca4_row174_col5" class="data row174 col5" >8.38015</td>
      <td id="T_c6ca4_row174_col6" class="data row174 col6" >0.0984925</td>
      <td id="T_c6ca4_row174_col7" class="data row174 col7" >0.99555</td>
      <td id="T_c6ca4_row174_col8" class="data row174 col8" >0.134766</td>
      <td id="T_c6ca4_row174_col9" class="data row174 col9" >1024.000000</td>
      <td id="T_c6ca4_row174_col10" class="data row174 col10" >torch.float32</td>
      <td id="T_c6ca4_row174_col11" class="data row174 col11" >(1024,)</td>
      <td id="T_c6ca4_row174_col12" class="data row174 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row175" class="row_heading level0 row175" >175</th>
      <td id="T_c6ca4_row175_col0" class="data row175 col0" >encoder.layers.18.self_attn.q_proj.weight</td>
      <td id="T_c6ca4_row175_col1" class="data row175 col1" >encoder.layers.18.self_attn.q_proj.weight</td>
      <td id="T_c6ca4_row175_col2" class="data row175 col2" >av_romanizer.w2v_model.encoder.layers.18.self_attn.q_proj.weight</td>
      <td id="T_c6ca4_row175_col3" class="data row175 col3" >28.0001</td>
      <td id="T_c6ca4_row175_col4" class="data row175 col4" >131.657</td>
      <td id="T_c6ca4_row175_col5" class="data row175 col5" >126.927</td>
      <td id="T_c6ca4_row175_col6" class="data row175 col6" >0.212675</td>
      <td id="T_c6ca4_row175_col7" class="data row175 col7" >0.977211</td>
      <td id="T_c6ca4_row175_col8" class="data row175 col8" >0.172119</td>
      <td id="T_c6ca4_row175_col9" class="data row175 col9" >1048576.000000</td>
      <td id="T_c6ca4_row175_col10" class="data row175 col10" >torch.float32</td>
      <td id="T_c6ca4_row175_col11" class="data row175 col11" >(1024, 1024)</td>
      <td id="T_c6ca4_row175_col12" class="data row175 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row176" class="row_heading level0 row176" >176</th>
      <td id="T_c6ca4_row176_col0" class="data row176 col0" >encoder.layers.18.self_attn.v_proj.bias</td>
      <td id="T_c6ca4_row176_col1" class="data row176 col1" >encoder.layers.18.self_attn.v_proj.bias</td>
      <td id="T_c6ca4_row176_col2" class="data row176 col2" >av_romanizer.w2v_model.encoder.layers.18.self_attn.v_proj.bias</td>
      <td id="T_c6ca4_row176_col3" class="data row176 col3" >0.290037</td>
      <td id="T_c6ca4_row176_col4" class="data row176 col4" >2.53151</td>
      <td id="T_c6ca4_row176_col5" class="data row176 col5" >2.42504</td>
      <td id="T_c6ca4_row176_col6" class="data row176 col6" >0.114571</td>
      <td id="T_c6ca4_row176_col7" class="data row176 col7" >0.994072</td>
      <td id="T_c6ca4_row176_col8" class="data row176 col8" >0.0388184</td>
      <td id="T_c6ca4_row176_col9" class="data row176 col9" >1024.000000</td>
      <td id="T_c6ca4_row176_col10" class="data row176 col10" >torch.float32</td>
      <td id="T_c6ca4_row176_col11" class="data row176 col11" >(1024,)</td>
      <td id="T_c6ca4_row176_col12" class="data row176 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row177" class="row_heading level0 row177" >177</th>
      <td id="T_c6ca4_row177_col0" class="data row177 col0" >encoder.layers.18.self_attn.v_proj.weight</td>
      <td id="T_c6ca4_row177_col1" class="data row177 col1" >encoder.layers.18.self_attn.v_proj.weight</td>
      <td id="T_c6ca4_row177_col2" class="data row177 col2" >av_romanizer.w2v_model.encoder.layers.18.self_attn.v_proj.weight</td>
      <td id="T_c6ca4_row177_col3" class="data row177 col3" >27.2747</td>
      <td id="T_c6ca4_row177_col4" class="data row177 col4" >171.111</td>
      <td id="T_c6ca4_row177_col5" class="data row177 col5" >163.078</td>
      <td id="T_c6ca4_row177_col6" class="data row177 col6" >0.159398</td>
      <td id="T_c6ca4_row177_col7" class="data row177 col7" >0.987827</td>
      <td id="T_c6ca4_row177_col8" class="data row177 col8" >0.158722</td>
      <td id="T_c6ca4_row177_col9" class="data row177 col9" >1048576.000000</td>
      <td id="T_c6ca4_row177_col10" class="data row177 col10" >torch.float32</td>
      <td id="T_c6ca4_row177_col11" class="data row177 col11" >(1024, 1024)</td>
      <td id="T_c6ca4_row177_col12" class="data row177 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row178" class="row_heading level0 row178" >178</th>
      <td id="T_c6ca4_row178_col0" class="data row178 col0" >encoder.layers.18.self_attn_layer_norm.bias</td>
      <td id="T_c6ca4_row178_col1" class="data row178 col1" >encoder.layers.18.self_attn_layer_norm.bias</td>
      <td id="T_c6ca4_row178_col2" class="data row178 col2" >av_romanizer.w2v_model.encoder.layers.18.self_attn_layer_norm.bias</td>
      <td id="T_c6ca4_row178_col3" class="data row178 col3" >0.260773</td>
      <td id="T_c6ca4_row178_col4" class="data row178 col4" >1.45768</td>
      <td id="T_c6ca4_row178_col5" class="data row178 col5" >1.4548</td>
      <td id="T_c6ca4_row178_col6" class="data row178 col6" >0.178896</td>
      <td id="T_c6ca4_row178_col7" class="data row178 col7" >0.983968</td>
      <td id="T_c6ca4_row178_col8" class="data row178 col8" >0.0380249</td>
      <td id="T_c6ca4_row178_col9" class="data row178 col9" >1024.000000</td>
      <td id="T_c6ca4_row178_col10" class="data row178 col10" >torch.float32</td>
      <td id="T_c6ca4_row178_col11" class="data row178 col11" >(1024,)</td>
      <td id="T_c6ca4_row178_col12" class="data row178 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row179" class="row_heading level0 row179" >179</th>
      <td id="T_c6ca4_row179_col0" class="data row179 col0" >encoder.layers.18.self_attn_layer_norm.weight</td>
      <td id="T_c6ca4_row179_col1" class="data row179 col1" >encoder.layers.18.self_attn_layer_norm.weight</td>
      <td id="T_c6ca4_row179_col2" class="data row179 col2" >av_romanizer.w2v_model.encoder.layers.18.self_attn_layer_norm.weight</td>
      <td id="T_c6ca4_row179_col3" class="data row179 col3" >0.508446</td>
      <td id="T_c6ca4_row179_col4" class="data row179 col4" >8.25415</td>
      <td id="T_c6ca4_row179_col5" class="data row179 col5" >8.17478</td>
      <td id="T_c6ca4_row179_col6" class="data row179 col6" >0.0615989</td>
      <td id="T_c6ca4_row179_col7" class="data row179 col7" >0.998131</td>
      <td id="T_c6ca4_row179_col8" class="data row179 col8" >0.0505371</td>
      <td id="T_c6ca4_row179_col9" class="data row179 col9" >1024.000000</td>
      <td id="T_c6ca4_row179_col10" class="data row179 col10" >torch.float32</td>
      <td id="T_c6ca4_row179_col11" class="data row179 col11" >(1024,)</td>
      <td id="T_c6ca4_row179_col12" class="data row179 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row180" class="row_heading level0 row180" >180</th>
      <td id="T_c6ca4_row180_col0" class="data row180 col0" >encoder.layers.19.fc1.bias</td>
      <td id="T_c6ca4_row180_col1" class="data row180 col1" >encoder.layers.19.fc1.bias</td>
      <td id="T_c6ca4_row180_col2" class="data row180 col2" >av_romanizer.w2v_model.encoder.layers.19.fc1.bias</td>
      <td id="T_c6ca4_row180_col3" class="data row180 col3" >0.725211</td>
      <td id="T_c6ca4_row180_col4" class="data row180 col4" >4.57963</td>
      <td id="T_c6ca4_row180_col5" class="data row180 col5" >4.44256</td>
      <td id="T_c6ca4_row180_col6" class="data row180 col6" >0.158356</td>
      <td id="T_c6ca4_row180_col7" class="data row180 col7" >0.987537</td>
      <td id="T_c6ca4_row180_col8" class="data row180 col8" >0.0523682</td>
      <td id="T_c6ca4_row180_col9" class="data row180 col9" >4096.000000</td>
      <td id="T_c6ca4_row180_col10" class="data row180 col10" >torch.float32</td>
      <td id="T_c6ca4_row180_col11" class="data row180 col11" >(4096,)</td>
      <td id="T_c6ca4_row180_col12" class="data row180 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row181" class="row_heading level0 row181" >181</th>
      <td id="T_c6ca4_row181_col0" class="data row181 col0" >encoder.layers.19.fc1.weight</td>
      <td id="T_c6ca4_row181_col1" class="data row181 col1" >encoder.layers.19.fc1.weight</td>
      <td id="T_c6ca4_row181_col2" class="data row181 col2" >av_romanizer.w2v_model.encoder.layers.19.fc1.weight</td>
      <td id="T_c6ca4_row181_col3" class="data row181 col3" >76.6595</td>
      <td id="T_c6ca4_row181_col4" class="data row181 col4" >318.909</td>
      <td id="T_c6ca4_row181_col5" class="data row181 col5" >306.154</td>
      <td id="T_c6ca4_row181_col6" class="data row181 col6" >0.240381</td>
      <td id="T_c6ca4_row181_col7" class="data row181 col7" >0.970738</td>
      <td id="T_c6ca4_row181_col8" class="data row181 col8" >0.274048</td>
      <td id="T_c6ca4_row181_col9" class="data row181 col9" >4194304.000000</td>
      <td id="T_c6ca4_row181_col10" class="data row181 col10" >torch.float32</td>
      <td id="T_c6ca4_row181_col11" class="data row181 col11" >(4096, 1024)</td>
      <td id="T_c6ca4_row181_col12" class="data row181 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row182" class="row_heading level0 row182" >182</th>
      <td id="T_c6ca4_row182_col0" class="data row182 col0" >encoder.layers.19.fc2.bias</td>
      <td id="T_c6ca4_row182_col1" class="data row182 col1" >encoder.layers.19.fc2.bias</td>
      <td id="T_c6ca4_row182_col2" class="data row182 col2" >av_romanizer.w2v_model.encoder.layers.19.fc2.bias</td>
      <td id="T_c6ca4_row182_col3" class="data row182 col3" >0.395631</td>
      <td id="T_c6ca4_row182_col4" class="data row182 col4" >6.41647</td>
      <td id="T_c6ca4_row182_col5" class="data row182 col5" >6.14527</td>
      <td id="T_c6ca4_row182_col6" class="data row182 col6" >0.0616587</td>
      <td id="T_c6ca4_row182_col7" class="data row182 col7" >0.998948</td>
      <td id="T_c6ca4_row182_col8" class="data row182 col8" >0.0367432</td>
      <td id="T_c6ca4_row182_col9" class="data row182 col9" >1024.000000</td>
      <td id="T_c6ca4_row182_col10" class="data row182 col10" >torch.float32</td>
      <td id="T_c6ca4_row182_col11" class="data row182 col11" >(1024,)</td>
      <td id="T_c6ca4_row182_col12" class="data row182 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row183" class="row_heading level0 row183" >183</th>
      <td id="T_c6ca4_row183_col0" class="data row183 col0" >encoder.layers.19.fc2.weight</td>
      <td id="T_c6ca4_row183_col1" class="data row183 col1" >encoder.layers.19.fc2.weight</td>
      <td id="T_c6ca4_row183_col2" class="data row183 col2" >av_romanizer.w2v_model.encoder.layers.19.fc2.weight</td>
      <td id="T_c6ca4_row183_col3" class="data row183 col3" >78.3951</td>
      <td id="T_c6ca4_row183_col4" class="data row183 col4" >334.834</td>
      <td id="T_c6ca4_row183_col5" class="data row183 col5" >321.979</td>
      <td id="T_c6ca4_row183_col6" class="data row183 col6" >0.234131</td>
      <td id="T_c6ca4_row183_col7" class="data row183 col7" >0.972263</td>
      <td id="T_c6ca4_row183_col8" class="data row183 col8" >0.2724</td>
      <td id="T_c6ca4_row183_col9" class="data row183 col9" >4194304.000000</td>
      <td id="T_c6ca4_row183_col10" class="data row183 col10" >torch.float32</td>
      <td id="T_c6ca4_row183_col11" class="data row183 col11" >(1024, 4096)</td>
      <td id="T_c6ca4_row183_col12" class="data row183 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row184" class="row_heading level0 row184" >184</th>
      <td id="T_c6ca4_row184_col0" class="data row184 col0" >encoder.layers.19.final_layer_norm.bias</td>
      <td id="T_c6ca4_row184_col1" class="data row184 col1" >encoder.layers.19.final_layer_norm.bias</td>
      <td id="T_c6ca4_row184_col2" class="data row184 col2" >av_romanizer.w2v_model.encoder.layers.19.final_layer_norm.bias</td>
      <td id="T_c6ca4_row184_col3" class="data row184 col3" >0.487104</td>
      <td id="T_c6ca4_row184_col4" class="data row184 col4" >7.91837</td>
      <td id="T_c6ca4_row184_col5" class="data row184 col5" >7.57111</td>
      <td id="T_c6ca4_row184_col6" class="data row184 col6" >0.0615156</td>
      <td id="T_c6ca4_row184_col7" class="data row184 col7" >0.999027</td>
      <td id="T_c6ca4_row184_col8" class="data row184 col8" >0.0576172</td>
      <td id="T_c6ca4_row184_col9" class="data row184 col9" >1024.000000</td>
      <td id="T_c6ca4_row184_col10" class="data row184 col10" >torch.float32</td>
      <td id="T_c6ca4_row184_col11" class="data row184 col11" >(1024,)</td>
      <td id="T_c6ca4_row184_col12" class="data row184 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row185" class="row_heading level0 row185" >185</th>
      <td id="T_c6ca4_row185_col0" class="data row185 col0" >encoder.layers.19.final_layer_norm.weight</td>
      <td id="T_c6ca4_row185_col1" class="data row185 col1" >encoder.layers.19.final_layer_norm.weight</td>
      <td id="T_c6ca4_row185_col2" class="data row185 col2" >av_romanizer.w2v_model.encoder.layers.19.final_layer_norm.weight</td>
      <td id="T_c6ca4_row185_col3" class="data row185 col3" >1.18169</td>
      <td id="T_c6ca4_row185_col4" class="data row185 col4" >34.7796</td>
      <td id="T_c6ca4_row185_col5" class="data row185 col5" >33.9978</td>
      <td id="T_c6ca4_row185_col6" class="data row185 col6" >0.0339765</td>
      <td id="T_c6ca4_row185_col7" class="data row185 col7" >0.999668</td>
      <td id="T_c6ca4_row185_col8" class="data row185 col8" >0.143555</td>
      <td id="T_c6ca4_row185_col9" class="data row185 col9" >1024.000000</td>
      <td id="T_c6ca4_row185_col10" class="data row185 col10" >torch.float32</td>
      <td id="T_c6ca4_row185_col11" class="data row185 col11" >(1024,)</td>
      <td id="T_c6ca4_row185_col12" class="data row185 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row186" class="row_heading level0 row186" >186</th>
      <td id="T_c6ca4_row186_col0" class="data row186 col0" >encoder.layers.19.self_attn.k_proj.bias</td>
      <td id="T_c6ca4_row186_col1" class="data row186 col1" >encoder.layers.19.self_attn.k_proj.bias</td>
      <td id="T_c6ca4_row186_col2" class="data row186 col2" >av_romanizer.w2v_model.encoder.layers.19.self_attn.k_proj.bias</td>
      <td id="T_c6ca4_row186_col3" class="data row186 col3" >0.0186953</td>
      <td id="T_c6ca4_row186_col4" class="data row186 col4" >0.113892</td>
      <td id="T_c6ca4_row186_col5" class="data row186 col5" >0.107996</td>
      <td id="T_c6ca4_row186_col6" class="data row186 col6" >0.16415</td>
      <td id="T_c6ca4_row186_col7" class="data row186 col7" >0.987205</td>
      <td id="T_c6ca4_row186_col8" class="data row186 col8" >0.00385666</td>
      <td id="T_c6ca4_row186_col9" class="data row186 col9" >1024.000000</td>
      <td id="T_c6ca4_row186_col10" class="data row186 col10" >torch.float32</td>
      <td id="T_c6ca4_row186_col11" class="data row186 col11" >(1024,)</td>
      <td id="T_c6ca4_row186_col12" class="data row186 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row187" class="row_heading level0 row187" >187</th>
      <td id="T_c6ca4_row187_col0" class="data row187 col0" >encoder.layers.19.self_attn.k_proj.weight</td>
      <td id="T_c6ca4_row187_col1" class="data row187 col1" >encoder.layers.19.self_attn.k_proj.weight</td>
      <td id="T_c6ca4_row187_col2" class="data row187 col2" >av_romanizer.w2v_model.encoder.layers.19.self_attn.k_proj.weight</td>
      <td id="T_c6ca4_row187_col3" class="data row187 col3" >29.4051</td>
      <td id="T_c6ca4_row187_col4" class="data row187 col4" >124.247</td>
      <td id="T_c6ca4_row187_col5" class="data row187 col5" >119.531</td>
      <td id="T_c6ca4_row187_col6" class="data row187 col6" >0.236666</td>
      <td id="T_c6ca4_row187_col7" class="data row187 col7" >0.971639</td>
      <td id="T_c6ca4_row187_col8" class="data row187 col8" >0.171875</td>
      <td id="T_c6ca4_row187_col9" class="data row187 col9" >1048576.000000</td>
      <td id="T_c6ca4_row187_col10" class="data row187 col10" >torch.float32</td>
      <td id="T_c6ca4_row187_col11" class="data row187 col11" >(1024, 1024)</td>
      <td id="T_c6ca4_row187_col12" class="data row187 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row188" class="row_heading level0 row188" >188</th>
      <td id="T_c6ca4_row188_col0" class="data row188 col0" >encoder.layers.19.self_attn.out_proj.bias</td>
      <td id="T_c6ca4_row188_col1" class="data row188 col1" >encoder.layers.19.self_attn.out_proj.bias</td>
      <td id="T_c6ca4_row188_col2" class="data row188 col2" >av_romanizer.w2v_model.encoder.layers.19.self_attn.out_proj.bias</td>
      <td id="T_c6ca4_row188_col3" class="data row188 col3" >0.32757</td>
      <td id="T_c6ca4_row188_col4" class="data row188 col4" >5.25705</td>
      <td id="T_c6ca4_row188_col5" class="data row188 col5" >5.08143</td>
      <td id="T_c6ca4_row188_col6" class="data row188 col6" >0.0623107</td>
      <td id="T_c6ca4_row188_col7" class="data row188 col7" >0.998569</td>
      <td id="T_c6ca4_row188_col8" class="data row188 col8" >0.0355225</td>
      <td id="T_c6ca4_row188_col9" class="data row188 col9" >1024.000000</td>
      <td id="T_c6ca4_row188_col10" class="data row188 col10" >torch.float32</td>
      <td id="T_c6ca4_row188_col11" class="data row188 col11" >(1024,)</td>
      <td id="T_c6ca4_row188_col12" class="data row188 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row189" class="row_heading level0 row189" >189</th>
      <td id="T_c6ca4_row189_col0" class="data row189 col0" >encoder.layers.19.self_attn.out_proj.weight</td>
      <td id="T_c6ca4_row189_col1" class="data row189 col1" >encoder.layers.19.self_attn.out_proj.weight</td>
      <td id="T_c6ca4_row189_col2" class="data row189 col2" >av_romanizer.w2v_model.encoder.layers.19.self_attn.out_proj.weight</td>
      <td id="T_c6ca4_row189_col3" class="data row189 col3" >32.8509</td>
      <td id="T_c6ca4_row189_col4" class="data row189 col4" >179.115</td>
      <td id="T_c6ca4_row189_col5" class="data row189 col5" >170.93</td>
      <td id="T_c6ca4_row189_col6" class="data row189 col6" >0.183407</td>
      <td id="T_c6ca4_row189_col7" class="data row189 col7" >0.98347</td>
      <td id="T_c6ca4_row189_col8" class="data row189 col8" >0.173706</td>
      <td id="T_c6ca4_row189_col9" class="data row189 col9" >1048576.000000</td>
      <td id="T_c6ca4_row189_col10" class="data row189 col10" >torch.float32</td>
      <td id="T_c6ca4_row189_col11" class="data row189 col11" >(1024, 1024)</td>
      <td id="T_c6ca4_row189_col12" class="data row189 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row190" class="row_heading level0 row190" >190</th>
      <td id="T_c6ca4_row190_col0" class="data row190 col0" >encoder.layers.19.self_attn.q_proj.bias</td>
      <td id="T_c6ca4_row190_col1" class="data row190 col1" >encoder.layers.19.self_attn.q_proj.bias</td>
      <td id="T_c6ca4_row190_col2" class="data row190 col2" >av_romanizer.w2v_model.encoder.layers.19.self_attn.q_proj.bias</td>
      <td id="T_c6ca4_row190_col3" class="data row190 col3" >0.787649</td>
      <td id="T_c6ca4_row190_col4" class="data row190 col4" >8.35649</td>
      <td id="T_c6ca4_row190_col5" class="data row190 col5" >8.12242</td>
      <td id="T_c6ca4_row190_col6" class="data row190 col6" >0.094256</td>
      <td id="T_c6ca4_row190_col7" class="data row190 col7" >0.995834</td>
      <td id="T_c6ca4_row190_col8" class="data row190 col8" >0.111328</td>
      <td id="T_c6ca4_row190_col9" class="data row190 col9" >1024.000000</td>
      <td id="T_c6ca4_row190_col10" class="data row190 col10" >torch.float32</td>
      <td id="T_c6ca4_row190_col11" class="data row190 col11" >(1024,)</td>
      <td id="T_c6ca4_row190_col12" class="data row190 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row191" class="row_heading level0 row191" >191</th>
      <td id="T_c6ca4_row191_col0" class="data row191 col0" >encoder.layers.19.self_attn.q_proj.weight</td>
      <td id="T_c6ca4_row191_col1" class="data row191 col1" >encoder.layers.19.self_attn.q_proj.weight</td>
      <td id="T_c6ca4_row191_col2" class="data row191 col2" >av_romanizer.w2v_model.encoder.layers.19.self_attn.q_proj.weight</td>
      <td id="T_c6ca4_row191_col3" class="data row191 col3" >29.2088</td>
      <td id="T_c6ca4_row191_col4" class="data row191 col4" >121.615</td>
      <td id="T_c6ca4_row191_col5" class="data row191 col5" >117.119</td>
      <td id="T_c6ca4_row191_col6" class="data row191 col6" >0.240174</td>
      <td id="T_c6ca4_row191_col7" class="data row191 col7" >0.970761</td>
      <td id="T_c6ca4_row191_col8" class="data row191 col8" >0.15918</td>
      <td id="T_c6ca4_row191_col9" class="data row191 col9" >1048576.000000</td>
      <td id="T_c6ca4_row191_col10" class="data row191 col10" >torch.float32</td>
      <td id="T_c6ca4_row191_col11" class="data row191 col11" >(1024, 1024)</td>
      <td id="T_c6ca4_row191_col12" class="data row191 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row192" class="row_heading level0 row192" >192</th>
      <td id="T_c6ca4_row192_col0" class="data row192 col0" >encoder.layers.19.self_attn.v_proj.bias</td>
      <td id="T_c6ca4_row192_col1" class="data row192 col1" >encoder.layers.19.self_attn.v_proj.bias</td>
      <td id="T_c6ca4_row192_col2" class="data row192 col2" >av_romanizer.w2v_model.encoder.layers.19.self_attn.v_proj.bias</td>
      <td id="T_c6ca4_row192_col3" class="data row192 col3" >0.306827</td>
      <td id="T_c6ca4_row192_col4" class="data row192 col4" >2.77071</td>
      <td id="T_c6ca4_row192_col5" class="data row192 col5" >2.63187</td>
      <td id="T_c6ca4_row192_col6" class="data row192 col6" >0.11074</td>
      <td id="T_c6ca4_row192_col7" class="data row192 col7" >0.994867</td>
      <td id="T_c6ca4_row192_col8" class="data row192 col8" >0.0424194</td>
      <td id="T_c6ca4_row192_col9" class="data row192 col9" >1024.000000</td>
      <td id="T_c6ca4_row192_col10" class="data row192 col10" >torch.float32</td>
      <td id="T_c6ca4_row192_col11" class="data row192 col11" >(1024,)</td>
      <td id="T_c6ca4_row192_col12" class="data row192 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row193" class="row_heading level0 row193" >193</th>
      <td id="T_c6ca4_row193_col0" class="data row193 col0" >encoder.layers.19.self_attn.v_proj.weight</td>
      <td id="T_c6ca4_row193_col1" class="data row193 col1" >encoder.layers.19.self_attn.v_proj.weight</td>
      <td id="T_c6ca4_row193_col2" class="data row193 col2" >av_romanizer.w2v_model.encoder.layers.19.self_attn.v_proj.weight</td>
      <td id="T_c6ca4_row193_col3" class="data row193 col3" >28.0903</td>
      <td id="T_c6ca4_row193_col4" class="data row193 col4" >178.821</td>
      <td id="T_c6ca4_row193_col5" class="data row193 col5" >170.651</td>
      <td id="T_c6ca4_row193_col6" class="data row193 col6" >0.157086</td>
      <td id="T_c6ca4_row193_col7" class="data row193 col7" >0.988165</td>
      <td id="T_c6ca4_row193_col8" class="data row193 col8" >0.156372</td>
      <td id="T_c6ca4_row193_col9" class="data row193 col9" >1048576.000000</td>
      <td id="T_c6ca4_row193_col10" class="data row193 col10" >torch.float32</td>
      <td id="T_c6ca4_row193_col11" class="data row193 col11" >(1024, 1024)</td>
      <td id="T_c6ca4_row193_col12" class="data row193 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row194" class="row_heading level0 row194" >194</th>
      <td id="T_c6ca4_row194_col0" class="data row194 col0" >encoder.layers.19.self_attn_layer_norm.bias</td>
      <td id="T_c6ca4_row194_col1" class="data row194 col1" >encoder.layers.19.self_attn_layer_norm.bias</td>
      <td id="T_c6ca4_row194_col2" class="data row194 col2" >av_romanizer.w2v_model.encoder.layers.19.self_attn_layer_norm.bias</td>
      <td id="T_c6ca4_row194_col3" class="data row194 col3" >0.236943</td>
      <td id="T_c6ca4_row194_col4" class="data row194 col4" >1.67608</td>
      <td id="T_c6ca4_row194_col5" class="data row194 col5" >1.62088</td>
      <td id="T_c6ca4_row194_col6" class="data row194 col6" >0.141367</td>
      <td id="T_c6ca4_row194_col7" class="data row194 col7" >0.990228</td>
      <td id="T_c6ca4_row194_col8" class="data row194 col8" >0.0327148</td>
      <td id="T_c6ca4_row194_col9" class="data row194 col9" >1024.000000</td>
      <td id="T_c6ca4_row194_col10" class="data row194 col10" >torch.float32</td>
      <td id="T_c6ca4_row194_col11" class="data row194 col11" >(1024,)</td>
      <td id="T_c6ca4_row194_col12" class="data row194 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row195" class="row_heading level0 row195" >195</th>
      <td id="T_c6ca4_row195_col0" class="data row195 col0" >encoder.layers.19.self_attn_layer_norm.weight</td>
      <td id="T_c6ca4_row195_col1" class="data row195 col1" >encoder.layers.19.self_attn_layer_norm.weight</td>
      <td id="T_c6ca4_row195_col2" class="data row195 col2" >av_romanizer.w2v_model.encoder.layers.19.self_attn_layer_norm.weight</td>
      <td id="T_c6ca4_row195_col3" class="data row195 col3" >0.487682</td>
      <td id="T_c6ca4_row195_col4" class="data row195 col4" >8.90485</td>
      <td id="T_c6ca4_row195_col5" class="data row195 col5" >8.94512</td>
      <td id="T_c6ca4_row195_col6" class="data row195 col6" >0.0547659</td>
      <td id="T_c6ca4_row195_col7" class="data row195 col7" >0.998517</td>
      <td id="T_c6ca4_row195_col8" class="data row195 col8" >0.0541992</td>
      <td id="T_c6ca4_row195_col9" class="data row195 col9" >1024.000000</td>
      <td id="T_c6ca4_row195_col10" class="data row195 col10" >torch.float32</td>
      <td id="T_c6ca4_row195_col11" class="data row195 col11" >(1024,)</td>
      <td id="T_c6ca4_row195_col12" class="data row195 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row196" class="row_heading level0 row196" >196</th>
      <td id="T_c6ca4_row196_col0" class="data row196 col0" >encoder.layers.2.fc1.bias</td>
      <td id="T_c6ca4_row196_col1" class="data row196 col1" >encoder.layers.2.fc1.bias</td>
      <td id="T_c6ca4_row196_col2" class="data row196 col2" >av_romanizer.w2v_model.encoder.layers.2.fc1.bias</td>
      <td id="T_c6ca4_row196_col3" class="data row196 col3" >0.772671</td>
      <td id="T_c6ca4_row196_col4" class="data row196 col4" >2.73407</td>
      <td id="T_c6ca4_row196_col5" class="data row196 col5" >3.00521</td>
      <td id="T_c6ca4_row196_col6" class="data row196 col6" >0.282608</td>
      <td id="T_c6ca4_row196_col7" class="data row196 col7" >0.968143</td>
      <td id="T_c6ca4_row196_col8" class="data row196 col8" >0.0536499</td>
      <td id="T_c6ca4_row196_col9" class="data row196 col9" >4096.000000</td>
      <td id="T_c6ca4_row196_col10" class="data row196 col10" >torch.float32</td>
      <td id="T_c6ca4_row196_col11" class="data row196 col11" >(4096,)</td>
      <td id="T_c6ca4_row196_col12" class="data row196 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row197" class="row_heading level0 row197" >197</th>
      <td id="T_c6ca4_row197_col0" class="data row197 col0" >encoder.layers.2.fc1.weight</td>
      <td id="T_c6ca4_row197_col1" class="data row197 col1" >encoder.layers.2.fc1.weight</td>
      <td id="T_c6ca4_row197_col2" class="data row197 col2" >av_romanizer.w2v_model.encoder.layers.2.fc1.weight</td>
      <td id="T_c6ca4_row197_col3" class="data row197 col3" >40.0271</td>
      <td id="T_c6ca4_row197_col4" class="data row197 col4" >290.042</td>
      <td id="T_c6ca4_row197_col5" class="data row197 col5" >277.689</td>
      <td id="T_c6ca4_row197_col6" class="data row197 col6" >0.138004</td>
      <td id="T_c6ca4_row197_col7" class="data row197 col7" >0.991001</td>
      <td id="T_c6ca4_row197_col8" class="data row197 col8" >0.183594</td>
      <td id="T_c6ca4_row197_col9" class="data row197 col9" >4194304.000000</td>
      <td id="T_c6ca4_row197_col10" class="data row197 col10" >torch.float32</td>
      <td id="T_c6ca4_row197_col11" class="data row197 col11" >(4096, 1024)</td>
      <td id="T_c6ca4_row197_col12" class="data row197 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row198" class="row_heading level0 row198" >198</th>
      <td id="T_c6ca4_row198_col0" class="data row198 col0" >encoder.layers.2.fc2.bias</td>
      <td id="T_c6ca4_row198_col1" class="data row198 col1" >encoder.layers.2.fc2.bias</td>
      <td id="T_c6ca4_row198_col2" class="data row198 col2" >av_romanizer.w2v_model.encoder.layers.2.fc2.bias</td>
      <td id="T_c6ca4_row198_col3" class="data row198 col3" >0.159084</td>
      <td id="T_c6ca4_row198_col4" class="data row198 col4" >2.86064</td>
      <td id="T_c6ca4_row198_col5" class="data row198 col5" >2.74197</td>
      <td id="T_c6ca4_row198_col6" class="data row198 col6" >0.0556112</td>
      <td id="T_c6ca4_row198_col7" class="data row198 col7" >0.999285</td>
      <td id="T_c6ca4_row198_col8" class="data row198 col8" >0.0244141</td>
      <td id="T_c6ca4_row198_col9" class="data row198 col9" >1024.000000</td>
      <td id="T_c6ca4_row198_col10" class="data row198 col10" >torch.float32</td>
      <td id="T_c6ca4_row198_col11" class="data row198 col11" >(1024,)</td>
      <td id="T_c6ca4_row198_col12" class="data row198 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row199" class="row_heading level0 row199" >199</th>
      <td id="T_c6ca4_row199_col0" class="data row199 col0" >encoder.layers.2.fc2.weight</td>
      <td id="T_c6ca4_row199_col1" class="data row199 col1" >encoder.layers.2.fc2.weight</td>
      <td id="T_c6ca4_row199_col2" class="data row199 col2" >av_romanizer.w2v_model.encoder.layers.2.fc2.weight</td>
      <td id="T_c6ca4_row199_col3" class="data row199 col3" >35.2684</td>
      <td id="T_c6ca4_row199_col4" class="data row199 col4" >273.101</td>
      <td id="T_c6ca4_row199_col5" class="data row199 col5" >258.886</td>
      <td id="T_c6ca4_row199_col6" class="data row199 col6" >0.12914</td>
      <td id="T_c6ca4_row199_col7" class="data row199 col7" >0.992633</td>
      <td id="T_c6ca4_row199_col8" class="data row199 col8" >0.182587</td>
      <td id="T_c6ca4_row199_col9" class="data row199 col9" >4194304.000000</td>
      <td id="T_c6ca4_row199_col10" class="data row199 col10" >torch.float32</td>
      <td id="T_c6ca4_row199_col11" class="data row199 col11" >(1024, 4096)</td>
      <td id="T_c6ca4_row199_col12" class="data row199 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row200" class="row_heading level0 row200" >200</th>
      <td id="T_c6ca4_row200_col0" class="data row200 col0" >encoder.layers.2.final_layer_norm.bias</td>
      <td id="T_c6ca4_row200_col1" class="data row200 col1" >encoder.layers.2.final_layer_norm.bias</td>
      <td id="T_c6ca4_row200_col2" class="data row200 col2" >av_romanizer.w2v_model.encoder.layers.2.final_layer_norm.bias</td>
      <td id="T_c6ca4_row200_col3" class="data row200 col3" >0.204116</td>
      <td id="T_c6ca4_row200_col4" class="data row200 col4" >2.24743</td>
      <td id="T_c6ca4_row200_col5" class="data row200 col5" >2.19498</td>
      <td id="T_c6ca4_row200_col6" class="data row200 col6" >0.090822</td>
      <td id="T_c6ca4_row200_col7" class="data row200 col7" >0.996056</td>
      <td id="T_c6ca4_row200_col8" class="data row200 col8" >0.0479736</td>
      <td id="T_c6ca4_row200_col9" class="data row200 col9" >1024.000000</td>
      <td id="T_c6ca4_row200_col10" class="data row200 col10" >torch.float32</td>
      <td id="T_c6ca4_row200_col11" class="data row200 col11" >(1024,)</td>
      <td id="T_c6ca4_row200_col12" class="data row200 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row201" class="row_heading level0 row201" >201</th>
      <td id="T_c6ca4_row201_col0" class="data row201 col0" >encoder.layers.2.final_layer_norm.weight</td>
      <td id="T_c6ca4_row201_col1" class="data row201 col1" >encoder.layers.2.final_layer_norm.weight</td>
      <td id="T_c6ca4_row201_col2" class="data row201 col2" >av_romanizer.w2v_model.encoder.layers.2.final_layer_norm.weight</td>
      <td id="T_c6ca4_row201_col3" class="data row201 col3" >0.32276</td>
      <td id="T_c6ca4_row201_col4" class="data row201 col4" >4.67564</td>
      <td id="T_c6ca4_row201_col5" class="data row201 col5" >4.58047</td>
      <td id="T_c6ca4_row201_col6" class="data row201 col6" >0.0690301</td>
      <td id="T_c6ca4_row201_col7" class="data row201 col7" >0.997779</td>
      <td id="T_c6ca4_row201_col8" class="data row201 col8" >0.0383301</td>
      <td id="T_c6ca4_row201_col9" class="data row201 col9" >1024.000000</td>
      <td id="T_c6ca4_row201_col10" class="data row201 col10" >torch.float32</td>
      <td id="T_c6ca4_row201_col11" class="data row201 col11" >(1024,)</td>
      <td id="T_c6ca4_row201_col12" class="data row201 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row202" class="row_heading level0 row202" >202</th>
      <td id="T_c6ca4_row202_col0" class="data row202 col0" >encoder.layers.2.self_attn.k_proj.bias</td>
      <td id="T_c6ca4_row202_col1" class="data row202 col1" >encoder.layers.2.self_attn.k_proj.bias</td>
      <td id="T_c6ca4_row202_col2" class="data row202 col2" >av_romanizer.w2v_model.encoder.layers.2.self_attn.k_proj.bias</td>
      <td id="T_c6ca4_row202_col3" class="data row202 col3" >0.00998393</td>
      <td id="T_c6ca4_row202_col4" class="data row202 col4" >0.111343</td>
      <td id="T_c6ca4_row202_col5" class="data row202 col5" >0.106169</td>
      <td id="T_c6ca4_row202_col6" class="data row202 col6" >0.0896686</td>
      <td id="T_c6ca4_row202_col7" class="data row202 col7" >0.996916</td>
      <td id="T_c6ca4_row202_col8" class="data row202 col8" >0.00189972</td>
      <td id="T_c6ca4_row202_col9" class="data row202 col9" >1024.000000</td>
      <td id="T_c6ca4_row202_col10" class="data row202 col10" >torch.float32</td>
      <td id="T_c6ca4_row202_col11" class="data row202 col11" >(1024,)</td>
      <td id="T_c6ca4_row202_col12" class="data row202 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row203" class="row_heading level0 row203" >203</th>
      <td id="T_c6ca4_row203_col0" class="data row203 col0" >encoder.layers.2.self_attn.k_proj.weight</td>
      <td id="T_c6ca4_row203_col1" class="data row203 col1" >encoder.layers.2.self_attn.k_proj.weight</td>
      <td id="T_c6ca4_row203_col2" class="data row203 col2" >av_romanizer.w2v_model.encoder.layers.2.self_attn.k_proj.weight</td>
      <td id="T_c6ca4_row203_col3" class="data row203 col3" >19.788</td>
      <td id="T_c6ca4_row203_col4" class="data row203 col4" >147.019</td>
      <td id="T_c6ca4_row203_col5" class="data row203 col5" >141.843</td>
      <td id="T_c6ca4_row203_col6" class="data row203 col6" >0.134595</td>
      <td id="T_c6ca4_row203_col7" class="data row203 col7" >0.991254</td>
      <td id="T_c6ca4_row203_col8" class="data row203 col8" >0.131836</td>
      <td id="T_c6ca4_row203_col9" class="data row203 col9" >1048576.000000</td>
      <td id="T_c6ca4_row203_col10" class="data row203 col10" >torch.float32</td>
      <td id="T_c6ca4_row203_col11" class="data row203 col11" >(1024, 1024)</td>
      <td id="T_c6ca4_row203_col12" class="data row203 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row204" class="row_heading level0 row204" >204</th>
      <td id="T_c6ca4_row204_col0" class="data row204 col0" >encoder.layers.2.self_attn.out_proj.bias</td>
      <td id="T_c6ca4_row204_col1" class="data row204 col1" >encoder.layers.2.self_attn.out_proj.bias</td>
      <td id="T_c6ca4_row204_col2" class="data row204 col2" >av_romanizer.w2v_model.encoder.layers.2.self_attn.out_proj.bias</td>
      <td id="T_c6ca4_row204_col3" class="data row204 col3" >0.2975</td>
      <td id="T_c6ca4_row204_col4" class="data row204 col4" >3.169</td>
      <td id="T_c6ca4_row204_col5" class="data row204 col5" >2.98838</td>
      <td id="T_c6ca4_row204_col6" class="data row204 col6" >0.0938781</td>
      <td id="T_c6ca4_row204_col7" class="data row204 col7" >0.99705</td>
      <td id="T_c6ca4_row204_col8" class="data row204 col8" >0.0722656</td>
      <td id="T_c6ca4_row204_col9" class="data row204 col9" >1024.000000</td>
      <td id="T_c6ca4_row204_col10" class="data row204 col10" >torch.float32</td>
      <td id="T_c6ca4_row204_col11" class="data row204 col11" >(1024,)</td>
      <td id="T_c6ca4_row204_col12" class="data row204 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row205" class="row_heading level0 row205" >205</th>
      <td id="T_c6ca4_row205_col0" class="data row205 col0" >encoder.layers.2.self_attn.out_proj.weight</td>
      <td id="T_c6ca4_row205_col1" class="data row205 col1" >encoder.layers.2.self_attn.out_proj.weight</td>
      <td id="T_c6ca4_row205_col2" class="data row205 col2" >av_romanizer.w2v_model.encoder.layers.2.self_attn.out_proj.weight</td>
      <td id="T_c6ca4_row205_col3" class="data row205 col3" >20.3423</td>
      <td id="T_c6ca4_row205_col4" class="data row205 col4" >119.246</td>
      <td id="T_c6ca4_row205_col5" class="data row205 col5" >112.821</td>
      <td id="T_c6ca4_row205_col6" class="data row205 col6" >0.170592</td>
      <td id="T_c6ca4_row205_col7" class="data row205 col7" >0.986155</td>
      <td id="T_c6ca4_row205_col8" class="data row205 col8" >0.152344</td>
      <td id="T_c6ca4_row205_col9" class="data row205 col9" >1048576.000000</td>
      <td id="T_c6ca4_row205_col10" class="data row205 col10" >torch.float32</td>
      <td id="T_c6ca4_row205_col11" class="data row205 col11" >(1024, 1024)</td>
      <td id="T_c6ca4_row205_col12" class="data row205 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row206" class="row_heading level0 row206" >206</th>
      <td id="T_c6ca4_row206_col0" class="data row206 col0" >encoder.layers.2.self_attn.q_proj.bias</td>
      <td id="T_c6ca4_row206_col1" class="data row206 col1" >encoder.layers.2.self_attn.q_proj.bias</td>
      <td id="T_c6ca4_row206_col2" class="data row206 col2" >av_romanizer.w2v_model.encoder.layers.2.self_attn.q_proj.bias</td>
      <td id="T_c6ca4_row206_col3" class="data row206 col3" >1.02059</td>
      <td id="T_c6ca4_row206_col4" class="data row206 col4" >10.2715</td>
      <td id="T_c6ca4_row206_col5" class="data row206 col5" >10.1402</td>
      <td id="T_c6ca4_row206_col6" class="data row206 col6" >0.099361</td>
      <td id="T_c6ca4_row206_col7" class="data row206 col7" >0.995083</td>
      <td id="T_c6ca4_row206_col8" class="data row206 col8" >0.112122</td>
      <td id="T_c6ca4_row206_col9" class="data row206 col9" >1024.000000</td>
      <td id="T_c6ca4_row206_col10" class="data row206 col10" >torch.float32</td>
      <td id="T_c6ca4_row206_col11" class="data row206 col11" >(1024,)</td>
      <td id="T_c6ca4_row206_col12" class="data row206 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row207" class="row_heading level0 row207" >207</th>
      <td id="T_c6ca4_row207_col0" class="data row207 col0" >encoder.layers.2.self_attn.q_proj.weight</td>
      <td id="T_c6ca4_row207_col1" class="data row207 col1" >encoder.layers.2.self_attn.q_proj.weight</td>
      <td id="T_c6ca4_row207_col2" class="data row207 col2" >av_romanizer.w2v_model.encoder.layers.2.self_attn.q_proj.weight</td>
      <td id="T_c6ca4_row207_col3" class="data row207 col3" >19.8533</td>
      <td id="T_c6ca4_row207_col4" class="data row207 col4" >146.045</td>
      <td id="T_c6ca4_row207_col5" class="data row207 col5" >140.64</td>
      <td id="T_c6ca4_row207_col6" class="data row207 col6" >0.13594</td>
      <td id="T_c6ca4_row207_col7" class="data row207 col7" >0.991116</td>
      <td id="T_c6ca4_row207_col8" class="data row207 col8" >0.132568</td>
      <td id="T_c6ca4_row207_col9" class="data row207 col9" >1048576.000000</td>
      <td id="T_c6ca4_row207_col10" class="data row207 col10" >torch.float32</td>
      <td id="T_c6ca4_row207_col11" class="data row207 col11" >(1024, 1024)</td>
      <td id="T_c6ca4_row207_col12" class="data row207 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row208" class="row_heading level0 row208" >208</th>
      <td id="T_c6ca4_row208_col0" class="data row208 col0" >encoder.layers.2.self_attn.v_proj.bias</td>
      <td id="T_c6ca4_row208_col1" class="data row208 col1" >encoder.layers.2.self_attn.v_proj.bias</td>
      <td id="T_c6ca4_row208_col2" class="data row208 col2" >av_romanizer.w2v_model.encoder.layers.2.self_attn.v_proj.bias</td>
      <td id="T_c6ca4_row208_col3" class="data row208 col3" >0.248166</td>
      <td id="T_c6ca4_row208_col4" class="data row208 col4" >0.936618</td>
      <td id="T_c6ca4_row208_col5" class="data row208 col5" >0.833305</td>
      <td id="T_c6ca4_row208_col6" class="data row208 col6" >0.26496</td>
      <td id="T_c6ca4_row208_col7" class="data row208 col7" >0.967384</td>
      <td id="T_c6ca4_row208_col8" class="data row208 col8" >0.037323</td>
      <td id="T_c6ca4_row208_col9" class="data row208 col9" >1024.000000</td>
      <td id="T_c6ca4_row208_col10" class="data row208 col10" >torch.float32</td>
      <td id="T_c6ca4_row208_col11" class="data row208 col11" >(1024,)</td>
      <td id="T_c6ca4_row208_col12" class="data row208 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row209" class="row_heading level0 row209" >209</th>
      <td id="T_c6ca4_row209_col0" class="data row209 col0" >encoder.layers.2.self_attn.v_proj.weight</td>
      <td id="T_c6ca4_row209_col1" class="data row209 col1" >encoder.layers.2.self_attn.v_proj.weight</td>
      <td id="T_c6ca4_row209_col2" class="data row209 col2" >av_romanizer.w2v_model.encoder.layers.2.self_attn.v_proj.weight</td>
      <td id="T_c6ca4_row209_col3" class="data row209 col3" >18.712</td>
      <td id="T_c6ca4_row209_col4" class="data row209 col4" >113.929</td>
      <td id="T_c6ca4_row209_col5" class="data row209 col5" >107.788</td>
      <td id="T_c6ca4_row209_col6" class="data row209 col6" >0.164242</td>
      <td id="T_c6ca4_row209_col7" class="data row209 col7" >0.98728</td>
      <td id="T_c6ca4_row209_col8" class="data row209 col8" >0.142769</td>
      <td id="T_c6ca4_row209_col9" class="data row209 col9" >1048576.000000</td>
      <td id="T_c6ca4_row209_col10" class="data row209 col10" >torch.float32</td>
      <td id="T_c6ca4_row209_col11" class="data row209 col11" >(1024, 1024)</td>
      <td id="T_c6ca4_row209_col12" class="data row209 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row210" class="row_heading level0 row210" >210</th>
      <td id="T_c6ca4_row210_col0" class="data row210 col0" >encoder.layers.2.self_attn_layer_norm.bias</td>
      <td id="T_c6ca4_row210_col1" class="data row210 col1" >encoder.layers.2.self_attn_layer_norm.bias</td>
      <td id="T_c6ca4_row210_col2" class="data row210 col2" >av_romanizer.w2v_model.encoder.layers.2.self_attn_layer_norm.bias</td>
      <td id="T_c6ca4_row210_col3" class="data row210 col3" >0.18644</td>
      <td id="T_c6ca4_row210_col4" class="data row210 col4" >0.851021</td>
      <td id="T_c6ca4_row210_col5" class="data row210 col5" >0.794898</td>
      <td id="T_c6ca4_row210_col6" class="data row210 col6" >0.219078</td>
      <td id="T_c6ca4_row210_col7" class="data row210 col7" >0.976636</td>
      <td id="T_c6ca4_row210_col8" class="data row210 col8" >0.0444946</td>
      <td id="T_c6ca4_row210_col9" class="data row210 col9" >1024.000000</td>
      <td id="T_c6ca4_row210_col10" class="data row210 col10" >torch.float32</td>
      <td id="T_c6ca4_row210_col11" class="data row210 col11" >(1024,)</td>
      <td id="T_c6ca4_row210_col12" class="data row210 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row211" class="row_heading level0 row211" >211</th>
      <td id="T_c6ca4_row211_col0" class="data row211 col0" >encoder.layers.2.self_attn_layer_norm.weight</td>
      <td id="T_c6ca4_row211_col1" class="data row211 col1" >encoder.layers.2.self_attn_layer_norm.weight</td>
      <td id="T_c6ca4_row211_col2" class="data row211 col2" >av_romanizer.w2v_model.encoder.layers.2.self_attn_layer_norm.weight</td>
      <td id="T_c6ca4_row211_col3" class="data row211 col3" >0.423984</td>
      <td id="T_c6ca4_row211_col4" class="data row211 col4" >4.6769</td>
      <td id="T_c6ca4_row211_col5" class="data row211 col5" >4.61157</td>
      <td id="T_c6ca4_row211_col6" class="data row211 col6" >0.0906548</td>
      <td id="T_c6ca4_row211_col7" class="data row211 col7" >0.995932</td>
      <td id="T_c6ca4_row211_col8" class="data row211 col8" >0.0755615</td>
      <td id="T_c6ca4_row211_col9" class="data row211 col9" >1024.000000</td>
      <td id="T_c6ca4_row211_col10" class="data row211 col10" >torch.float32</td>
      <td id="T_c6ca4_row211_col11" class="data row211 col11" >(1024,)</td>
      <td id="T_c6ca4_row211_col12" class="data row211 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row212" class="row_heading level0 row212" >212</th>
      <td id="T_c6ca4_row212_col0" class="data row212 col0" >encoder.layers.20.fc1.bias</td>
      <td id="T_c6ca4_row212_col1" class="data row212 col1" >encoder.layers.20.fc1.bias</td>
      <td id="T_c6ca4_row212_col2" class="data row212 col2" >av_romanizer.w2v_model.encoder.layers.20.fc1.bias</td>
      <td id="T_c6ca4_row212_col3" class="data row212 col3" >0.855965</td>
      <td id="T_c6ca4_row212_col4" class="data row212 col4" >5.94947</td>
      <td id="T_c6ca4_row212_col5" class="data row212 col5" >5.7524</td>
      <td id="T_c6ca4_row212_col6" class="data row212 col6" >0.143873</td>
      <td id="T_c6ca4_row212_col7" class="data row212 col7" >0.989863</td>
      <td id="T_c6ca4_row212_col8" class="data row212 col8" >0.0603027</td>
      <td id="T_c6ca4_row212_col9" class="data row212 col9" >4096.000000</td>
      <td id="T_c6ca4_row212_col10" class="data row212 col10" >torch.float32</td>
      <td id="T_c6ca4_row212_col11" class="data row212 col11" >(4096,)</td>
      <td id="T_c6ca4_row212_col12" class="data row212 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row213" class="row_heading level0 row213" >213</th>
      <td id="T_c6ca4_row213_col0" class="data row213 col0" >encoder.layers.20.fc1.weight</td>
      <td id="T_c6ca4_row213_col1" class="data row213 col1" >encoder.layers.20.fc1.weight</td>
      <td id="T_c6ca4_row213_col2" class="data row213 col2" >av_romanizer.w2v_model.encoder.layers.20.fc1.weight</td>
      <td id="T_c6ca4_row213_col3" class="data row213 col3" >83.6005</td>
      <td id="T_c6ca4_row213_col4" class="data row213 col4" >316.998</td>
      <td id="T_c6ca4_row213_col5" class="data row213 col5" >303.924</td>
      <td id="T_c6ca4_row213_col6" class="data row213 col6" >0.263726</td>
      <td id="T_c6ca4_row213_col7" class="data row213 col7" >0.964616</td>
      <td id="T_c6ca4_row213_col8" class="data row213 col8" >0.307739</td>
      <td id="T_c6ca4_row213_col9" class="data row213 col9" >4194304.000000</td>
      <td id="T_c6ca4_row213_col10" class="data row213 col10" >torch.float32</td>
      <td id="T_c6ca4_row213_col11" class="data row213 col11" >(4096, 1024)</td>
      <td id="T_c6ca4_row213_col12" class="data row213 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row214" class="row_heading level0 row214" >214</th>
      <td id="T_c6ca4_row214_col0" class="data row214 col0" >encoder.layers.20.fc2.bias</td>
      <td id="T_c6ca4_row214_col1" class="data row214 col1" >encoder.layers.20.fc2.bias</td>
      <td id="T_c6ca4_row214_col2" class="data row214 col2" >av_romanizer.w2v_model.encoder.layers.20.fc2.bias</td>
      <td id="T_c6ca4_row214_col3" class="data row214 col3" >0.422599</td>
      <td id="T_c6ca4_row214_col4" class="data row214 col4" >5.94113</td>
      <td id="T_c6ca4_row214_col5" class="data row214 col5" >5.6772</td>
      <td id="T_c6ca4_row214_col6" class="data row214 col6" >0.0711311</td>
      <td id="T_c6ca4_row214_col7" class="data row214 col7" >0.998385</td>
      <td id="T_c6ca4_row214_col8" class="data row214 col8" >0.0410156</td>
      <td id="T_c6ca4_row214_col9" class="data row214 col9" >1024.000000</td>
      <td id="T_c6ca4_row214_col10" class="data row214 col10" >torch.float32</td>
      <td id="T_c6ca4_row214_col11" class="data row214 col11" >(1024,)</td>
      <td id="T_c6ca4_row214_col12" class="data row214 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row215" class="row_heading level0 row215" >215</th>
      <td id="T_c6ca4_row215_col0" class="data row215 col0" >encoder.layers.20.fc2.weight</td>
      <td id="T_c6ca4_row215_col1" class="data row215 col1" >encoder.layers.20.fc2.weight</td>
      <td id="T_c6ca4_row215_col2" class="data row215 col2" >av_romanizer.w2v_model.encoder.layers.20.fc2.weight</td>
      <td id="T_c6ca4_row215_col3" class="data row215 col3" >83.8217</td>
      <td id="T_c6ca4_row215_col4" class="data row215 col4" >341.736</td>
      <td id="T_c6ca4_row215_col5" class="data row215 col5" >327.669</td>
      <td id="T_c6ca4_row215_col6" class="data row215 col6" >0.245282</td>
      <td id="T_c6ca4_row215_col7" class="data row215 col7" >0.969511</td>
      <td id="T_c6ca4_row215_col8" class="data row215 col8" >0.388184</td>
      <td id="T_c6ca4_row215_col9" class="data row215 col9" >4194304.000000</td>
      <td id="T_c6ca4_row215_col10" class="data row215 col10" >torch.float32</td>
      <td id="T_c6ca4_row215_col11" class="data row215 col11" >(1024, 4096)</td>
      <td id="T_c6ca4_row215_col12" class="data row215 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row216" class="row_heading level0 row216" >216</th>
      <td id="T_c6ca4_row216_col0" class="data row216 col0" >encoder.layers.20.final_layer_norm.bias</td>
      <td id="T_c6ca4_row216_col1" class="data row216 col1" >encoder.layers.20.final_layer_norm.bias</td>
      <td id="T_c6ca4_row216_col2" class="data row216 col2" >av_romanizer.w2v_model.encoder.layers.20.final_layer_norm.bias</td>
      <td id="T_c6ca4_row216_col3" class="data row216 col3" >0.470602</td>
      <td id="T_c6ca4_row216_col4" class="data row216 col4" >8.29236</td>
      <td id="T_c6ca4_row216_col5" class="data row216 col5" >8.0471</td>
      <td id="T_c6ca4_row216_col6" class="data row216 col6" >0.0567513</td>
      <td id="T_c6ca4_row216_col7" class="data row216 col7" >0.998791</td>
      <td id="T_c6ca4_row216_col8" class="data row216 col8" >0.0537109</td>
      <td id="T_c6ca4_row216_col9" class="data row216 col9" >1024.000000</td>
      <td id="T_c6ca4_row216_col10" class="data row216 col10" >torch.float32</td>
      <td id="T_c6ca4_row216_col11" class="data row216 col11" >(1024,)</td>
      <td id="T_c6ca4_row216_col12" class="data row216 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row217" class="row_heading level0 row217" >217</th>
      <td id="T_c6ca4_row217_col0" class="data row217 col0" >encoder.layers.20.final_layer_norm.weight</td>
      <td id="T_c6ca4_row217_col1" class="data row217 col1" >encoder.layers.20.final_layer_norm.weight</td>
      <td id="T_c6ca4_row217_col2" class="data row217 col2" >av_romanizer.w2v_model.encoder.layers.20.final_layer_norm.weight</td>
      <td id="T_c6ca4_row217_col3" class="data row217 col3" >1.86176</td>
      <td id="T_c6ca4_row217_col4" class="data row217 col4" >42.1247</td>
      <td id="T_c6ca4_row217_col5" class="data row217 col5" >40.6498</td>
      <td id="T_c6ca4_row217_col6" class="data row217 col6" >0.0441965</td>
      <td id="T_c6ca4_row217_col7" class="data row217 col7" >0.999623</td>
      <td id="T_c6ca4_row217_col8" class="data row217 col8" >0.217773</td>
      <td id="T_c6ca4_row217_col9" class="data row217 col9" >1024.000000</td>
      <td id="T_c6ca4_row217_col10" class="data row217 col10" >torch.float32</td>
      <td id="T_c6ca4_row217_col11" class="data row217 col11" >(1024,)</td>
      <td id="T_c6ca4_row217_col12" class="data row217 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row218" class="row_heading level0 row218" >218</th>
      <td id="T_c6ca4_row218_col0" class="data row218 col0" >encoder.layers.20.self_attn.k_proj.bias</td>
      <td id="T_c6ca4_row218_col1" class="data row218 col1" >encoder.layers.20.self_attn.k_proj.bias</td>
      <td id="T_c6ca4_row218_col2" class="data row218 col2" >av_romanizer.w2v_model.encoder.layers.20.self_attn.k_proj.bias</td>
      <td id="T_c6ca4_row218_col3" class="data row218 col3" >0.0325409</td>
      <td id="T_c6ca4_row218_col4" class="data row218 col4" >0.0763827</td>
      <td id="T_c6ca4_row218_col5" class="data row218 col5" >0.0878344</td>
      <td id="T_c6ca4_row218_col6" class="data row218 col6" >0.426024</td>
      <td id="T_c6ca4_row218_col7" class="data row218 col7" >0.930857</td>
      <td id="T_c6ca4_row218_col8" class="data row218 col8" >0.0175629</td>
      <td id="T_c6ca4_row218_col9" class="data row218 col9" >1024.000000</td>
      <td id="T_c6ca4_row218_col10" class="data row218 col10" >torch.float32</td>
      <td id="T_c6ca4_row218_col11" class="data row218 col11" >(1024,)</td>
      <td id="T_c6ca4_row218_col12" class="data row218 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row219" class="row_heading level0 row219" >219</th>
      <td id="T_c6ca4_row219_col0" class="data row219 col0" >encoder.layers.20.self_attn.k_proj.weight</td>
      <td id="T_c6ca4_row219_col1" class="data row219 col1" >encoder.layers.20.self_attn.k_proj.weight</td>
      <td id="T_c6ca4_row219_col2" class="data row219 col2" >av_romanizer.w2v_model.encoder.layers.20.self_attn.k_proj.weight</td>
      <td id="T_c6ca4_row219_col3" class="data row219 col3" >31.7376</td>
      <td id="T_c6ca4_row219_col4" class="data row219 col4" >113.961</td>
      <td id="T_c6ca4_row219_col5" class="data row219 col5" >109.425</td>
      <td id="T_c6ca4_row219_col6" class="data row219 col6" >0.278495</td>
      <td id="T_c6ca4_row219_col7" class="data row219 col7" >0.960438</td>
      <td id="T_c6ca4_row219_col8" class="data row219 col8" >0.195801</td>
      <td id="T_c6ca4_row219_col9" class="data row219 col9" >1048576.000000</td>
      <td id="T_c6ca4_row219_col10" class="data row219 col10" >torch.float32</td>
      <td id="T_c6ca4_row219_col11" class="data row219 col11" >(1024, 1024)</td>
      <td id="T_c6ca4_row219_col12" class="data row219 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row220" class="row_heading level0 row220" >220</th>
      <td id="T_c6ca4_row220_col0" class="data row220 col0" >encoder.layers.20.self_attn.out_proj.bias</td>
      <td id="T_c6ca4_row220_col1" class="data row220 col1" >encoder.layers.20.self_attn.out_proj.bias</td>
      <td id="T_c6ca4_row220_col2" class="data row220 col2" >av_romanizer.w2v_model.encoder.layers.20.self_attn.out_proj.bias</td>
      <td id="T_c6ca4_row220_col3" class="data row220 col3" >0.400451</td>
      <td id="T_c6ca4_row220_col4" class="data row220 col4" >5.71833</td>
      <td id="T_c6ca4_row220_col5" class="data row220 col5" >5.51246</td>
      <td id="T_c6ca4_row220_col6" class="data row220 col6" >0.0700294</td>
      <td id="T_c6ca4_row220_col7" class="data row220 col7" >0.998129</td>
      <td id="T_c6ca4_row220_col8" class="data row220 col8" >0.041748</td>
      <td id="T_c6ca4_row220_col9" class="data row220 col9" >1024.000000</td>
      <td id="T_c6ca4_row220_col10" class="data row220 col10" >torch.float32</td>
      <td id="T_c6ca4_row220_col11" class="data row220 col11" >(1024,)</td>
      <td id="T_c6ca4_row220_col12" class="data row220 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row221" class="row_heading level0 row221" >221</th>
      <td id="T_c6ca4_row221_col0" class="data row221 col0" >encoder.layers.20.self_attn.out_proj.weight</td>
      <td id="T_c6ca4_row221_col1" class="data row221 col1" >encoder.layers.20.self_attn.out_proj.weight</td>
      <td id="T_c6ca4_row221_col2" class="data row221 col2" >av_romanizer.w2v_model.encoder.layers.20.self_attn.out_proj.weight</td>
      <td id="T_c6ca4_row221_col3" class="data row221 col3" >40.0973</td>
      <td id="T_c6ca4_row221_col4" class="data row221 col4" >198.318</td>
      <td id="T_c6ca4_row221_col5" class="data row221 col5" >190.561</td>
      <td id="T_c6ca4_row221_col6" class="data row221 col6" >0.202187</td>
      <td id="T_c6ca4_row221_col7" class="data row221 col7" >0.979524</td>
      <td id="T_c6ca4_row221_col8" class="data row221 col8" >0.240967</td>
      <td id="T_c6ca4_row221_col9" class="data row221 col9" >1048576.000000</td>
      <td id="T_c6ca4_row221_col10" class="data row221 col10" >torch.float32</td>
      <td id="T_c6ca4_row221_col11" class="data row221 col11" >(1024, 1024)</td>
      <td id="T_c6ca4_row221_col12" class="data row221 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row222" class="row_heading level0 row222" >222</th>
      <td id="T_c6ca4_row222_col0" class="data row222 col0" >encoder.layers.20.self_attn.q_proj.bias</td>
      <td id="T_c6ca4_row222_col1" class="data row222 col1" >encoder.layers.20.self_attn.q_proj.bias</td>
      <td id="T_c6ca4_row222_col2" class="data row222 col2" >av_romanizer.w2v_model.encoder.layers.20.self_attn.q_proj.bias</td>
      <td id="T_c6ca4_row222_col3" class="data row222 col3" >0.896881</td>
      <td id="T_c6ca4_row222_col4" class="data row222 col4" >8.02675</td>
      <td id="T_c6ca4_row222_col5" class="data row222 col5" >7.71964</td>
      <td id="T_c6ca4_row222_col6" class="data row222 col6" >0.111736</td>
      <td id="T_c6ca4_row222_col7" class="data row222 col7" >0.99427</td>
      <td id="T_c6ca4_row222_col8" class="data row222 col8" >0.12207</td>
      <td id="T_c6ca4_row222_col9" class="data row222 col9" >1024.000000</td>
      <td id="T_c6ca4_row222_col10" class="data row222 col10" >torch.float32</td>
      <td id="T_c6ca4_row222_col11" class="data row222 col11" >(1024,)</td>
      <td id="T_c6ca4_row222_col12" class="data row222 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row223" class="row_heading level0 row223" >223</th>
      <td id="T_c6ca4_row223_col0" class="data row223 col0" >encoder.layers.20.self_attn.q_proj.weight</td>
      <td id="T_c6ca4_row223_col1" class="data row223 col1" >encoder.layers.20.self_attn.q_proj.weight</td>
      <td id="T_c6ca4_row223_col2" class="data row223 col2" >av_romanizer.w2v_model.encoder.layers.20.self_attn.q_proj.weight</td>
      <td id="T_c6ca4_row223_col3" class="data row223 col3" >30.8928</td>
      <td id="T_c6ca4_row223_col4" class="data row223 col4" >111.376</td>
      <td id="T_c6ca4_row223_col5" class="data row223 col5" >106.972</td>
      <td id="T_c6ca4_row223_col6" class="data row223 col6" >0.277373</td>
      <td id="T_c6ca4_row223_col7" class="data row223 col7" >0.960762</td>
      <td id="T_c6ca4_row223_col8" class="data row223 col8" >0.174561</td>
      <td id="T_c6ca4_row223_col9" class="data row223 col9" >1048576.000000</td>
      <td id="T_c6ca4_row223_col10" class="data row223 col10" >torch.float32</td>
      <td id="T_c6ca4_row223_col11" class="data row223 col11" >(1024, 1024)</td>
      <td id="T_c6ca4_row223_col12" class="data row223 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row224" class="row_heading level0 row224" >224</th>
      <td id="T_c6ca4_row224_col0" class="data row224 col0" >encoder.layers.20.self_attn.v_proj.bias</td>
      <td id="T_c6ca4_row224_col1" class="data row224 col1" >encoder.layers.20.self_attn.v_proj.bias</td>
      <td id="T_c6ca4_row224_col2" class="data row224 col2" >av_romanizer.w2v_model.encoder.layers.20.self_attn.v_proj.bias</td>
      <td id="T_c6ca4_row224_col3" class="data row224 col3" >0.382003</td>
      <td id="T_c6ca4_row224_col4" class="data row224 col4" >3.56256</td>
      <td id="T_c6ca4_row224_col5" class="data row224 col5" >3.45858</td>
      <td id="T_c6ca4_row224_col6" class="data row224 col6" >0.107227</td>
      <td id="T_c6ca4_row224_col7" class="data row224 col7" >0.994517</td>
      <td id="T_c6ca4_row224_col8" class="data row224 col8" >0.0491943</td>
      <td id="T_c6ca4_row224_col9" class="data row224 col9" >1024.000000</td>
      <td id="T_c6ca4_row224_col10" class="data row224 col10" >torch.float32</td>
      <td id="T_c6ca4_row224_col11" class="data row224 col11" >(1024,)</td>
      <td id="T_c6ca4_row224_col12" class="data row224 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row225" class="row_heading level0 row225" >225</th>
      <td id="T_c6ca4_row225_col0" class="data row225 col0" >encoder.layers.20.self_attn.v_proj.weight</td>
      <td id="T_c6ca4_row225_col1" class="data row225 col1" >encoder.layers.20.self_attn.v_proj.weight</td>
      <td id="T_c6ca4_row225_col2" class="data row225 col2" >av_romanizer.w2v_model.encoder.layers.20.self_attn.v_proj.weight</td>
      <td id="T_c6ca4_row225_col3" class="data row225 col3" >33.3974</td>
      <td id="T_c6ca4_row225_col4" class="data row225 col4" >194.562</td>
      <td id="T_c6ca4_row225_col5" class="data row225 col5" >186.678</td>
      <td id="T_c6ca4_row225_col6" class="data row225 col6" >0.171654</td>
      <td id="T_c6ca4_row225_col7" class="data row225 col7" >0.985501</td>
      <td id="T_c6ca4_row225_col8" class="data row225 col8" >0.192383</td>
      <td id="T_c6ca4_row225_col9" class="data row225 col9" >1048576.000000</td>
      <td id="T_c6ca4_row225_col10" class="data row225 col10" >torch.float32</td>
      <td id="T_c6ca4_row225_col11" class="data row225 col11" >(1024, 1024)</td>
      <td id="T_c6ca4_row225_col12" class="data row225 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row226" class="row_heading level0 row226" >226</th>
      <td id="T_c6ca4_row226_col0" class="data row226 col0" >encoder.layers.20.self_attn_layer_norm.bias</td>
      <td id="T_c6ca4_row226_col1" class="data row226 col1" >encoder.layers.20.self_attn_layer_norm.bias</td>
      <td id="T_c6ca4_row226_col2" class="data row226 col2" >av_romanizer.w2v_model.encoder.layers.20.self_attn_layer_norm.bias</td>
      <td id="T_c6ca4_row226_col3" class="data row226 col3" >0.265538</td>
      <td id="T_c6ca4_row226_col4" class="data row226 col4" >1.84231</td>
      <td id="T_c6ca4_row226_col5" class="data row226 col5" >1.82456</td>
      <td id="T_c6ca4_row226_col6" class="data row226 col6" >0.144133</td>
      <td id="T_c6ca4_row226_col7" class="data row226 col7" >0.989559</td>
      <td id="T_c6ca4_row226_col8" class="data row226 col8" >0.0408154</td>
      <td id="T_c6ca4_row226_col9" class="data row226 col9" >1024.000000</td>
      <td id="T_c6ca4_row226_col10" class="data row226 col10" >torch.float32</td>
      <td id="T_c6ca4_row226_col11" class="data row226 col11" >(1024,)</td>
      <td id="T_c6ca4_row226_col12" class="data row226 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row227" class="row_heading level0 row227" >227</th>
      <td id="T_c6ca4_row227_col0" class="data row227 col0" >encoder.layers.20.self_attn_layer_norm.weight</td>
      <td id="T_c6ca4_row227_col1" class="data row227 col1" >encoder.layers.20.self_attn_layer_norm.weight</td>
      <td id="T_c6ca4_row227_col2" class="data row227 col2" >av_romanizer.w2v_model.encoder.layers.20.self_attn_layer_norm.weight</td>
      <td id="T_c6ca4_row227_col3" class="data row227 col3" >0.535975</td>
      <td id="T_c6ca4_row227_col4" class="data row227 col4" >9.69223</td>
      <td id="T_c6ca4_row227_col5" class="data row227 col5" >9.89071</td>
      <td id="T_c6ca4_row227_col6" class="data row227 col6" >0.0552995</td>
      <td id="T_c6ca4_row227_col7" class="data row227 col7" >0.998707</td>
      <td id="T_c6ca4_row227_col8" class="data row227 col8" >0.0654297</td>
      <td id="T_c6ca4_row227_col9" class="data row227 col9" >1024.000000</td>
      <td id="T_c6ca4_row227_col10" class="data row227 col10" >torch.float32</td>
      <td id="T_c6ca4_row227_col11" class="data row227 col11" >(1024,)</td>
      <td id="T_c6ca4_row227_col12" class="data row227 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row228" class="row_heading level0 row228" >228</th>
      <td id="T_c6ca4_row228_col0" class="data row228 col0" >encoder.layers.21.fc1.bias</td>
      <td id="T_c6ca4_row228_col1" class="data row228 col1" >encoder.layers.21.fc1.bias</td>
      <td id="T_c6ca4_row228_col2" class="data row228 col2" >av_romanizer.w2v_model.encoder.layers.21.fc1.bias</td>
      <td id="T_c6ca4_row228_col3" class="data row228 col3" >1.04538</td>
      <td id="T_c6ca4_row228_col4" class="data row228 col4" >7.34571</td>
      <td id="T_c6ca4_row228_col5" class="data row228 col5" >7.10024</td>
      <td id="T_c6ca4_row228_col6" class="data row228 col6" >0.142312</td>
      <td id="T_c6ca4_row228_col7" class="data row228 col7" >0.990101</td>
      <td id="T_c6ca4_row228_col8" class="data row228 col8" >0.0809898</td>
      <td id="T_c6ca4_row228_col9" class="data row228 col9" >4096.000000</td>
      <td id="T_c6ca4_row228_col10" class="data row228 col10" >torch.float32</td>
      <td id="T_c6ca4_row228_col11" class="data row228 col11" >(4096,)</td>
      <td id="T_c6ca4_row228_col12" class="data row228 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row229" class="row_heading level0 row229" >229</th>
      <td id="T_c6ca4_row229_col0" class="data row229 col0" >encoder.layers.21.fc1.weight</td>
      <td id="T_c6ca4_row229_col1" class="data row229 col1" >encoder.layers.21.fc1.weight</td>
      <td id="T_c6ca4_row229_col2" class="data row229 col2" >av_romanizer.w2v_model.encoder.layers.21.fc1.weight</td>
      <td id="T_c6ca4_row229_col3" class="data row229 col3" >90.7404</td>
      <td id="T_c6ca4_row229_col4" class="data row229 col4" >316.935</td>
      <td id="T_c6ca4_row229_col5" class="data row229 col5" >302.441</td>
      <td id="T_c6ca4_row229_col6" class="data row229 col6" >0.286306</td>
      <td id="T_c6ca4_row229_col7" class="data row229 col7" >0.958146</td>
      <td id="T_c6ca4_row229_col8" class="data row229 col8" >0.350403</td>
      <td id="T_c6ca4_row229_col9" class="data row229 col9" >4194304.000000</td>
      <td id="T_c6ca4_row229_col10" class="data row229 col10" >torch.float32</td>
      <td id="T_c6ca4_row229_col11" class="data row229 col11" >(4096, 1024)</td>
      <td id="T_c6ca4_row229_col12" class="data row229 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row230" class="row_heading level0 row230" >230</th>
      <td id="T_c6ca4_row230_col0" class="data row230 col0" >encoder.layers.21.fc2.bias</td>
      <td id="T_c6ca4_row230_col1" class="data row230 col1" >encoder.layers.21.fc2.bias</td>
      <td id="T_c6ca4_row230_col2" class="data row230 col2" >av_romanizer.w2v_model.encoder.layers.21.fc2.bias</td>
      <td id="T_c6ca4_row230_col3" class="data row230 col3" >0.399444</td>
      <td id="T_c6ca4_row230_col4" class="data row230 col4" >5.45059</td>
      <td id="T_c6ca4_row230_col5" class="data row230 col5" >5.14165</td>
      <td id="T_c6ca4_row230_col6" class="data row230 col6" >0.0732845</td>
      <td id="T_c6ca4_row230_col7" class="data row230 col7" >0.998856</td>
      <td id="T_c6ca4_row230_col8" class="data row230 col8" >0.0407715</td>
      <td id="T_c6ca4_row230_col9" class="data row230 col9" >1024.000000</td>
      <td id="T_c6ca4_row230_col10" class="data row230 col10" >torch.float32</td>
      <td id="T_c6ca4_row230_col11" class="data row230 col11" >(1024,)</td>
      <td id="T_c6ca4_row230_col12" class="data row230 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row231" class="row_heading level0 row231" >231</th>
      <td id="T_c6ca4_row231_col0" class="data row231 col0" >encoder.layers.21.fc2.weight</td>
      <td id="T_c6ca4_row231_col1" class="data row231 col1" >encoder.layers.21.fc2.weight</td>
      <td id="T_c6ca4_row231_col2" class="data row231 col2" >av_romanizer.w2v_model.encoder.layers.21.fc2.weight</td>
      <td id="T_c6ca4_row231_col3" class="data row231 col3" >87.9976</td>
      <td id="T_c6ca4_row231_col4" class="data row231 col4" >344.65</td>
      <td id="T_c6ca4_row231_col5" class="data row231 col5" >328.172</td>
      <td id="T_c6ca4_row231_col6" class="data row231 col6" >0.255325</td>
      <td id="T_c6ca4_row231_col7" class="data row231 col7" >0.966968</td>
      <td id="T_c6ca4_row231_col8" class="data row231 col8" >0.343018</td>
      <td id="T_c6ca4_row231_col9" class="data row231 col9" >4194304.000000</td>
      <td id="T_c6ca4_row231_col10" class="data row231 col10" >torch.float32</td>
      <td id="T_c6ca4_row231_col11" class="data row231 col11" >(1024, 4096)</td>
      <td id="T_c6ca4_row231_col12" class="data row231 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row232" class="row_heading level0 row232" >232</th>
      <td id="T_c6ca4_row232_col0" class="data row232 col0" >encoder.layers.21.final_layer_norm.bias</td>
      <td id="T_c6ca4_row232_col1" class="data row232 col1" >encoder.layers.21.final_layer_norm.bias</td>
      <td id="T_c6ca4_row232_col2" class="data row232 col2" >av_romanizer.w2v_model.encoder.layers.21.final_layer_norm.bias</td>
      <td id="T_c6ca4_row232_col3" class="data row232 col3" >1.0257</td>
      <td id="T_c6ca4_row232_col4" class="data row232 col4" >9.09107</td>
      <td id="T_c6ca4_row232_col5" class="data row232 col5" >8.72288</td>
      <td id="T_c6ca4_row232_col6" class="data row232 col6" >0.112825</td>
      <td id="T_c6ca4_row232_col7" class="data row232 col7" >0.994221</td>
      <td id="T_c6ca4_row232_col8" class="data row232 col8" >0.105957</td>
      <td id="T_c6ca4_row232_col9" class="data row232 col9" >1024.000000</td>
      <td id="T_c6ca4_row232_col10" class="data row232 col10" >torch.float32</td>
      <td id="T_c6ca4_row232_col11" class="data row232 col11" >(1024,)</td>
      <td id="T_c6ca4_row232_col12" class="data row232 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row233" class="row_heading level0 row233" >233</th>
      <td id="T_c6ca4_row233_col0" class="data row233 col0" >encoder.layers.21.final_layer_norm.weight</td>
      <td id="T_c6ca4_row233_col1" class="data row233 col1" >encoder.layers.21.final_layer_norm.weight</td>
      <td id="T_c6ca4_row233_col2" class="data row233 col2" >av_romanizer.w2v_model.encoder.layers.21.final_layer_norm.weight</td>
      <td id="T_c6ca4_row233_col3" class="data row233 col3" >3.34722</td>
      <td id="T_c6ca4_row233_col4" class="data row233 col4" >55.8101</td>
      <td id="T_c6ca4_row233_col5" class="data row233 col5" >52.8248</td>
      <td id="T_c6ca4_row233_col6" class="data row233 col6" >0.0599752</td>
      <td id="T_c6ca4_row233_col7" class="data row233 col7" >0.999611</td>
      <td id="T_c6ca4_row233_col8" class="data row233 col8" >0.335938</td>
      <td id="T_c6ca4_row233_col9" class="data row233 col9" >1024.000000</td>
      <td id="T_c6ca4_row233_col10" class="data row233 col10" >torch.float32</td>
      <td id="T_c6ca4_row233_col11" class="data row233 col11" >(1024,)</td>
      <td id="T_c6ca4_row233_col12" class="data row233 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row234" class="row_heading level0 row234" >234</th>
      <td id="T_c6ca4_row234_col0" class="data row234 col0" >encoder.layers.21.self_attn.k_proj.bias</td>
      <td id="T_c6ca4_row234_col1" class="data row234 col1" >encoder.layers.21.self_attn.k_proj.bias</td>
      <td id="T_c6ca4_row234_col2" class="data row234 col2" >av_romanizer.w2v_model.encoder.layers.21.self_attn.k_proj.bias</td>
      <td id="T_c6ca4_row234_col3" class="data row234 col3" >0.0465226</td>
      <td id="T_c6ca4_row234_col4" class="data row234 col4" >0.21034</td>
      <td id="T_c6ca4_row234_col5" class="data row234 col5" >0.215799</td>
      <td id="T_c6ca4_row234_col6" class="data row234 col6" >0.221178</td>
      <td id="T_c6ca4_row234_col7" class="data row234 col7" >0.976487</td>
      <td id="T_c6ca4_row234_col8" class="data row234 col8" >0.00915146</td>
      <td id="T_c6ca4_row234_col9" class="data row234 col9" >1024.000000</td>
      <td id="T_c6ca4_row234_col10" class="data row234 col10" >torch.float32</td>
      <td id="T_c6ca4_row234_col11" class="data row234 col11" >(1024,)</td>
      <td id="T_c6ca4_row234_col12" class="data row234 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row235" class="row_heading level0 row235" >235</th>
      <td id="T_c6ca4_row235_col0" class="data row235 col0" >encoder.layers.21.self_attn.k_proj.weight</td>
      <td id="T_c6ca4_row235_col1" class="data row235 col1" >encoder.layers.21.self_attn.k_proj.weight</td>
      <td id="T_c6ca4_row235_col2" class="data row235 col2" >av_romanizer.w2v_model.encoder.layers.21.self_attn.k_proj.weight</td>
      <td id="T_c6ca4_row235_col3" class="data row235 col3" >33.3309</td>
      <td id="T_c6ca4_row235_col4" class="data row235 col4" >111.532</td>
      <td id="T_c6ca4_row235_col5" class="data row235 col5" >106.578</td>
      <td id="T_c6ca4_row235_col6" class="data row235 col6" >0.298846</td>
      <td id="T_c6ca4_row235_col7" class="data row235 col7" >0.954302</td>
      <td id="T_c6ca4_row235_col8" class="data row235 col8" >0.174866</td>
      <td id="T_c6ca4_row235_col9" class="data row235 col9" >1048576.000000</td>
      <td id="T_c6ca4_row235_col10" class="data row235 col10" >torch.float32</td>
      <td id="T_c6ca4_row235_col11" class="data row235 col11" >(1024, 1024)</td>
      <td id="T_c6ca4_row235_col12" class="data row235 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row236" class="row_heading level0 row236" >236</th>
      <td id="T_c6ca4_row236_col0" class="data row236 col0" >encoder.layers.21.self_attn.out_proj.bias</td>
      <td id="T_c6ca4_row236_col1" class="data row236 col1" >encoder.layers.21.self_attn.out_proj.bias</td>
      <td id="T_c6ca4_row236_col2" class="data row236 col2" >av_romanizer.w2v_model.encoder.layers.21.self_attn.out_proj.bias</td>
      <td id="T_c6ca4_row236_col3" class="data row236 col3" >0.396243</td>
      <td id="T_c6ca4_row236_col4" class="data row236 col4" >5.47682</td>
      <td id="T_c6ca4_row236_col5" class="data row236 col5" >5.23366</td>
      <td id="T_c6ca4_row236_col6" class="data row236 col6" >0.0723491</td>
      <td id="T_c6ca4_row236_col7" class="data row236 col7" >0.998293</td>
      <td id="T_c6ca4_row236_col8" class="data row236 col8" >0.0366211</td>
      <td id="T_c6ca4_row236_col9" class="data row236 col9" >1024.000000</td>
      <td id="T_c6ca4_row236_col10" class="data row236 col10" >torch.float32</td>
      <td id="T_c6ca4_row236_col11" class="data row236 col11" >(1024,)</td>
      <td id="T_c6ca4_row236_col12" class="data row236 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row237" class="row_heading level0 row237" >237</th>
      <td id="T_c6ca4_row237_col0" class="data row237 col0" >encoder.layers.21.self_attn.out_proj.weight</td>
      <td id="T_c6ca4_row237_col1" class="data row237 col1" >encoder.layers.21.self_attn.out_proj.weight</td>
      <td id="T_c6ca4_row237_col2" class="data row237 col2" >av_romanizer.w2v_model.encoder.layers.21.self_attn.out_proj.weight</td>
      <td id="T_c6ca4_row237_col3" class="data row237 col3" >45.8489</td>
      <td id="T_c6ca4_row237_col4" class="data row237 col4" >206.024</td>
      <td id="T_c6ca4_row237_col5" class="data row237 col5" >199.027</td>
      <td id="T_c6ca4_row237_col6" class="data row237 col6" >0.222541</td>
      <td id="T_c6ca4_row237_col7" class="data row237 col7" >0.974964</td>
      <td id="T_c6ca4_row237_col8" class="data row237 col8" >0.297852</td>
      <td id="T_c6ca4_row237_col9" class="data row237 col9" >1048576.000000</td>
      <td id="T_c6ca4_row237_col10" class="data row237 col10" >torch.float32</td>
      <td id="T_c6ca4_row237_col11" class="data row237 col11" >(1024, 1024)</td>
      <td id="T_c6ca4_row237_col12" class="data row237 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row238" class="row_heading level0 row238" >238</th>
      <td id="T_c6ca4_row238_col0" class="data row238 col0" >encoder.layers.21.self_attn.q_proj.bias</td>
      <td id="T_c6ca4_row238_col1" class="data row238 col1" >encoder.layers.21.self_attn.q_proj.bias</td>
      <td id="T_c6ca4_row238_col2" class="data row238 col2" >av_romanizer.w2v_model.encoder.layers.21.self_attn.q_proj.bias</td>
      <td id="T_c6ca4_row238_col3" class="data row238 col3" >0.810494</td>
      <td id="T_c6ca4_row238_col4" class="data row238 col4" >7.19212</td>
      <td id="T_c6ca4_row238_col5" class="data row238 col5" >6.94407</td>
      <td id="T_c6ca4_row238_col6" class="data row238 col6" >0.112692</td>
      <td id="T_c6ca4_row238_col7" class="data row238 col7" >0.994039</td>
      <td id="T_c6ca4_row238_col8" class="data row238 col8" >0.108398</td>
      <td id="T_c6ca4_row238_col9" class="data row238 col9" >1024.000000</td>
      <td id="T_c6ca4_row238_col10" class="data row238 col10" >torch.float32</td>
      <td id="T_c6ca4_row238_col11" class="data row238 col11" >(1024,)</td>
      <td id="T_c6ca4_row238_col12" class="data row238 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row239" class="row_heading level0 row239" >239</th>
      <td id="T_c6ca4_row239_col0" class="data row239 col0" >encoder.layers.21.self_attn.q_proj.weight</td>
      <td id="T_c6ca4_row239_col1" class="data row239 col1" >encoder.layers.21.self_attn.q_proj.weight</td>
      <td id="T_c6ca4_row239_col2" class="data row239 col2" >av_romanizer.w2v_model.encoder.layers.21.self_attn.q_proj.weight</td>
      <td id="T_c6ca4_row239_col3" class="data row239 col3" >32.6919</td>
      <td id="T_c6ca4_row239_col4" class="data row239 col4" >108.386</td>
      <td id="T_c6ca4_row239_col5" class="data row239 col5" >103.561</td>
      <td id="T_c6ca4_row239_col6" class="data row239 col6" >0.301625</td>
      <td id="T_c6ca4_row239_col7" class="data row239 col7" >0.953429</td>
      <td id="T_c6ca4_row239_col8" class="data row239 col8" >0.194275</td>
      <td id="T_c6ca4_row239_col9" class="data row239 col9" >1048576.000000</td>
      <td id="T_c6ca4_row239_col10" class="data row239 col10" >torch.float32</td>
      <td id="T_c6ca4_row239_col11" class="data row239 col11" >(1024, 1024)</td>
      <td id="T_c6ca4_row239_col12" class="data row239 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row240" class="row_heading level0 row240" >240</th>
      <td id="T_c6ca4_row240_col0" class="data row240 col0" >encoder.layers.21.self_attn.v_proj.bias</td>
      <td id="T_c6ca4_row240_col1" class="data row240 col1" >encoder.layers.21.self_attn.v_proj.bias</td>
      <td id="T_c6ca4_row240_col2" class="data row240 col2" >av_romanizer.w2v_model.encoder.layers.21.self_attn.v_proj.bias</td>
      <td id="T_c6ca4_row240_col3" class="data row240 col3" >0.507792</td>
      <td id="T_c6ca4_row240_col4" class="data row240 col4" >5.7069</td>
      <td id="T_c6ca4_row240_col5" class="data row240 col5" >5.4515</td>
      <td id="T_c6ca4_row240_col6" class="data row240 col6" >0.0889786</td>
      <td id="T_c6ca4_row240_col7" class="data row240 col7" >0.996904</td>
      <td id="T_c6ca4_row240_col8" class="data row240 col8" >0.074707</td>
      <td id="T_c6ca4_row240_col9" class="data row240 col9" >1024.000000</td>
      <td id="T_c6ca4_row240_col10" class="data row240 col10" >torch.float32</td>
      <td id="T_c6ca4_row240_col11" class="data row240 col11" >(1024,)</td>
      <td id="T_c6ca4_row240_col12" class="data row240 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row241" class="row_heading level0 row241" >241</th>
      <td id="T_c6ca4_row241_col0" class="data row241 col0" >encoder.layers.21.self_attn.v_proj.weight</td>
      <td id="T_c6ca4_row241_col1" class="data row241 col1" >encoder.layers.21.self_attn.v_proj.weight</td>
      <td id="T_c6ca4_row241_col2" class="data row241 col2" >av_romanizer.w2v_model.encoder.layers.21.self_attn.v_proj.weight</td>
      <td id="T_c6ca4_row241_col3" class="data row241 col3" >39.7818</td>
      <td id="T_c6ca4_row241_col4" class="data row241 col4" >200.084</td>
      <td id="T_c6ca4_row241_col5" class="data row241 col5" >192.883</td>
      <td id="T_c6ca4_row241_col6" class="data row241 col6" >0.198826</td>
      <td id="T_c6ca4_row241_col7" class="data row241 col7" >0.980168</td>
      <td id="T_c6ca4_row241_col8" class="data row241 col8" >0.22998</td>
      <td id="T_c6ca4_row241_col9" class="data row241 col9" >1048576.000000</td>
      <td id="T_c6ca4_row241_col10" class="data row241 col10" >torch.float32</td>
      <td id="T_c6ca4_row241_col11" class="data row241 col11" >(1024, 1024)</td>
      <td id="T_c6ca4_row241_col12" class="data row241 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row242" class="row_heading level0 row242" >242</th>
      <td id="T_c6ca4_row242_col0" class="data row242 col0" >encoder.layers.21.self_attn_layer_norm.bias</td>
      <td id="T_c6ca4_row242_col1" class="data row242 col1" >encoder.layers.21.self_attn_layer_norm.bias</td>
      <td id="T_c6ca4_row242_col2" class="data row242 col2" >av_romanizer.w2v_model.encoder.layers.21.self_attn_layer_norm.bias</td>
      <td id="T_c6ca4_row242_col3" class="data row242 col3" >0.343091</td>
      <td id="T_c6ca4_row242_col4" class="data row242 col4" >2.24686</td>
      <td id="T_c6ca4_row242_col5" class="data row242 col5" >2.1507</td>
      <td id="T_c6ca4_row242_col6" class="data row242 col6" >0.152698</td>
      <td id="T_c6ca4_row242_col7" class="data row242 col7" >0.988777</td>
      <td id="T_c6ca4_row242_col8" class="data row242 col8" >0.0380325</td>
      <td id="T_c6ca4_row242_col9" class="data row242 col9" >1024.000000</td>
      <td id="T_c6ca4_row242_col10" class="data row242 col10" >torch.float32</td>
      <td id="T_c6ca4_row242_col11" class="data row242 col11" >(1024,)</td>
      <td id="T_c6ca4_row242_col12" class="data row242 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row243" class="row_heading level0 row243" >243</th>
      <td id="T_c6ca4_row243_col0" class="data row243 col0" >encoder.layers.21.self_attn_layer_norm.weight</td>
      <td id="T_c6ca4_row243_col1" class="data row243 col1" >encoder.layers.21.self_attn_layer_norm.weight</td>
      <td id="T_c6ca4_row243_col2" class="data row243 col2" >av_romanizer.w2v_model.encoder.layers.21.self_attn_layer_norm.weight</td>
      <td id="T_c6ca4_row243_col3" class="data row243 col3" >0.577567</td>
      <td id="T_c6ca4_row243_col4" class="data row243 col4" >10.985</td>
      <td id="T_c6ca4_row243_col5" class="data row243 col5" >11.1985</td>
      <td id="T_c6ca4_row243_col6" class="data row243 col6" >0.0525776</td>
      <td id="T_c6ca4_row243_col7" class="data row243 col7" >0.998829</td>
      <td id="T_c6ca4_row243_col8" class="data row243 col8" >0.10498</td>
      <td id="T_c6ca4_row243_col9" class="data row243 col9" >1024.000000</td>
      <td id="T_c6ca4_row243_col10" class="data row243 col10" >torch.float32</td>
      <td id="T_c6ca4_row243_col11" class="data row243 col11" >(1024,)</td>
      <td id="T_c6ca4_row243_col12" class="data row243 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row244" class="row_heading level0 row244" >244</th>
      <td id="T_c6ca4_row244_col0" class="data row244 col0" >encoder.layers.22.fc1.bias</td>
      <td id="T_c6ca4_row244_col1" class="data row244 col1" >encoder.layers.22.fc1.bias</td>
      <td id="T_c6ca4_row244_col2" class="data row244 col2" >av_romanizer.w2v_model.encoder.layers.22.fc1.bias</td>
      <td id="T_c6ca4_row244_col3" class="data row244 col3" >0.977723</td>
      <td id="T_c6ca4_row244_col4" class="data row244 col4" >8.01064</td>
      <td id="T_c6ca4_row244_col5" class="data row244 col5" >7.88979</td>
      <td id="T_c6ca4_row244_col6" class="data row244 col6" >0.122053</td>
      <td id="T_c6ca4_row244_col7" class="data row244 col7" >0.992553</td>
      <td id="T_c6ca4_row244_col8" class="data row244 col8" >0.0810547</td>
      <td id="T_c6ca4_row244_col9" class="data row244 col9" >4096.000000</td>
      <td id="T_c6ca4_row244_col10" class="data row244 col10" >torch.float32</td>
      <td id="T_c6ca4_row244_col11" class="data row244 col11" >(4096,)</td>
      <td id="T_c6ca4_row244_col12" class="data row244 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row245" class="row_heading level0 row245" >245</th>
      <td id="T_c6ca4_row245_col0" class="data row245 col0" >encoder.layers.22.fc1.weight</td>
      <td id="T_c6ca4_row245_col1" class="data row245 col1" >encoder.layers.22.fc1.weight</td>
      <td id="T_c6ca4_row245_col2" class="data row245 col2" >av_romanizer.w2v_model.encoder.layers.22.fc1.weight</td>
      <td id="T_c6ca4_row245_col3" class="data row245 col3" >95.9962</td>
      <td id="T_c6ca4_row245_col4" class="data row245 col4" >322.858</td>
      <td id="T_c6ca4_row245_col5" class="data row245 col5" >309.93</td>
      <td id="T_c6ca4_row245_col6" class="data row245 col6" >0.297332</td>
      <td id="T_c6ca4_row245_col7" class="data row245 col7" >0.954788</td>
      <td id="T_c6ca4_row245_col8" class="data row245 col8" >0.420532</td>
      <td id="T_c6ca4_row245_col9" class="data row245 col9" >4194304.000000</td>
      <td id="T_c6ca4_row245_col10" class="data row245 col10" >torch.float32</td>
      <td id="T_c6ca4_row245_col11" class="data row245 col11" >(4096, 1024)</td>
      <td id="T_c6ca4_row245_col12" class="data row245 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row246" class="row_heading level0 row246" >246</th>
      <td id="T_c6ca4_row246_col0" class="data row246 col0" >encoder.layers.22.fc2.bias</td>
      <td id="T_c6ca4_row246_col1" class="data row246 col1" >encoder.layers.22.fc2.bias</td>
      <td id="T_c6ca4_row246_col2" class="data row246 col2" >av_romanizer.w2v_model.encoder.layers.22.fc2.bias</td>
      <td id="T_c6ca4_row246_col3" class="data row246 col3" >0.322693</td>
      <td id="T_c6ca4_row246_col4" class="data row246 col4" >4.75479</td>
      <td id="T_c6ca4_row246_col5" class="data row246 col5" >4.49962</td>
      <td id="T_c6ca4_row246_col6" class="data row246 col6" >0.0678668</td>
      <td id="T_c6ca4_row246_col7" class="data row246 col7" >0.999088</td>
      <td id="T_c6ca4_row246_col8" class="data row246 col8" >0.0344238</td>
      <td id="T_c6ca4_row246_col9" class="data row246 col9" >1024.000000</td>
      <td id="T_c6ca4_row246_col10" class="data row246 col10" >torch.float32</td>
      <td id="T_c6ca4_row246_col11" class="data row246 col11" >(1024,)</td>
      <td id="T_c6ca4_row246_col12" class="data row246 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row247" class="row_heading level0 row247" >247</th>
      <td id="T_c6ca4_row247_col0" class="data row247 col0" >encoder.layers.22.fc2.weight</td>
      <td id="T_c6ca4_row247_col1" class="data row247 col1" >encoder.layers.22.fc2.weight</td>
      <td id="T_c6ca4_row247_col2" class="data row247 col2" >av_romanizer.w2v_model.encoder.layers.22.fc2.weight</td>
      <td id="T_c6ca4_row247_col3" class="data row247 col3" >92.2318</td>
      <td id="T_c6ca4_row247_col4" class="data row247 col4" >357.015</td>
      <td id="T_c6ca4_row247_col5" class="data row247 col5" >341.808</td>
      <td id="T_c6ca4_row247_col6" class="data row247 col6" >0.258342</td>
      <td id="T_c6ca4_row247_col7" class="data row247 col7" >0.966093</td>
      <td id="T_c6ca4_row247_col8" class="data row247 col8" >0.367676</td>
      <td id="T_c6ca4_row247_col9" class="data row247 col9" >4194304.000000</td>
      <td id="T_c6ca4_row247_col10" class="data row247 col10" >torch.float32</td>
      <td id="T_c6ca4_row247_col11" class="data row247 col11" >(1024, 4096)</td>
      <td id="T_c6ca4_row247_col12" class="data row247 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row248" class="row_heading level0 row248" >248</th>
      <td id="T_c6ca4_row248_col0" class="data row248 col0" >encoder.layers.22.final_layer_norm.bias</td>
      <td id="T_c6ca4_row248_col1" class="data row248 col1" >encoder.layers.22.final_layer_norm.bias</td>
      <td id="T_c6ca4_row248_col2" class="data row248 col2" >av_romanizer.w2v_model.encoder.layers.22.final_layer_norm.bias</td>
      <td id="T_c6ca4_row248_col3" class="data row248 col3" >0.789614</td>
      <td id="T_c6ca4_row248_col4" class="data row248 col4" >14.8564</td>
      <td id="T_c6ca4_row248_col5" class="data row248 col5" >14.9006</td>
      <td id="T_c6ca4_row248_col6" class="data row248 col6" >0.0531498</td>
      <td id="T_c6ca4_row248_col7" class="data row248 col7" >0.998596</td>
      <td id="T_c6ca4_row248_col8" class="data row248 col8" >0.136719</td>
      <td id="T_c6ca4_row248_col9" class="data row248 col9" >1024.000000</td>
      <td id="T_c6ca4_row248_col10" class="data row248 col10" >torch.float32</td>
      <td id="T_c6ca4_row248_col11" class="data row248 col11" >(1024,)</td>
      <td id="T_c6ca4_row248_col12" class="data row248 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row249" class="row_heading level0 row249" >249</th>
      <td id="T_c6ca4_row249_col0" class="data row249 col0" >encoder.layers.22.final_layer_norm.weight</td>
      <td id="T_c6ca4_row249_col1" class="data row249 col1" >encoder.layers.22.final_layer_norm.weight</td>
      <td id="T_c6ca4_row249_col2" class="data row249 col2" >av_romanizer.w2v_model.encoder.layers.22.final_layer_norm.weight</td>
      <td id="T_c6ca4_row249_col3" class="data row249 col3" >2.57572</td>
      <td id="T_c6ca4_row249_col4" class="data row249 col4" >80.2889</td>
      <td id="T_c6ca4_row249_col5" class="data row249 col5" >78.4007</td>
      <td id="T_c6ca4_row249_col6" class="data row249 col6" >0.0320806</td>
      <td id="T_c6ca4_row249_col7" class="data row249 col7" >0.999756</td>
      <td id="T_c6ca4_row249_col8" class="data row249 col8" >0.355469</td>
      <td id="T_c6ca4_row249_col9" class="data row249 col9" >1024.000000</td>
      <td id="T_c6ca4_row249_col10" class="data row249 col10" >torch.float32</td>
      <td id="T_c6ca4_row249_col11" class="data row249 col11" >(1024,)</td>
      <td id="T_c6ca4_row249_col12" class="data row249 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row250" class="row_heading level0 row250" >250</th>
      <td id="T_c6ca4_row250_col0" class="data row250 col0" >encoder.layers.22.self_attn.k_proj.bias</td>
      <td id="T_c6ca4_row250_col1" class="data row250 col1" >encoder.layers.22.self_attn.k_proj.bias</td>
      <td id="T_c6ca4_row250_col2" class="data row250 col2" >av_romanizer.w2v_model.encoder.layers.22.self_attn.k_proj.bias</td>
      <td id="T_c6ca4_row250_col3" class="data row250 col3" >0.0484643</td>
      <td id="T_c6ca4_row250_col4" class="data row250 col4" >0.246447</td>
      <td id="T_c6ca4_row250_col5" class="data row250 col5" >0.247943</td>
      <td id="T_c6ca4_row250_col6" class="data row250 col6" >0.196652</td>
      <td id="T_c6ca4_row250_col7" class="data row250 col7" >0.980799</td>
      <td id="T_c6ca4_row250_col8" class="data row250 col8" >0.015976</td>
      <td id="T_c6ca4_row250_col9" class="data row250 col9" >1024.000000</td>
      <td id="T_c6ca4_row250_col10" class="data row250 col10" >torch.float32</td>
      <td id="T_c6ca4_row250_col11" class="data row250 col11" >(1024,)</td>
      <td id="T_c6ca4_row250_col12" class="data row250 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row251" class="row_heading level0 row251" >251</th>
      <td id="T_c6ca4_row251_col0" class="data row251 col0" >encoder.layers.22.self_attn.k_proj.weight</td>
      <td id="T_c6ca4_row251_col1" class="data row251 col1" >encoder.layers.22.self_attn.k_proj.weight</td>
      <td id="T_c6ca4_row251_col2" class="data row251 col2" >av_romanizer.w2v_model.encoder.layers.22.self_attn.k_proj.weight</td>
      <td id="T_c6ca4_row251_col3" class="data row251 col3" >36.9468</td>
      <td id="T_c6ca4_row251_col4" class="data row251 col4" >97.2729</td>
      <td id="T_c6ca4_row251_col5" class="data row251 col5" >90.4015</td>
      <td id="T_c6ca4_row251_col6" class="data row251 col6" >0.379826</td>
      <td id="T_c6ca4_row251_col7" class="data row251 col7" >0.925068</td>
      <td id="T_c6ca4_row251_col8" class="data row251 col8" >0.430664</td>
      <td id="T_c6ca4_row251_col9" class="data row251 col9" >1048576.000000</td>
      <td id="T_c6ca4_row251_col10" class="data row251 col10" >torch.float32</td>
      <td id="T_c6ca4_row251_col11" class="data row251 col11" >(1024, 1024)</td>
      <td id="T_c6ca4_row251_col12" class="data row251 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row252" class="row_heading level0 row252" >252</th>
      <td id="T_c6ca4_row252_col0" class="data row252 col0" >encoder.layers.22.self_attn.out_proj.bias</td>
      <td id="T_c6ca4_row252_col1" class="data row252 col1" >encoder.layers.22.self_attn.out_proj.bias</td>
      <td id="T_c6ca4_row252_col2" class="data row252 col2" >av_romanizer.w2v_model.encoder.layers.22.self_attn.out_proj.bias</td>
      <td id="T_c6ca4_row252_col3" class="data row252 col3" >0.39272</td>
      <td id="T_c6ca4_row252_col4" class="data row252 col4" >5.20081</td>
      <td id="T_c6ca4_row252_col5" class="data row252 col5" >5.08227</td>
      <td id="T_c6ca4_row252_col6" class="data row252 col6" >0.0755113</td>
      <td id="T_c6ca4_row252_col7" class="data row252 col7" >0.997348</td>
      <td id="T_c6ca4_row252_col8" class="data row252 col8" >0.125</td>
      <td id="T_c6ca4_row252_col9" class="data row252 col9" >1024.000000</td>
      <td id="T_c6ca4_row252_col10" class="data row252 col10" >torch.float32</td>
      <td id="T_c6ca4_row252_col11" class="data row252 col11" >(1024,)</td>
      <td id="T_c6ca4_row252_col12" class="data row252 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row253" class="row_heading level0 row253" >253</th>
      <td id="T_c6ca4_row253_col0" class="data row253 col0" >encoder.layers.22.self_attn.out_proj.weight</td>
      <td id="T_c6ca4_row253_col1" class="data row253 col1" >encoder.layers.22.self_attn.out_proj.weight</td>
      <td id="T_c6ca4_row253_col2" class="data row253 col2" >av_romanizer.w2v_model.encoder.layers.22.self_attn.out_proj.weight</td>
      <td id="T_c6ca4_row253_col3" class="data row253 col3" >58.2094</td>
      <td id="T_c6ca4_row253_col4" class="data row253 col4" >230.632</td>
      <td id="T_c6ca4_row253_col5" class="data row253 col5" >231.009</td>
      <td id="T_c6ca4_row253_col6" class="data row253 col6" >0.25239</td>
      <td id="T_c6ca4_row253_col7" class="data row253 col7" >0.968203</td>
      <td id="T_c6ca4_row253_col8" class="data row253 col8" >0.431641</td>
      <td id="T_c6ca4_row253_col9" class="data row253 col9" >1048576.000000</td>
      <td id="T_c6ca4_row253_col10" class="data row253 col10" >torch.float32</td>
      <td id="T_c6ca4_row253_col11" class="data row253 col11" >(1024, 1024)</td>
      <td id="T_c6ca4_row253_col12" class="data row253 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row254" class="row_heading level0 row254" >254</th>
      <td id="T_c6ca4_row254_col0" class="data row254 col0" >encoder.layers.22.self_attn.q_proj.bias</td>
      <td id="T_c6ca4_row254_col1" class="data row254 col1" >encoder.layers.22.self_attn.q_proj.bias</td>
      <td id="T_c6ca4_row254_col2" class="data row254 col2" >av_romanizer.w2v_model.encoder.layers.22.self_attn.q_proj.bias</td>
      <td id="T_c6ca4_row254_col3" class="data row254 col3" >0.747937</td>
      <td id="T_c6ca4_row254_col4" class="data row254 col4" >5.52696</td>
      <td id="T_c6ca4_row254_col5" class="data row254 col5" >5.26321</td>
      <td id="T_c6ca4_row254_col6" class="data row254 col6" >0.135325</td>
      <td id="T_c6ca4_row254_col7" class="data row254 col7" >0.99158</td>
      <td id="T_c6ca4_row254_col8" class="data row254 col8" >0.128906</td>
      <td id="T_c6ca4_row254_col9" class="data row254 col9" >1024.000000</td>
      <td id="T_c6ca4_row254_col10" class="data row254 col10" >torch.float32</td>
      <td id="T_c6ca4_row254_col11" class="data row254 col11" >(1024,)</td>
      <td id="T_c6ca4_row254_col12" class="data row254 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row255" class="row_heading level0 row255" >255</th>
      <td id="T_c6ca4_row255_col0" class="data row255 col0" >encoder.layers.22.self_attn.q_proj.weight</td>
      <td id="T_c6ca4_row255_col1" class="data row255 col1" >encoder.layers.22.self_attn.q_proj.weight</td>
      <td id="T_c6ca4_row255_col2" class="data row255 col2" >av_romanizer.w2v_model.encoder.layers.22.self_attn.q_proj.weight</td>
      <td id="T_c6ca4_row255_col3" class="data row255 col3" >36.1217</td>
      <td id="T_c6ca4_row255_col4" class="data row255 col4" >95.1196</td>
      <td id="T_c6ca4_row255_col5" class="data row255 col5" >88.3317</td>
      <td id="T_c6ca4_row255_col6" class="data row255 col6" >0.379751</td>
      <td id="T_c6ca4_row255_col7" class="data row255 col7" >0.925096</td>
      <td id="T_c6ca4_row255_col8" class="data row255 col8" >0.280762</td>
      <td id="T_c6ca4_row255_col9" class="data row255 col9" >1048576.000000</td>
      <td id="T_c6ca4_row255_col10" class="data row255 col10" >torch.float32</td>
      <td id="T_c6ca4_row255_col11" class="data row255 col11" >(1024, 1024)</td>
      <td id="T_c6ca4_row255_col12" class="data row255 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row256" class="row_heading level0 row256" >256</th>
      <td id="T_c6ca4_row256_col0" class="data row256 col0" >encoder.layers.22.self_attn.v_proj.bias</td>
      <td id="T_c6ca4_row256_col1" class="data row256 col1" >encoder.layers.22.self_attn.v_proj.bias</td>
      <td id="T_c6ca4_row256_col2" class="data row256 col2" >av_romanizer.w2v_model.encoder.layers.22.self_attn.v_proj.bias</td>
      <td id="T_c6ca4_row256_col3" class="data row256 col3" >0.593597</td>
      <td id="T_c6ca4_row256_col4" class="data row256 col4" >6.84901</td>
      <td id="T_c6ca4_row256_col5" class="data row256 col5" >6.59901</td>
      <td id="T_c6ca4_row256_col6" class="data row256 col6" >0.0866691</td>
      <td id="T_c6ca4_row256_col7" class="data row256 col7" >0.996793</td>
      <td id="T_c6ca4_row256_col8" class="data row256 col8" >0.0749512</td>
      <td id="T_c6ca4_row256_col9" class="data row256 col9" >1024.000000</td>
      <td id="T_c6ca4_row256_col10" class="data row256 col10" >torch.float32</td>
      <td id="T_c6ca4_row256_col11" class="data row256 col11" >(1024,)</td>
      <td id="T_c6ca4_row256_col12" class="data row256 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row257" class="row_heading level0 row257" >257</th>
      <td id="T_c6ca4_row257_col0" class="data row257 col0" >encoder.layers.22.self_attn.v_proj.weight</td>
      <td id="T_c6ca4_row257_col1" class="data row257 col1" >encoder.layers.22.self_attn.v_proj.weight</td>
      <td id="T_c6ca4_row257_col2" class="data row257 col2" >av_romanizer.w2v_model.encoder.layers.22.self_attn.v_proj.weight</td>
      <td id="T_c6ca4_row257_col3" class="data row257 col3" >50.9729</td>
      <td id="T_c6ca4_row257_col4" class="data row257 col4" >219.681</td>
      <td id="T_c6ca4_row257_col5" class="data row257 col5" >218.045</td>
      <td id="T_c6ca4_row257_col6" class="data row257 col6" >0.232031</td>
      <td id="T_c6ca4_row257_col7" class="data row257 col7" >0.972907</td>
      <td id="T_c6ca4_row257_col8" class="data row257 col8" >0.308472</td>
      <td id="T_c6ca4_row257_col9" class="data row257 col9" >1048576.000000</td>
      <td id="T_c6ca4_row257_col10" class="data row257 col10" >torch.float32</td>
      <td id="T_c6ca4_row257_col11" class="data row257 col11" >(1024, 1024)</td>
      <td id="T_c6ca4_row257_col12" class="data row257 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row258" class="row_heading level0 row258" >258</th>
      <td id="T_c6ca4_row258_col0" class="data row258 col0" >encoder.layers.22.self_attn_layer_norm.bias</td>
      <td id="T_c6ca4_row258_col1" class="data row258 col1" >encoder.layers.22.self_attn_layer_norm.bias</td>
      <td id="T_c6ca4_row258_col2" class="data row258 col2" >av_romanizer.w2v_model.encoder.layers.22.self_attn_layer_norm.bias</td>
      <td id="T_c6ca4_row258_col3" class="data row258 col3" >0.435331</td>
      <td id="T_c6ca4_row258_col4" class="data row258 col4" >2.72278</td>
      <td id="T_c6ca4_row258_col5" class="data row258 col5" >2.77176</td>
      <td id="T_c6ca4_row258_col6" class="data row258 col6" >0.159885</td>
      <td id="T_c6ca4_row258_col7" class="data row258 col7" >0.987603</td>
      <td id="T_c6ca4_row258_col8" class="data row258 col8" >0.0437012</td>
      <td id="T_c6ca4_row258_col9" class="data row258 col9" >1024.000000</td>
      <td id="T_c6ca4_row258_col10" class="data row258 col10" >torch.float32</td>
      <td id="T_c6ca4_row258_col11" class="data row258 col11" >(1024,)</td>
      <td id="T_c6ca4_row258_col12" class="data row258 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row259" class="row_heading level0 row259" >259</th>
      <td id="T_c6ca4_row259_col0" class="data row259 col0" >encoder.layers.22.self_attn_layer_norm.weight</td>
      <td id="T_c6ca4_row259_col1" class="data row259 col1" >encoder.layers.22.self_attn_layer_norm.weight</td>
      <td id="T_c6ca4_row259_col2" class="data row259 col2" >av_romanizer.w2v_model.encoder.layers.22.self_attn_layer_norm.weight</td>
      <td id="T_c6ca4_row259_col3" class="data row259 col3" >3.15674</td>
      <td id="T_c6ca4_row259_col4" class="data row259 col4" >12.9103</td>
      <td id="T_c6ca4_row259_col5" class="data row259 col5" >15.8127</td>
      <td id="T_c6ca4_row259_col6" class="data row259 col6" >0.244513</td>
      <td id="T_c6ca4_row259_col7" class="data row259 col7" >0.996226</td>
      <td id="T_c6ca4_row259_col8" class="data row259 col8" >0.216309</td>
      <td id="T_c6ca4_row259_col9" class="data row259 col9" >1024.000000</td>
      <td id="T_c6ca4_row259_col10" class="data row259 col10" >torch.float32</td>
      <td id="T_c6ca4_row259_col11" class="data row259 col11" >(1024,)</td>
      <td id="T_c6ca4_row259_col12" class="data row259 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row260" class="row_heading level0 row260" >260</th>
      <td id="T_c6ca4_row260_col0" class="data row260 col0" >encoder.layers.23.fc1.bias</td>
      <td id="T_c6ca4_row260_col1" class="data row260 col1" >encoder.layers.23.fc1.bias</td>
      <td id="T_c6ca4_row260_col2" class="data row260 col2" >av_romanizer.w2v_model.encoder.layers.23.fc1.bias</td>
      <td id="T_c6ca4_row260_col3" class="data row260 col3" >1.39457</td>
      <td id="T_c6ca4_row260_col4" class="data row260 col4" >8.8982</td>
      <td id="T_c6ca4_row260_col5" class="data row260 col5" >8.37476</td>
      <td id="T_c6ca4_row260_col6" class="data row260 col6" >0.156725</td>
      <td id="T_c6ca4_row260_col7" class="data row260 col7" >0.988789</td>
      <td id="T_c6ca4_row260_col8" class="data row260 col8" >0.100452</td>
      <td id="T_c6ca4_row260_col9" class="data row260 col9" >4096.000000</td>
      <td id="T_c6ca4_row260_col10" class="data row260 col10" >torch.float32</td>
      <td id="T_c6ca4_row260_col11" class="data row260 col11" >(4096,)</td>
      <td id="T_c6ca4_row260_col12" class="data row260 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row261" class="row_heading level0 row261" >261</th>
      <td id="T_c6ca4_row261_col0" class="data row261 col0" >encoder.layers.23.fc1.weight</td>
      <td id="T_c6ca4_row261_col1" class="data row261 col1" >encoder.layers.23.fc1.weight</td>
      <td id="T_c6ca4_row261_col2" class="data row261 col2" >av_romanizer.w2v_model.encoder.layers.23.fc1.weight</td>
      <td id="T_c6ca4_row261_col3" class="data row261 col3" >99.7938</td>
      <td id="T_c6ca4_row261_col4" class="data row261 col4" >287.71</td>
      <td id="T_c6ca4_row261_col5" class="data row261 col5" >280.059</td>
      <td id="T_c6ca4_row261_col6" class="data row261 col6" >0.346855</td>
      <td id="T_c6ca4_row261_col7" class="data row261 col7" >0.938566</td>
      <td id="T_c6ca4_row261_col8" class="data row261 col8" >0.369629</td>
      <td id="T_c6ca4_row261_col9" class="data row261 col9" >4194304.000000</td>
      <td id="T_c6ca4_row261_col10" class="data row261 col10" >torch.float32</td>
      <td id="T_c6ca4_row261_col11" class="data row261 col11" >(4096, 1024)</td>
      <td id="T_c6ca4_row261_col12" class="data row261 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row262" class="row_heading level0 row262" >262</th>
      <td id="T_c6ca4_row262_col0" class="data row262 col0" >encoder.layers.23.fc2.bias</td>
      <td id="T_c6ca4_row262_col1" class="data row262 col1" >encoder.layers.23.fc2.bias</td>
      <td id="T_c6ca4_row262_col2" class="data row262 col2" >av_romanizer.w2v_model.encoder.layers.23.fc2.bias</td>
      <td id="T_c6ca4_row262_col3" class="data row262 col3" >0.206358</td>
      <td id="T_c6ca4_row262_col4" class="data row262 col4" >2.4778</td>
      <td id="T_c6ca4_row262_col5" class="data row262 col5" >2.36105</td>
      <td id="T_c6ca4_row262_col6" class="data row262 col6" >0.0832827</td>
      <td id="T_c6ca4_row262_col7" class="data row262 col7" >0.997526</td>
      <td id="T_c6ca4_row262_col8" class="data row262 col8" >0.0355835</td>
      <td id="T_c6ca4_row262_col9" class="data row262 col9" >1024.000000</td>
      <td id="T_c6ca4_row262_col10" class="data row262 col10" >torch.float32</td>
      <td id="T_c6ca4_row262_col11" class="data row262 col11" >(1024,)</td>
      <td id="T_c6ca4_row262_col12" class="data row262 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row263" class="row_heading level0 row263" >263</th>
      <td id="T_c6ca4_row263_col0" class="data row263 col0" >encoder.layers.23.fc2.weight</td>
      <td id="T_c6ca4_row263_col1" class="data row263 col1" >encoder.layers.23.fc2.weight</td>
      <td id="T_c6ca4_row263_col2" class="data row263 col2" >av_romanizer.w2v_model.encoder.layers.23.fc2.weight</td>
      <td id="T_c6ca4_row263_col3" class="data row263 col3" >80.6991</td>
      <td id="T_c6ca4_row263_col4" class="data row263 col4" >315.254</td>
      <td id="T_c6ca4_row263_col5" class="data row263 col5" >306.354</td>
      <td id="T_c6ca4_row263_col6" class="data row263 col6" >0.255982</td>
      <td id="T_c6ca4_row263_col7" class="data row263 col7" >0.966695</td>
      <td id="T_c6ca4_row263_col8" class="data row263 col8" >0.371445</td>
      <td id="T_c6ca4_row263_col9" class="data row263 col9" >4194304.000000</td>
      <td id="T_c6ca4_row263_col10" class="data row263 col10" >torch.float32</td>
      <td id="T_c6ca4_row263_col11" class="data row263 col11" >(1024, 4096)</td>
      <td id="T_c6ca4_row263_col12" class="data row263 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row264" class="row_heading level0 row264" >264</th>
      <td id="T_c6ca4_row264_col0" class="data row264 col0" >encoder.layers.23.final_layer_norm.bias</td>
      <td id="T_c6ca4_row264_col1" class="data row264 col1" >encoder.layers.23.final_layer_norm.bias</td>
      <td id="T_c6ca4_row264_col2" class="data row264 col2" >av_romanizer.w2v_model.encoder.layers.23.final_layer_norm.bias</td>
      <td id="T_c6ca4_row264_col3" class="data row264 col3" >1.50271</td>
      <td id="T_c6ca4_row264_col4" class="data row264 col4" >11.4909</td>
      <td id="T_c6ca4_row264_col5" class="data row264 col5" >10.4804</td>
      <td id="T_c6ca4_row264_col6" class="data row264 col6" >0.130774</td>
      <td id="T_c6ca4_row264_col7" class="data row264 col7" >0.994863</td>
      <td id="T_c6ca4_row264_col8" class="data row264 col8" >0.330078</td>
      <td id="T_c6ca4_row264_col9" class="data row264 col9" >1024.000000</td>
      <td id="T_c6ca4_row264_col10" class="data row264 col10" >torch.float32</td>
      <td id="T_c6ca4_row264_col11" class="data row264 col11" >(1024,)</td>
      <td id="T_c6ca4_row264_col12" class="data row264 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row265" class="row_heading level0 row265" >265</th>
      <td id="T_c6ca4_row265_col0" class="data row265 col0" >encoder.layers.23.final_layer_norm.weight</td>
      <td id="T_c6ca4_row265_col1" class="data row265 col1" >encoder.layers.23.final_layer_norm.weight</td>
      <td id="T_c6ca4_row265_col2" class="data row265 col2" >av_romanizer.w2v_model.encoder.layers.23.final_layer_norm.weight</td>
      <td id="T_c6ca4_row265_col3" class="data row265 col3" >2.15038</td>
      <td id="T_c6ca4_row265_col4" class="data row265 col4" >62.3686</td>
      <td id="T_c6ca4_row265_col5" class="data row265 col5" >63.3586</td>
      <td id="T_c6ca4_row265_col6" class="data row265 col6" >0.0344786</td>
      <td id="T_c6ca4_row265_col7" class="data row265 col7" >0.999539</td>
      <td id="T_c6ca4_row265_col8" class="data row265 col8" >0.28894</td>
      <td id="T_c6ca4_row265_col9" class="data row265 col9" >1024.000000</td>
      <td id="T_c6ca4_row265_col10" class="data row265 col10" >torch.float32</td>
      <td id="T_c6ca4_row265_col11" class="data row265 col11" >(1024,)</td>
      <td id="T_c6ca4_row265_col12" class="data row265 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row266" class="row_heading level0 row266" >266</th>
      <td id="T_c6ca4_row266_col0" class="data row266 col0" >encoder.layers.23.self_attn.k_proj.bias</td>
      <td id="T_c6ca4_row266_col1" class="data row266 col1" >encoder.layers.23.self_attn.k_proj.bias</td>
      <td id="T_c6ca4_row266_col2" class="data row266 col2" >av_romanizer.w2v_model.encoder.layers.23.self_attn.k_proj.bias</td>
      <td id="T_c6ca4_row266_col3" class="data row266 col3" >0.18684</td>
      <td id="T_c6ca4_row266_col4" class="data row266 col4" >0.557519</td>
      <td id="T_c6ca4_row266_col5" class="data row266 col5" >0.597318</td>
      <td id="T_c6ca4_row266_col6" class="data row266 col6" >0.335128</td>
      <td id="T_c6ca4_row266_col7" class="data row266 col7" >0.949965</td>
      <td id="T_c6ca4_row266_col8" class="data row266 col8" >0.0678711</td>
      <td id="T_c6ca4_row266_col9" class="data row266 col9" >1024.000000</td>
      <td id="T_c6ca4_row266_col10" class="data row266 col10" >torch.float32</td>
      <td id="T_c6ca4_row266_col11" class="data row266 col11" >(1024,)</td>
      <td id="T_c6ca4_row266_col12" class="data row266 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row267" class="row_heading level0 row267" >267</th>
      <td id="T_c6ca4_row267_col0" class="data row267 col0" >encoder.layers.23.self_attn.k_proj.weight</td>
      <td id="T_c6ca4_row267_col1" class="data row267 col1" >encoder.layers.23.self_attn.k_proj.weight</td>
      <td id="T_c6ca4_row267_col2" class="data row267 col2" >av_romanizer.w2v_model.encoder.layers.23.self_attn.k_proj.weight</td>
      <td id="T_c6ca4_row267_col3" class="data row267 col3" >37.576</td>
      <td id="T_c6ca4_row267_col4" class="data row267 col4" >87.1314</td>
      <td id="T_c6ca4_row267_col5" class="data row267 col5" >82.7907</td>
      <td id="T_c6ca4_row267_col6" class="data row267 col6" >0.431256</td>
      <td id="T_c6ca4_row267_col7" class="data row267 col7" >0.903439</td>
      <td id="T_c6ca4_row267_col8" class="data row267 col8" >0.256836</td>
      <td id="T_c6ca4_row267_col9" class="data row267 col9" >1048576.000000</td>
      <td id="T_c6ca4_row267_col10" class="data row267 col10" >torch.float32</td>
      <td id="T_c6ca4_row267_col11" class="data row267 col11" >(1024, 1024)</td>
      <td id="T_c6ca4_row267_col12" class="data row267 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row268" class="row_heading level0 row268" >268</th>
      <td id="T_c6ca4_row268_col0" class="data row268 col0" >encoder.layers.23.self_attn.out_proj.bias</td>
      <td id="T_c6ca4_row268_col1" class="data row268 col1" >encoder.layers.23.self_attn.out_proj.bias</td>
      <td id="T_c6ca4_row268_col2" class="data row268 col2" >av_romanizer.w2v_model.encoder.layers.23.self_attn.out_proj.bias</td>
      <td id="T_c6ca4_row268_col3" class="data row268 col3" >0.389175</td>
      <td id="T_c6ca4_row268_col4" class="data row268 col4" >4.61679</td>
      <td id="T_c6ca4_row268_col5" class="data row268 col5" >4.45805</td>
      <td id="T_c6ca4_row268_col6" class="data row268 col6" >0.0842958</td>
      <td id="T_c6ca4_row268_col7" class="data row268 col7" >0.996933</td>
      <td id="T_c6ca4_row268_col8" class="data row268 col8" >0.112793</td>
      <td id="T_c6ca4_row268_col9" class="data row268 col9" >1024.000000</td>
      <td id="T_c6ca4_row268_col10" class="data row268 col10" >torch.float32</td>
      <td id="T_c6ca4_row268_col11" class="data row268 col11" >(1024,)</td>
      <td id="T_c6ca4_row268_col12" class="data row268 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row269" class="row_heading level0 row269" >269</th>
      <td id="T_c6ca4_row269_col0" class="data row269 col0" >encoder.layers.23.self_attn.out_proj.weight</td>
      <td id="T_c6ca4_row269_col1" class="data row269 col1" >encoder.layers.23.self_attn.out_proj.weight</td>
      <td id="T_c6ca4_row269_col2" class="data row269 col2" >av_romanizer.w2v_model.encoder.layers.23.self_attn.out_proj.weight</td>
      <td id="T_c6ca4_row269_col3" class="data row269 col3" >51.2779</td>
      <td id="T_c6ca4_row269_col4" class="data row269 col4" >241.011</td>
      <td id="T_c6ca4_row269_col5" class="data row269 col5" >235.504</td>
      <td id="T_c6ca4_row269_col6" class="data row269 col6" >0.212762</td>
      <td id="T_c6ca4_row269_col7" class="data row269 col7" >0.977104</td>
      <td id="T_c6ca4_row269_col8" class="data row269 col8" >0.488281</td>
      <td id="T_c6ca4_row269_col9" class="data row269 col9" >1048576.000000</td>
      <td id="T_c6ca4_row269_col10" class="data row269 col10" >torch.float32</td>
      <td id="T_c6ca4_row269_col11" class="data row269 col11" >(1024, 1024)</td>
      <td id="T_c6ca4_row269_col12" class="data row269 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row270" class="row_heading level0 row270" >270</th>
      <td id="T_c6ca4_row270_col0" class="data row270 col0" >encoder.layers.23.self_attn.q_proj.bias</td>
      <td id="T_c6ca4_row270_col1" class="data row270 col1" >encoder.layers.23.self_attn.q_proj.bias</td>
      <td id="T_c6ca4_row270_col2" class="data row270 col2" >av_romanizer.w2v_model.encoder.layers.23.self_attn.q_proj.bias</td>
      <td id="T_c6ca4_row270_col3" class="data row270 col3" >0.651156</td>
      <td id="T_c6ca4_row270_col4" class="data row270 col4" >3.00463</td>
      <td id="T_c6ca4_row270_col5" class="data row270 col5" >3.15945</td>
      <td id="T_c6ca4_row270_col6" class="data row270 col6" >0.216718</td>
      <td id="T_c6ca4_row270_col7" class="data row270 col7" >0.97893</td>
      <td id="T_c6ca4_row270_col8" class="data row270 col8" >0.085083</td>
      <td id="T_c6ca4_row270_col9" class="data row270 col9" >1024.000000</td>
      <td id="T_c6ca4_row270_col10" class="data row270 col10" >torch.float32</td>
      <td id="T_c6ca4_row270_col11" class="data row270 col11" >(1024,)</td>
      <td id="T_c6ca4_row270_col12" class="data row270 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row271" class="row_heading level0 row271" >271</th>
      <td id="T_c6ca4_row271_col0" class="data row271 col0" >encoder.layers.23.self_attn.q_proj.weight</td>
      <td id="T_c6ca4_row271_col1" class="data row271 col1" >encoder.layers.23.self_attn.q_proj.weight</td>
      <td id="T_c6ca4_row271_col2" class="data row271 col2" >av_romanizer.w2v_model.encoder.layers.23.self_attn.q_proj.weight</td>
      <td id="T_c6ca4_row271_col3" class="data row271 col3" >36.4834</td>
      <td id="T_c6ca4_row271_col4" class="data row271 col4" >86.7775</td>
      <td id="T_c6ca4_row271_col5" class="data row271 col5" >82.3122</td>
      <td id="T_c6ca4_row271_col6" class="data row271 col6" >0.420425</td>
      <td id="T_c6ca4_row271_col7" class="data row271 col7" >0.908223</td>
      <td id="T_c6ca4_row271_col8" class="data row271 col8" >0.206425</td>
      <td id="T_c6ca4_row271_col9" class="data row271 col9" >1048576.000000</td>
      <td id="T_c6ca4_row271_col10" class="data row271 col10" >torch.float32</td>
      <td id="T_c6ca4_row271_col11" class="data row271 col11" >(1024, 1024)</td>
      <td id="T_c6ca4_row271_col12" class="data row271 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row272" class="row_heading level0 row272" >272</th>
      <td id="T_c6ca4_row272_col0" class="data row272 col0" >encoder.layers.23.self_attn.v_proj.bias</td>
      <td id="T_c6ca4_row272_col1" class="data row272 col1" >encoder.layers.23.self_attn.v_proj.bias</td>
      <td id="T_c6ca4_row272_col2" class="data row272 col2" >av_romanizer.w2v_model.encoder.layers.23.self_attn.v_proj.bias</td>
      <td id="T_c6ca4_row272_col3" class="data row272 col3" >0.790402</td>
      <td id="T_c6ca4_row272_col4" class="data row272 col4" >5.46876</td>
      <td id="T_c6ca4_row272_col5" class="data row272 col5" >5.08903</td>
      <td id="T_c6ca4_row272_col6" class="data row272 col6" >0.144531</td>
      <td id="T_c6ca4_row272_col7" class="data row272 col7" >0.991367</td>
      <td id="T_c6ca4_row272_col8" class="data row272 col8" >0.0878906</td>
      <td id="T_c6ca4_row272_col9" class="data row272 col9" >1024.000000</td>
      <td id="T_c6ca4_row272_col10" class="data row272 col10" >torch.float32</td>
      <td id="T_c6ca4_row272_col11" class="data row272 col11" >(1024,)</td>
      <td id="T_c6ca4_row272_col12" class="data row272 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row273" class="row_heading level0 row273" >273</th>
      <td id="T_c6ca4_row273_col0" class="data row273 col0" >encoder.layers.23.self_attn.v_proj.weight</td>
      <td id="T_c6ca4_row273_col1" class="data row273 col1" >encoder.layers.23.self_attn.v_proj.weight</td>
      <td id="T_c6ca4_row273_col2" class="data row273 col2" >av_romanizer.w2v_model.encoder.layers.23.self_attn.v_proj.weight</td>
      <td id="T_c6ca4_row273_col3" class="data row273 col3" >49.5108</td>
      <td id="T_c6ca4_row273_col4" class="data row273 col4" >228.914</td>
      <td id="T_c6ca4_row273_col5" class="data row273 col5" >223.106</td>
      <td id="T_c6ca4_row273_col6" class="data row273 col6" >0.216286</td>
      <td id="T_c6ca4_row273_col7" class="data row273 col7" >0.976332</td>
      <td id="T_c6ca4_row273_col8" class="data row273 col8" >0.297852</td>
      <td id="T_c6ca4_row273_col9" class="data row273 col9" >1048576.000000</td>
      <td id="T_c6ca4_row273_col10" class="data row273 col10" >torch.float32</td>
      <td id="T_c6ca4_row273_col11" class="data row273 col11" >(1024, 1024)</td>
      <td id="T_c6ca4_row273_col12" class="data row273 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row274" class="row_heading level0 row274" >274</th>
      <td id="T_c6ca4_row274_col0" class="data row274 col0" >encoder.layers.23.self_attn_layer_norm.bias</td>
      <td id="T_c6ca4_row274_col1" class="data row274 col1" >encoder.layers.23.self_attn_layer_norm.bias</td>
      <td id="T_c6ca4_row274_col2" class="data row274 col2" >av_romanizer.w2v_model.encoder.layers.23.self_attn_layer_norm.bias</td>
      <td id="T_c6ca4_row274_col3" class="data row274 col3" >0.771724</td>
      <td id="T_c6ca4_row274_col4" class="data row274 col4" >2.91652</td>
      <td id="T_c6ca4_row274_col5" class="data row274 col5" >2.60864</td>
      <td id="T_c6ca4_row274_col6" class="data row274 col6" >0.264604</td>
      <td id="T_c6ca4_row274_col7" class="data row274 col7" >0.96709</td>
      <td id="T_c6ca4_row274_col8" class="data row274 col8" >0.091423</td>
      <td id="T_c6ca4_row274_col9" class="data row274 col9" >1024.000000</td>
      <td id="T_c6ca4_row274_col10" class="data row274 col10" >torch.float32</td>
      <td id="T_c6ca4_row274_col11" class="data row274 col11" >(1024,)</td>
      <td id="T_c6ca4_row274_col12" class="data row274 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row275" class="row_heading level0 row275" >275</th>
      <td id="T_c6ca4_row275_col0" class="data row275 col0" >encoder.layers.23.self_attn_layer_norm.weight</td>
      <td id="T_c6ca4_row275_col1" class="data row275 col1" >encoder.layers.23.self_attn_layer_norm.weight</td>
      <td id="T_c6ca4_row275_col2" class="data row275 col2" >av_romanizer.w2v_model.encoder.layers.23.self_attn_layer_norm.weight</td>
      <td id="T_c6ca4_row275_col3" class="data row275 col3" >1.86232</td>
      <td id="T_c6ca4_row275_col4" class="data row275 col4" >13.5828</td>
      <td id="T_c6ca4_row275_col5" class="data row275 col5" >15.1195</td>
      <td id="T_c6ca4_row275_col6" class="data row275 col6" >0.137109</td>
      <td id="T_c6ca4_row275_col7" class="data row275 col7" >0.997305</td>
      <td id="T_c6ca4_row275_col8" class="data row275 col8" >0.271973</td>
      <td id="T_c6ca4_row275_col9" class="data row275 col9" >1024.000000</td>
      <td id="T_c6ca4_row275_col10" class="data row275 col10" >torch.float32</td>
      <td id="T_c6ca4_row275_col11" class="data row275 col11" >(1024,)</td>
      <td id="T_c6ca4_row275_col12" class="data row275 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row276" class="row_heading level0 row276" >276</th>
      <td id="T_c6ca4_row276_col0" class="data row276 col0" >encoder.layers.3.fc1.bias</td>
      <td id="T_c6ca4_row276_col1" class="data row276 col1" >encoder.layers.3.fc1.bias</td>
      <td id="T_c6ca4_row276_col2" class="data row276 col2" >av_romanizer.w2v_model.encoder.layers.3.fc1.bias</td>
      <td id="T_c6ca4_row276_col3" class="data row276 col3" >0.725953</td>
      <td id="T_c6ca4_row276_col4" class="data row276 col4" >2.98948</td>
      <td id="T_c6ca4_row276_col5" class="data row276 col5" >3.24963</td>
      <td id="T_c6ca4_row276_col6" class="data row276 col6" >0.242836</td>
      <td id="T_c6ca4_row276_col7" class="data row276 col7" >0.976359</td>
      <td id="T_c6ca4_row276_col8" class="data row276 col8" >0.0603504</td>
      <td id="T_c6ca4_row276_col9" class="data row276 col9" >4096.000000</td>
      <td id="T_c6ca4_row276_col10" class="data row276 col10" >torch.float32</td>
      <td id="T_c6ca4_row276_col11" class="data row276 col11" >(4096,)</td>
      <td id="T_c6ca4_row276_col12" class="data row276 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row277" class="row_heading level0 row277" >277</th>
      <td id="T_c6ca4_row277_col0" class="data row277 col0" >encoder.layers.3.fc1.weight</td>
      <td id="T_c6ca4_row277_col1" class="data row277 col1" >encoder.layers.3.fc1.weight</td>
      <td id="T_c6ca4_row277_col2" class="data row277 col2" >av_romanizer.w2v_model.encoder.layers.3.fc1.weight</td>
      <td id="T_c6ca4_row277_col3" class="data row277 col3" >41.8252</td>
      <td id="T_c6ca4_row277_col4" class="data row277 col4" >298.248</td>
      <td id="T_c6ca4_row277_col5" class="data row277 col5" >285.259</td>
      <td id="T_c6ca4_row277_col6" class="data row277 col6" >0.140237</td>
      <td id="T_c6ca4_row277_col7" class="data row277 col7" >0.990711</td>
      <td id="T_c6ca4_row277_col8" class="data row277 col8" >0.241096</td>
      <td id="T_c6ca4_row277_col9" class="data row277 col9" >4194304.000000</td>
      <td id="T_c6ca4_row277_col10" class="data row277 col10" >torch.float32</td>
      <td id="T_c6ca4_row277_col11" class="data row277 col11" >(4096, 1024)</td>
      <td id="T_c6ca4_row277_col12" class="data row277 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row278" class="row_heading level0 row278" >278</th>
      <td id="T_c6ca4_row278_col0" class="data row278 col0" >encoder.layers.3.fc2.bias</td>
      <td id="T_c6ca4_row278_col1" class="data row278 col1" >encoder.layers.3.fc2.bias</td>
      <td id="T_c6ca4_row278_col2" class="data row278 col2" >av_romanizer.w2v_model.encoder.layers.3.fc2.bias</td>
      <td id="T_c6ca4_row278_col3" class="data row278 col3" >0.153496</td>
      <td id="T_c6ca4_row278_col4" class="data row278 col4" >2.81206</td>
      <td id="T_c6ca4_row278_col5" class="data row278 col5" >2.69683</td>
      <td id="T_c6ca4_row278_col6" class="data row278 col6" >0.0545849</td>
      <td id="T_c6ca4_row278_col7" class="data row278 col7" >0.999322</td>
      <td id="T_c6ca4_row278_col8" class="data row278 col8" >0.026123</td>
      <td id="T_c6ca4_row278_col9" class="data row278 col9" >1024.000000</td>
      <td id="T_c6ca4_row278_col10" class="data row278 col10" >torch.float32</td>
      <td id="T_c6ca4_row278_col11" class="data row278 col11" >(1024,)</td>
      <td id="T_c6ca4_row278_col12" class="data row278 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row279" class="row_heading level0 row279" >279</th>
      <td id="T_c6ca4_row279_col0" class="data row279 col0" >encoder.layers.3.fc2.weight</td>
      <td id="T_c6ca4_row279_col1" class="data row279 col1" >encoder.layers.3.fc2.weight</td>
      <td id="T_c6ca4_row279_col2" class="data row279 col2" >av_romanizer.w2v_model.encoder.layers.3.fc2.weight</td>
      <td id="T_c6ca4_row279_col3" class="data row279 col3" >36.6506</td>
      <td id="T_c6ca4_row279_col4" class="data row279 col4" >285.252</td>
      <td id="T_c6ca4_row279_col5" class="data row279 col5" >270.523</td>
      <td id="T_c6ca4_row279_col6" class="data row279 col6" >0.128485</td>
      <td id="T_c6ca4_row279_col7" class="data row279 col7" >0.992702</td>
      <td id="T_c6ca4_row279_col8" class="data row279 col8" >0.162598</td>
      <td id="T_c6ca4_row279_col9" class="data row279 col9" >4194304.000000</td>
      <td id="T_c6ca4_row279_col10" class="data row279 col10" >torch.float32</td>
      <td id="T_c6ca4_row279_col11" class="data row279 col11" >(1024, 4096)</td>
      <td id="T_c6ca4_row279_col12" class="data row279 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row280" class="row_heading level0 row280" >280</th>
      <td id="T_c6ca4_row280_col0" class="data row280 col0" >encoder.layers.3.final_layer_norm.bias</td>
      <td id="T_c6ca4_row280_col1" class="data row280 col1" >encoder.layers.3.final_layer_norm.bias</td>
      <td id="T_c6ca4_row280_col2" class="data row280 col2" >av_romanizer.w2v_model.encoder.layers.3.final_layer_norm.bias</td>
      <td id="T_c6ca4_row280_col3" class="data row280 col3" >0.213224</td>
      <td id="T_c6ca4_row280_col4" class="data row280 col4" >2.34874</td>
      <td id="T_c6ca4_row280_col5" class="data row280 col5" >2.2925</td>
      <td id="T_c6ca4_row280_col6" class="data row280 col6" >0.0907825</td>
      <td id="T_c6ca4_row280_col7" class="data row280 col7" >0.996072</td>
      <td id="T_c6ca4_row280_col8" class="data row280 col8" >0.0378418</td>
      <td id="T_c6ca4_row280_col9" class="data row280 col9" >1024.000000</td>
      <td id="T_c6ca4_row280_col10" class="data row280 col10" >torch.float32</td>
      <td id="T_c6ca4_row280_col11" class="data row280 col11" >(1024,)</td>
      <td id="T_c6ca4_row280_col12" class="data row280 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row281" class="row_heading level0 row281" >281</th>
      <td id="T_c6ca4_row281_col0" class="data row281 col0" >encoder.layers.3.final_layer_norm.weight</td>
      <td id="T_c6ca4_row281_col1" class="data row281 col1" >encoder.layers.3.final_layer_norm.weight</td>
      <td id="T_c6ca4_row281_col2" class="data row281 col2" >av_romanizer.w2v_model.encoder.layers.3.final_layer_norm.weight</td>
      <td id="T_c6ca4_row281_col3" class="data row281 col3" >0.358105</td>
      <td id="T_c6ca4_row281_col4" class="data row281 col4" >4.95374</td>
      <td id="T_c6ca4_row281_col5" class="data row281 col5" >4.755</td>
      <td id="T_c6ca4_row281_col6" class="data row281 col6" >0.0722898</td>
      <td id="T_c6ca4_row281_col7" class="data row281 col7" >0.998116</td>
      <td id="T_c6ca4_row281_col8" class="data row281 col8" >0.0397339</td>
      <td id="T_c6ca4_row281_col9" class="data row281 col9" >1024.000000</td>
      <td id="T_c6ca4_row281_col10" class="data row281 col10" >torch.float32</td>
      <td id="T_c6ca4_row281_col11" class="data row281 col11" >(1024,)</td>
      <td id="T_c6ca4_row281_col12" class="data row281 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row282" class="row_heading level0 row282" >282</th>
      <td id="T_c6ca4_row282_col0" class="data row282 col0" >encoder.layers.3.self_attn.k_proj.bias</td>
      <td id="T_c6ca4_row282_col1" class="data row282 col1" >encoder.layers.3.self_attn.k_proj.bias</td>
      <td id="T_c6ca4_row282_col2" class="data row282 col2" >av_romanizer.w2v_model.encoder.layers.3.self_attn.k_proj.bias</td>
      <td id="T_c6ca4_row282_col3" class="data row282 col3" >0.0100166</td>
      <td id="T_c6ca4_row282_col4" class="data row282 col4" >0.106109</td>
      <td id="T_c6ca4_row282_col5" class="data row282 col5" >0.100681</td>
      <td id="T_c6ca4_row282_col6" class="data row282 col6" >0.0943988</td>
      <td id="T_c6ca4_row282_col7" class="data row282 col7" >0.996684</td>
      <td id="T_c6ca4_row282_col8" class="data row282 col8" >0.00175714</td>
      <td id="T_c6ca4_row282_col9" class="data row282 col9" >1024.000000</td>
      <td id="T_c6ca4_row282_col10" class="data row282 col10" >torch.float32</td>
      <td id="T_c6ca4_row282_col11" class="data row282 col11" >(1024,)</td>
      <td id="T_c6ca4_row282_col12" class="data row282 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row283" class="row_heading level0 row283" >283</th>
      <td id="T_c6ca4_row283_col0" class="data row283 col0" >encoder.layers.3.self_attn.k_proj.weight</td>
      <td id="T_c6ca4_row283_col1" class="data row283 col1" >encoder.layers.3.self_attn.k_proj.weight</td>
      <td id="T_c6ca4_row283_col2" class="data row283 col2" >av_romanizer.w2v_model.encoder.layers.3.self_attn.k_proj.weight</td>
      <td id="T_c6ca4_row283_col3" class="data row283 col3" >20.2772</td>
      <td id="T_c6ca4_row283_col4" class="data row283 col4" >150.175</td>
      <td id="T_c6ca4_row283_col5" class="data row283 col5" >145.442</td>
      <td id="T_c6ca4_row283_col6" class="data row283 col6" >0.135024</td>
      <td id="T_c6ca4_row283_col7" class="data row283 col7" >0.9911</td>
      <td id="T_c6ca4_row283_col8" class="data row283 col8" >0.239746</td>
      <td id="T_c6ca4_row283_col9" class="data row283 col9" >1048576.000000</td>
      <td id="T_c6ca4_row283_col10" class="data row283 col10" >torch.float32</td>
      <td id="T_c6ca4_row283_col11" class="data row283 col11" >(1024, 1024)</td>
      <td id="T_c6ca4_row283_col12" class="data row283 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row284" class="row_heading level0 row284" >284</th>
      <td id="T_c6ca4_row284_col0" class="data row284 col0" >encoder.layers.3.self_attn.out_proj.bias</td>
      <td id="T_c6ca4_row284_col1" class="data row284 col1" >encoder.layers.3.self_attn.out_proj.bias</td>
      <td id="T_c6ca4_row284_col2" class="data row284 col2" >av_romanizer.w2v_model.encoder.layers.3.self_attn.out_proj.bias</td>
      <td id="T_c6ca4_row284_col3" class="data row284 col3" >0.293017</td>
      <td id="T_c6ca4_row284_col4" class="data row284 col4" >3.45807</td>
      <td id="T_c6ca4_row284_col5" class="data row284 col5" >3.26656</td>
      <td id="T_c6ca4_row284_col6" class="data row284 col6" >0.0847343</td>
      <td id="T_c6ca4_row284_col7" class="data row284 col7" >0.997823</td>
      <td id="T_c6ca4_row284_col8" class="data row284 col8" >0.0908203</td>
      <td id="T_c6ca4_row284_col9" class="data row284 col9" >1024.000000</td>
      <td id="T_c6ca4_row284_col10" class="data row284 col10" >torch.float32</td>
      <td id="T_c6ca4_row284_col11" class="data row284 col11" >(1024,)</td>
      <td id="T_c6ca4_row284_col12" class="data row284 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row285" class="row_heading level0 row285" >285</th>
      <td id="T_c6ca4_row285_col0" class="data row285 col0" >encoder.layers.3.self_attn.out_proj.weight</td>
      <td id="T_c6ca4_row285_col1" class="data row285 col1" >encoder.layers.3.self_attn.out_proj.weight</td>
      <td id="T_c6ca4_row285_col2" class="data row285 col2" >av_romanizer.w2v_model.encoder.layers.3.self_attn.out_proj.weight</td>
      <td id="T_c6ca4_row285_col3" class="data row285 col3" >21.3125</td>
      <td id="T_c6ca4_row285_col4" class="data row285 col4" >124.735</td>
      <td id="T_c6ca4_row285_col5" class="data row285 col5" >118.113</td>
      <td id="T_c6ca4_row285_col6" class="data row285 col6" >0.170862</td>
      <td id="T_c6ca4_row285_col7" class="data row285 col7" >0.986073</td>
      <td id="T_c6ca4_row285_col8" class="data row285 col8" >0.117676</td>
      <td id="T_c6ca4_row285_col9" class="data row285 col9" >1048576.000000</td>
      <td id="T_c6ca4_row285_col10" class="data row285 col10" >torch.float32</td>
      <td id="T_c6ca4_row285_col11" class="data row285 col11" >(1024, 1024)</td>
      <td id="T_c6ca4_row285_col12" class="data row285 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row286" class="row_heading level0 row286" >286</th>
      <td id="T_c6ca4_row286_col0" class="data row286 col0" >encoder.layers.3.self_attn.q_proj.bias</td>
      <td id="T_c6ca4_row286_col1" class="data row286 col1" >encoder.layers.3.self_attn.q_proj.bias</td>
      <td id="T_c6ca4_row286_col2" class="data row286 col2" >av_romanizer.w2v_model.encoder.layers.3.self_attn.q_proj.bias</td>
      <td id="T_c6ca4_row286_col3" class="data row286 col3" >1.00769</td>
      <td id="T_c6ca4_row286_col4" class="data row286 col4" >10.5189</td>
      <td id="T_c6ca4_row286_col5" class="data row286 col5" >10.4098</td>
      <td id="T_c6ca4_row286_col6" class="data row286 col6" >0.0957983</td>
      <td id="T_c6ca4_row286_col7" class="data row286 col7" >0.995418</td>
      <td id="T_c6ca4_row286_col8" class="data row286 col8" >0.121216</td>
      <td id="T_c6ca4_row286_col9" class="data row286 col9" >1024.000000</td>
      <td id="T_c6ca4_row286_col10" class="data row286 col10" >torch.float32</td>
      <td id="T_c6ca4_row286_col11" class="data row286 col11" >(1024,)</td>
      <td id="T_c6ca4_row286_col12" class="data row286 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row287" class="row_heading level0 row287" >287</th>
      <td id="T_c6ca4_row287_col0" class="data row287 col0" >encoder.layers.3.self_attn.q_proj.weight</td>
      <td id="T_c6ca4_row287_col1" class="data row287 col1" >encoder.layers.3.self_attn.q_proj.weight</td>
      <td id="T_c6ca4_row287_col2" class="data row287 col2" >av_romanizer.w2v_model.encoder.layers.3.self_attn.q_proj.weight</td>
      <td id="T_c6ca4_row287_col3" class="data row287 col3" >20.4577</td>
      <td id="T_c6ca4_row287_col4" class="data row287 col4" >148.163</td>
      <td id="T_c6ca4_row287_col5" class="data row287 col5" >143.285</td>
      <td id="T_c6ca4_row287_col6" class="data row287 col6" >0.138075</td>
      <td id="T_c6ca4_row287_col7" class="data row287 col7" >0.990704</td>
      <td id="T_c6ca4_row287_col8" class="data row287 col8" >0.222656</td>
      <td id="T_c6ca4_row287_col9" class="data row287 col9" >1048576.000000</td>
      <td id="T_c6ca4_row287_col10" class="data row287 col10" >torch.float32</td>
      <td id="T_c6ca4_row287_col11" class="data row287 col11" >(1024, 1024)</td>
      <td id="T_c6ca4_row287_col12" class="data row287 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row288" class="row_heading level0 row288" >288</th>
      <td id="T_c6ca4_row288_col0" class="data row288 col0" >encoder.layers.3.self_attn.v_proj.bias</td>
      <td id="T_c6ca4_row288_col1" class="data row288 col1" >encoder.layers.3.self_attn.v_proj.bias</td>
      <td id="T_c6ca4_row288_col2" class="data row288 col2" >av_romanizer.w2v_model.encoder.layers.3.self_attn.v_proj.bias</td>
      <td id="T_c6ca4_row288_col3" class="data row288 col3" >0.238568</td>
      <td id="T_c6ca4_row288_col4" class="data row288 col4" >1.11681</td>
      <td id="T_c6ca4_row288_col5" class="data row288 col5" >1.03472</td>
      <td id="T_c6ca4_row288_col6" class="data row288 col6" >0.213615</td>
      <td id="T_c6ca4_row288_col7" class="data row288 col7" >0.97829</td>
      <td id="T_c6ca4_row288_col8" class="data row288 col8" >0.0351715</td>
      <td id="T_c6ca4_row288_col9" class="data row288 col9" >1024.000000</td>
      <td id="T_c6ca4_row288_col10" class="data row288 col10" >torch.float32</td>
      <td id="T_c6ca4_row288_col11" class="data row288 col11" >(1024,)</td>
      <td id="T_c6ca4_row288_col12" class="data row288 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row289" class="row_heading level0 row289" >289</th>
      <td id="T_c6ca4_row289_col0" class="data row289 col0" >encoder.layers.3.self_attn.v_proj.weight</td>
      <td id="T_c6ca4_row289_col1" class="data row289 col1" >encoder.layers.3.self_attn.v_proj.weight</td>
      <td id="T_c6ca4_row289_col2" class="data row289 col2" >av_romanizer.w2v_model.encoder.layers.3.self_attn.v_proj.weight</td>
      <td id="T_c6ca4_row289_col3" class="data row289 col3" >18.7372</td>
      <td id="T_c6ca4_row289_col4" class="data row289 col4" >122.844</td>
      <td id="T_c6ca4_row289_col5" class="data row289 col5" >116.433</td>
      <td id="T_c6ca4_row289_col6" class="data row289 col6" >0.152528</td>
      <td id="T_c6ca4_row289_col7" class="data row289 col7" >0.989164</td>
      <td id="T_c6ca4_row289_col8" class="data row289 col8" >0.104004</td>
      <td id="T_c6ca4_row289_col9" class="data row289 col9" >1048576.000000</td>
      <td id="T_c6ca4_row289_col10" class="data row289 col10" >torch.float32</td>
      <td id="T_c6ca4_row289_col11" class="data row289 col11" >(1024, 1024)</td>
      <td id="T_c6ca4_row289_col12" class="data row289 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row290" class="row_heading level0 row290" >290</th>
      <td id="T_c6ca4_row290_col0" class="data row290 col0" >encoder.layers.3.self_attn_layer_norm.bias</td>
      <td id="T_c6ca4_row290_col1" class="data row290 col1" >encoder.layers.3.self_attn_layer_norm.bias</td>
      <td id="T_c6ca4_row290_col2" class="data row290 col2" >av_romanizer.w2v_model.encoder.layers.3.self_attn_layer_norm.bias</td>
      <td id="T_c6ca4_row290_col3" class="data row290 col3" >0.1755</td>
      <td id="T_c6ca4_row290_col4" class="data row290 col4" >0.621128</td>
      <td id="T_c6ca4_row290_col5" class="data row290 col5" >0.586571</td>
      <td id="T_c6ca4_row290_col6" class="data row290 col6" >0.28255</td>
      <td id="T_c6ca4_row290_col7" class="data row290 col7" >0.95937</td>
      <td id="T_c6ca4_row290_col8" class="data row290 col8" >0.0233459</td>
      <td id="T_c6ca4_row290_col9" class="data row290 col9" >1024.000000</td>
      <td id="T_c6ca4_row290_col10" class="data row290 col10" >torch.float32</td>
      <td id="T_c6ca4_row290_col11" class="data row290 col11" >(1024,)</td>
      <td id="T_c6ca4_row290_col12" class="data row290 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row291" class="row_heading level0 row291" >291</th>
      <td id="T_c6ca4_row291_col0" class="data row291 col0" >encoder.layers.3.self_attn_layer_norm.weight</td>
      <td id="T_c6ca4_row291_col1" class="data row291 col1" >encoder.layers.3.self_attn_layer_norm.weight</td>
      <td id="T_c6ca4_row291_col2" class="data row291 col2" >av_romanizer.w2v_model.encoder.layers.3.self_attn_layer_norm.weight</td>
      <td id="T_c6ca4_row291_col3" class="data row291 col3" >0.459044</td>
      <td id="T_c6ca4_row291_col4" class="data row291 col4" >4.80752</td>
      <td id="T_c6ca4_row291_col5" class="data row291 col5" >4.55538</td>
      <td id="T_c6ca4_row291_col6" class="data row291 col6" >0.0954845</td>
      <td id="T_c6ca4_row291_col7" class="data row291 col7" >0.99664</td>
      <td id="T_c6ca4_row291_col8" class="data row291 col8" >0.0657959</td>
      <td id="T_c6ca4_row291_col9" class="data row291 col9" >1024.000000</td>
      <td id="T_c6ca4_row291_col10" class="data row291 col10" >torch.float32</td>
      <td id="T_c6ca4_row291_col11" class="data row291 col11" >(1024,)</td>
      <td id="T_c6ca4_row291_col12" class="data row291 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row292" class="row_heading level0 row292" >292</th>
      <td id="T_c6ca4_row292_col0" class="data row292 col0" >encoder.layers.4.fc1.bias</td>
      <td id="T_c6ca4_row292_col1" class="data row292 col1" >encoder.layers.4.fc1.bias</td>
      <td id="T_c6ca4_row292_col2" class="data row292 col2" >av_romanizer.w2v_model.encoder.layers.4.fc1.bias</td>
      <td id="T_c6ca4_row292_col3" class="data row292 col3" >0.731023</td>
      <td id="T_c6ca4_row292_col4" class="data row292 col4" >3.97721</td>
      <td id="T_c6ca4_row292_col5" class="data row292 col5" >4.09707</td>
      <td id="T_c6ca4_row292_col6" class="data row292 col6" >0.183803</td>
      <td id="T_c6ca4_row292_col7" class="data row292 col7" >0.984043</td>
      <td id="T_c6ca4_row292_col8" class="data row292 col8" >0.0550079</td>
      <td id="T_c6ca4_row292_col9" class="data row292 col9" >4096.000000</td>
      <td id="T_c6ca4_row292_col10" class="data row292 col10" >torch.float32</td>
      <td id="T_c6ca4_row292_col11" class="data row292 col11" >(4096,)</td>
      <td id="T_c6ca4_row292_col12" class="data row292 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row293" class="row_heading level0 row293" >293</th>
      <td id="T_c6ca4_row293_col0" class="data row293 col0" >encoder.layers.4.fc1.weight</td>
      <td id="T_c6ca4_row293_col1" class="data row293 col1" >encoder.layers.4.fc1.weight</td>
      <td id="T_c6ca4_row293_col2" class="data row293 col2" >av_romanizer.w2v_model.encoder.layers.4.fc1.weight</td>
      <td id="T_c6ca4_row293_col3" class="data row293 col3" >43.2004</td>
      <td id="T_c6ca4_row293_col4" class="data row293 col4" >302.149</td>
      <td id="T_c6ca4_row293_col5" class="data row293 col5" >288.839</td>
      <td id="T_c6ca4_row293_col6" class="data row293 col6" >0.142977</td>
      <td id="T_c6ca4_row293_col7" class="data row293 col7" >0.990323</td>
      <td id="T_c6ca4_row293_col8" class="data row293 col8" >0.179718</td>
      <td id="T_c6ca4_row293_col9" class="data row293 col9" >4194304.000000</td>
      <td id="T_c6ca4_row293_col10" class="data row293 col10" >torch.float32</td>
      <td id="T_c6ca4_row293_col11" class="data row293 col11" >(4096, 1024)</td>
      <td id="T_c6ca4_row293_col12" class="data row293 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row294" class="row_heading level0 row294" >294</th>
      <td id="T_c6ca4_row294_col0" class="data row294 col0" >encoder.layers.4.fc2.bias</td>
      <td id="T_c6ca4_row294_col1" class="data row294 col1" >encoder.layers.4.fc2.bias</td>
      <td id="T_c6ca4_row294_col2" class="data row294 col2" >av_romanizer.w2v_model.encoder.layers.4.fc2.bias</td>
      <td id="T_c6ca4_row294_col3" class="data row294 col3" >0.176726</td>
      <td id="T_c6ca4_row294_col4" class="data row294 col4" >3.0279</td>
      <td id="T_c6ca4_row294_col5" class="data row294 col5" >2.88294</td>
      <td id="T_c6ca4_row294_col6" class="data row294 col6" >0.0583659</td>
      <td id="T_c6ca4_row294_col7" class="data row294 col7" >0.999415</td>
      <td id="T_c6ca4_row294_col8" class="data row294 col8" >0.0390625</td>
      <td id="T_c6ca4_row294_col9" class="data row294 col9" >1024.000000</td>
      <td id="T_c6ca4_row294_col10" class="data row294 col10" >torch.float32</td>
      <td id="T_c6ca4_row294_col11" class="data row294 col11" >(1024,)</td>
      <td id="T_c6ca4_row294_col12" class="data row294 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row295" class="row_heading level0 row295" >295</th>
      <td id="T_c6ca4_row295_col0" class="data row295 col0" >encoder.layers.4.fc2.weight</td>
      <td id="T_c6ca4_row295_col1" class="data row295 col1" >encoder.layers.4.fc2.weight</td>
      <td id="T_c6ca4_row295_col2" class="data row295 col2" >av_romanizer.w2v_model.encoder.layers.4.fc2.weight</td>
      <td id="T_c6ca4_row295_col3" class="data row295 col3" >37.6004</td>
      <td id="T_c6ca4_row295_col4" class="data row295 col4" >290.31</td>
      <td id="T_c6ca4_row295_col5" class="data row295 col5" >275.565</td>
      <td id="T_c6ca4_row295_col6" class="data row295 col6" >0.129518</td>
      <td id="T_c6ca4_row295_col7" class="data row295 col7" >0.992523</td>
      <td id="T_c6ca4_row295_col8" class="data row295 col8" >0.170898</td>
      <td id="T_c6ca4_row295_col9" class="data row295 col9" >4194304.000000</td>
      <td id="T_c6ca4_row295_col10" class="data row295 col10" >torch.float32</td>
      <td id="T_c6ca4_row295_col11" class="data row295 col11" >(1024, 4096)</td>
      <td id="T_c6ca4_row295_col12" class="data row295 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row296" class="row_heading level0 row296" >296</th>
      <td id="T_c6ca4_row296_col0" class="data row296 col0" >encoder.layers.4.final_layer_norm.bias</td>
      <td id="T_c6ca4_row296_col1" class="data row296 col1" >encoder.layers.4.final_layer_norm.bias</td>
      <td id="T_c6ca4_row296_col2" class="data row296 col2" >av_romanizer.w2v_model.encoder.layers.4.final_layer_norm.bias</td>
      <td id="T_c6ca4_row296_col3" class="data row296 col3" >0.20411</td>
      <td id="T_c6ca4_row296_col4" class="data row296 col4" >2.29348</td>
      <td id="T_c6ca4_row296_col5" class="data row296 col5" >2.21811</td>
      <td id="T_c6ca4_row296_col6" class="data row296 col6" >0.088996</td>
      <td id="T_c6ca4_row296_col7" class="data row296 col7" >0.996464</td>
      <td id="T_c6ca4_row296_col8" class="data row296 col8" >0.0249023</td>
      <td id="T_c6ca4_row296_col9" class="data row296 col9" >1024.000000</td>
      <td id="T_c6ca4_row296_col10" class="data row296 col10" >torch.float32</td>
      <td id="T_c6ca4_row296_col11" class="data row296 col11" >(1024,)</td>
      <td id="T_c6ca4_row296_col12" class="data row296 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row297" class="row_heading level0 row297" >297</th>
      <td id="T_c6ca4_row297_col0" class="data row297 col0" >encoder.layers.4.final_layer_norm.weight</td>
      <td id="T_c6ca4_row297_col1" class="data row297 col1" >encoder.layers.4.final_layer_norm.weight</td>
      <td id="T_c6ca4_row297_col2" class="data row297 col2" >av_romanizer.w2v_model.encoder.layers.4.final_layer_norm.weight</td>
      <td id="T_c6ca4_row297_col3" class="data row297 col3" >0.427058</td>
      <td id="T_c6ca4_row297_col4" class="data row297 col4" >5.21576</td>
      <td id="T_c6ca4_row297_col5" class="data row297 col5" >4.89882</td>
      <td id="T_c6ca4_row297_col6" class="data row297 col6" >0.0818784</td>
      <td id="T_c6ca4_row297_col7" class="data row297 col7" >0.998397</td>
      <td id="T_c6ca4_row297_col8" class="data row297 col8" >0.0415039</td>
      <td id="T_c6ca4_row297_col9" class="data row297 col9" >1024.000000</td>
      <td id="T_c6ca4_row297_col10" class="data row297 col10" >torch.float32</td>
      <td id="T_c6ca4_row297_col11" class="data row297 col11" >(1024,)</td>
      <td id="T_c6ca4_row297_col12" class="data row297 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row298" class="row_heading level0 row298" >298</th>
      <td id="T_c6ca4_row298_col0" class="data row298 col0" >encoder.layers.4.self_attn.k_proj.bias</td>
      <td id="T_c6ca4_row298_col1" class="data row298 col1" >encoder.layers.4.self_attn.k_proj.bias</td>
      <td id="T_c6ca4_row298_col2" class="data row298 col2" >av_romanizer.w2v_model.encoder.layers.4.self_attn.k_proj.bias</td>
      <td id="T_c6ca4_row298_col3" class="data row298 col3" >0.0123599</td>
      <td id="T_c6ca4_row298_col4" class="data row298 col4" >0.106072</td>
      <td id="T_c6ca4_row298_col5" class="data row298 col5" >0.101996</td>
      <td id="T_c6ca4_row298_col6" class="data row298 col6" >0.116523</td>
      <td id="T_c6ca4_row298_col7" class="data row298 col7" >0.993708</td>
      <td id="T_c6ca4_row298_col8" class="data row298 col8" >0.00225639</td>
      <td id="T_c6ca4_row298_col9" class="data row298 col9" >1024.000000</td>
      <td id="T_c6ca4_row298_col10" class="data row298 col10" >torch.float32</td>
      <td id="T_c6ca4_row298_col11" class="data row298 col11" >(1024,)</td>
      <td id="T_c6ca4_row298_col12" class="data row298 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row299" class="row_heading level0 row299" >299</th>
      <td id="T_c6ca4_row299_col0" class="data row299 col0" >encoder.layers.4.self_attn.k_proj.weight</td>
      <td id="T_c6ca4_row299_col1" class="data row299 col1" >encoder.layers.4.self_attn.k_proj.weight</td>
      <td id="T_c6ca4_row299_col2" class="data row299 col2" >av_romanizer.w2v_model.encoder.layers.4.self_attn.k_proj.weight</td>
      <td id="T_c6ca4_row299_col3" class="data row299 col3" >21.2212</td>
      <td id="T_c6ca4_row299_col4" class="data row299 col4" >151.256</td>
      <td id="T_c6ca4_row299_col5" class="data row299 col5" >146.14</td>
      <td id="T_c6ca4_row299_col6" class="data row299 col6" >0.1403</td>
      <td id="T_c6ca4_row299_col7" class="data row299 col7" >0.990406</td>
      <td id="T_c6ca4_row299_col8" class="data row299 col8" >0.141602</td>
      <td id="T_c6ca4_row299_col9" class="data row299 col9" >1048576.000000</td>
      <td id="T_c6ca4_row299_col10" class="data row299 col10" >torch.float32</td>
      <td id="T_c6ca4_row299_col11" class="data row299 col11" >(1024, 1024)</td>
      <td id="T_c6ca4_row299_col12" class="data row299 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row300" class="row_heading level0 row300" >300</th>
      <td id="T_c6ca4_row300_col0" class="data row300 col0" >encoder.layers.4.self_attn.out_proj.bias</td>
      <td id="T_c6ca4_row300_col1" class="data row300 col1" >encoder.layers.4.self_attn.out_proj.bias</td>
      <td id="T_c6ca4_row300_col2" class="data row300 col2" >av_romanizer.w2v_model.encoder.layers.4.self_attn.out_proj.bias</td>
      <td id="T_c6ca4_row300_col3" class="data row300 col3" >0.304321</td>
      <td id="T_c6ca4_row300_col4" class="data row300 col4" >3.6886</td>
      <td id="T_c6ca4_row300_col5" class="data row300 col5" >3.47843</td>
      <td id="T_c6ca4_row300_col6" class="data row300 col6" >0.082503</td>
      <td id="T_c6ca4_row300_col7" class="data row300 col7" >0.998112</td>
      <td id="T_c6ca4_row300_col8" class="data row300 col8" >0.09375</td>
      <td id="T_c6ca4_row300_col9" class="data row300 col9" >1024.000000</td>
      <td id="T_c6ca4_row300_col10" class="data row300 col10" >torch.float32</td>
      <td id="T_c6ca4_row300_col11" class="data row300 col11" >(1024,)</td>
      <td id="T_c6ca4_row300_col12" class="data row300 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row301" class="row_heading level0 row301" >301</th>
      <td id="T_c6ca4_row301_col0" class="data row301 col0" >encoder.layers.4.self_attn.out_proj.weight</td>
      <td id="T_c6ca4_row301_col1" class="data row301 col1" >encoder.layers.4.self_attn.out_proj.weight</td>
      <td id="T_c6ca4_row301_col2" class="data row301 col2" >av_romanizer.w2v_model.encoder.layers.4.self_attn.out_proj.weight</td>
      <td id="T_c6ca4_row301_col3" class="data row301 col3" >21.1162</td>
      <td id="T_c6ca4_row301_col4" class="data row301 col4" >123.194</td>
      <td id="T_c6ca4_row301_col5" class="data row301 col5" >116.681</td>
      <td id="T_c6ca4_row301_col6" class="data row301 col6" >0.171406</td>
      <td id="T_c6ca4_row301_col7" class="data row301 col7" >0.985966</td>
      <td id="T_c6ca4_row301_col8" class="data row301 col8" >0.131592</td>
      <td id="T_c6ca4_row301_col9" class="data row301 col9" >1048576.000000</td>
      <td id="T_c6ca4_row301_col10" class="data row301 col10" >torch.float32</td>
      <td id="T_c6ca4_row301_col11" class="data row301 col11" >(1024, 1024)</td>
      <td id="T_c6ca4_row301_col12" class="data row301 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row302" class="row_heading level0 row302" >302</th>
      <td id="T_c6ca4_row302_col0" class="data row302 col0" >encoder.layers.4.self_attn.q_proj.bias</td>
      <td id="T_c6ca4_row302_col1" class="data row302 col1" >encoder.layers.4.self_attn.q_proj.bias</td>
      <td id="T_c6ca4_row302_col2" class="data row302 col2" >av_romanizer.w2v_model.encoder.layers.4.self_attn.q_proj.bias</td>
      <td id="T_c6ca4_row302_col3" class="data row302 col3" >0.944594</td>
      <td id="T_c6ca4_row302_col4" class="data row302 col4" >9.77992</td>
      <td id="T_c6ca4_row302_col5" class="data row302 col5" >9.53155</td>
      <td id="T_c6ca4_row302_col6" class="data row302 col6" >0.096585</td>
      <td id="T_c6ca4_row302_col7" class="data row302 col7" >0.995545</td>
      <td id="T_c6ca4_row302_col8" class="data row302 col8" >0.11731</td>
      <td id="T_c6ca4_row302_col9" class="data row302 col9" >1024.000000</td>
      <td id="T_c6ca4_row302_col10" class="data row302 col10" >torch.float32</td>
      <td id="T_c6ca4_row302_col11" class="data row302 col11" >(1024,)</td>
      <td id="T_c6ca4_row302_col12" class="data row302 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row303" class="row_heading level0 row303" >303</th>
      <td id="T_c6ca4_row303_col0" class="data row303 col0" >encoder.layers.4.self_attn.q_proj.weight</td>
      <td id="T_c6ca4_row303_col1" class="data row303 col1" >encoder.layers.4.self_attn.q_proj.weight</td>
      <td id="T_c6ca4_row303_col2" class="data row303 col2" >av_romanizer.w2v_model.encoder.layers.4.self_attn.q_proj.weight</td>
      <td id="T_c6ca4_row303_col3" class="data row303 col3" >21.3783</td>
      <td id="T_c6ca4_row303_col4" class="data row303 col4" >149.764</td>
      <td id="T_c6ca4_row303_col5" class="data row303 col5" >144.623</td>
      <td id="T_c6ca4_row303_col6" class="data row303 col6" >0.142747</td>
      <td id="T_c6ca4_row303_col7" class="data row303 col7" >0.99006</td>
      <td id="T_c6ca4_row303_col8" class="data row303 col8" >0.131348</td>
      <td id="T_c6ca4_row303_col9" class="data row303 col9" >1048576.000000</td>
      <td id="T_c6ca4_row303_col10" class="data row303 col10" >torch.float32</td>
      <td id="T_c6ca4_row303_col11" class="data row303 col11" >(1024, 1024)</td>
      <td id="T_c6ca4_row303_col12" class="data row303 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row304" class="row_heading level0 row304" >304</th>
      <td id="T_c6ca4_row304_col0" class="data row304 col0" >encoder.layers.4.self_attn.v_proj.bias</td>
      <td id="T_c6ca4_row304_col1" class="data row304 col1" >encoder.layers.4.self_attn.v_proj.bias</td>
      <td id="T_c6ca4_row304_col2" class="data row304 col2" >av_romanizer.w2v_model.encoder.layers.4.self_attn.v_proj.bias</td>
      <td id="T_c6ca4_row304_col3" class="data row304 col3" >0.214227</td>
      <td id="T_c6ca4_row304_col4" class="data row304 col4" >1.0758</td>
      <td id="T_c6ca4_row304_col5" class="data row304 col5" >0.982841</td>
      <td id="T_c6ca4_row304_col6" class="data row304 col6" >0.199133</td>
      <td id="T_c6ca4_row304_col7" class="data row304 col7" >0.982384</td>
      <td id="T_c6ca4_row304_col8" class="data row304 col8" >0.043457</td>
      <td id="T_c6ca4_row304_col9" class="data row304 col9" >1024.000000</td>
      <td id="T_c6ca4_row304_col10" class="data row304 col10" >torch.float32</td>
      <td id="T_c6ca4_row304_col11" class="data row304 col11" >(1024,)</td>
      <td id="T_c6ca4_row304_col12" class="data row304 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row305" class="row_heading level0 row305" >305</th>
      <td id="T_c6ca4_row305_col0" class="data row305 col0" >encoder.layers.4.self_attn.v_proj.weight</td>
      <td id="T_c6ca4_row305_col1" class="data row305 col1" >encoder.layers.4.self_attn.v_proj.weight</td>
      <td id="T_c6ca4_row305_col2" class="data row305 col2" >av_romanizer.w2v_model.encoder.layers.4.self_attn.v_proj.weight</td>
      <td id="T_c6ca4_row305_col3" class="data row305 col3" >19.058</td>
      <td id="T_c6ca4_row305_col4" class="data row305 col4" >122.728</td>
      <td id="T_c6ca4_row305_col5" class="data row305 col5" >116.343</td>
      <td id="T_c6ca4_row305_col6" class="data row305 col6" >0.155286</td>
      <td id="T_c6ca4_row305_col7" class="data row305 col7" >0.988709</td>
      <td id="T_c6ca4_row305_col8" class="data row305 col8" >0.10553</td>
      <td id="T_c6ca4_row305_col9" class="data row305 col9" >1048576.000000</td>
      <td id="T_c6ca4_row305_col10" class="data row305 col10" >torch.float32</td>
      <td id="T_c6ca4_row305_col11" class="data row305 col11" >(1024, 1024)</td>
      <td id="T_c6ca4_row305_col12" class="data row305 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row306" class="row_heading level0 row306" >306</th>
      <td id="T_c6ca4_row306_col0" class="data row306 col0" >encoder.layers.4.self_attn_layer_norm.bias</td>
      <td id="T_c6ca4_row306_col1" class="data row306 col1" >encoder.layers.4.self_attn_layer_norm.bias</td>
      <td id="T_c6ca4_row306_col2" class="data row306 col2" >av_romanizer.w2v_model.encoder.layers.4.self_attn_layer_norm.bias</td>
      <td id="T_c6ca4_row306_col3" class="data row306 col3" >0.171862</td>
      <td id="T_c6ca4_row306_col4" class="data row306 col4" >0.604278</td>
      <td id="T_c6ca4_row306_col5" class="data row306 col5" >0.579123</td>
      <td id="T_c6ca4_row306_col6" class="data row306 col6" >0.284408</td>
      <td id="T_c6ca4_row306_col7" class="data row306 col7" >0.958703</td>
      <td id="T_c6ca4_row306_col8" class="data row306 col8" >0.0377884</td>
      <td id="T_c6ca4_row306_col9" class="data row306 col9" >1024.000000</td>
      <td id="T_c6ca4_row306_col10" class="data row306 col10" >torch.float32</td>
      <td id="T_c6ca4_row306_col11" class="data row306 col11" >(1024,)</td>
      <td id="T_c6ca4_row306_col12" class="data row306 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row307" class="row_heading level0 row307" >307</th>
      <td id="T_c6ca4_row307_col0" class="data row307 col0" >encoder.layers.4.self_attn_layer_norm.weight</td>
      <td id="T_c6ca4_row307_col1" class="data row307 col1" >encoder.layers.4.self_attn_layer_norm.weight</td>
      <td id="T_c6ca4_row307_col2" class="data row307 col2" >av_romanizer.w2v_model.encoder.layers.4.self_attn_layer_norm.weight</td>
      <td id="T_c6ca4_row307_col3" class="data row307 col3" >0.346473</td>
      <td id="T_c6ca4_row307_col4" class="data row307 col4" >5.09366</td>
      <td id="T_c6ca4_row307_col5" class="data row307 col5" >5.05697</td>
      <td id="T_c6ca4_row307_col6" class="data row307 col6" >0.0680205</td>
      <td id="T_c6ca4_row307_col7" class="data row307 col7" >0.997696</td>
      <td id="T_c6ca4_row307_col8" class="data row307 col8" >0.0385742</td>
      <td id="T_c6ca4_row307_col9" class="data row307 col9" >1024.000000</td>
      <td id="T_c6ca4_row307_col10" class="data row307 col10" >torch.float32</td>
      <td id="T_c6ca4_row307_col11" class="data row307 col11" >(1024,)</td>
      <td id="T_c6ca4_row307_col12" class="data row307 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row308" class="row_heading level0 row308" >308</th>
      <td id="T_c6ca4_row308_col0" class="data row308 col0" >encoder.layers.5.fc1.bias</td>
      <td id="T_c6ca4_row308_col1" class="data row308 col1" >encoder.layers.5.fc1.bias</td>
      <td id="T_c6ca4_row308_col2" class="data row308 col2" >av_romanizer.w2v_model.encoder.layers.5.fc1.bias</td>
      <td id="T_c6ca4_row308_col3" class="data row308 col3" >0.706937</td>
      <td id="T_c6ca4_row308_col4" class="data row308 col4" >3.86419</td>
      <td id="T_c6ca4_row308_col5" class="data row308 col5" >3.91208</td>
      <td id="T_c6ca4_row308_col6" class="data row308 col6" >0.182946</td>
      <td id="T_c6ca4_row308_col7" class="data row308 col7" >0.983546</td>
      <td id="T_c6ca4_row308_col8" class="data row308 col8" >0.0809631</td>
      <td id="T_c6ca4_row308_col9" class="data row308 col9" >4096.000000</td>
      <td id="T_c6ca4_row308_col10" class="data row308 col10" >torch.float32</td>
      <td id="T_c6ca4_row308_col11" class="data row308 col11" >(4096,)</td>
      <td id="T_c6ca4_row308_col12" class="data row308 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row309" class="row_heading level0 row309" >309</th>
      <td id="T_c6ca4_row309_col0" class="data row309 col0" >encoder.layers.5.fc1.weight</td>
      <td id="T_c6ca4_row309_col1" class="data row309 col1" >encoder.layers.5.fc1.weight</td>
      <td id="T_c6ca4_row309_col2" class="data row309 col2" >av_romanizer.w2v_model.encoder.layers.5.fc1.weight</td>
      <td id="T_c6ca4_row309_col3" class="data row309 col3" >44.4618</td>
      <td id="T_c6ca4_row309_col4" class="data row309 col4" >302.833</td>
      <td id="T_c6ca4_row309_col5" class="data row309 col5" >289.406</td>
      <td id="T_c6ca4_row309_col6" class="data row309 col6" >0.14682</td>
      <td id="T_c6ca4_row309_col7" class="data row309 col7" >0.98975</td>
      <td id="T_c6ca4_row309_col8" class="data row309 col8" >0.163269</td>
      <td id="T_c6ca4_row309_col9" class="data row309 col9" >4194304.000000</td>
      <td id="T_c6ca4_row309_col10" class="data row309 col10" >torch.float32</td>
      <td id="T_c6ca4_row309_col11" class="data row309 col11" >(4096, 1024)</td>
      <td id="T_c6ca4_row309_col12" class="data row309 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row310" class="row_heading level0 row310" >310</th>
      <td id="T_c6ca4_row310_col0" class="data row310 col0" >encoder.layers.5.fc2.bias</td>
      <td id="T_c6ca4_row310_col1" class="data row310 col1" >encoder.layers.5.fc2.bias</td>
      <td id="T_c6ca4_row310_col2" class="data row310 col2" >av_romanizer.w2v_model.encoder.layers.5.fc2.bias</td>
      <td id="T_c6ca4_row310_col3" class="data row310 col3" >0.171871</td>
      <td id="T_c6ca4_row310_col4" class="data row310 col4" >2.95108</td>
      <td id="T_c6ca4_row310_col5" class="data row310 col5" >2.81225</td>
      <td id="T_c6ca4_row310_col6" class="data row310 col6" >0.0582402</td>
      <td id="T_c6ca4_row310_col7" class="data row310 col7" >0.999382</td>
      <td id="T_c6ca4_row310_col8" class="data row310 col8" >0.0439453</td>
      <td id="T_c6ca4_row310_col9" class="data row310 col9" >1024.000000</td>
      <td id="T_c6ca4_row310_col10" class="data row310 col10" >torch.float32</td>
      <td id="T_c6ca4_row310_col11" class="data row310 col11" >(1024,)</td>
      <td id="T_c6ca4_row310_col12" class="data row310 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row311" class="row_heading level0 row311" >311</th>
      <td id="T_c6ca4_row311_col0" class="data row311 col0" >encoder.layers.5.fc2.weight</td>
      <td id="T_c6ca4_row311_col1" class="data row311 col1" >encoder.layers.5.fc2.weight</td>
      <td id="T_c6ca4_row311_col2" class="data row311 col2" >av_romanizer.w2v_model.encoder.layers.5.fc2.weight</td>
      <td id="T_c6ca4_row311_col3" class="data row311 col3" >39.0947</td>
      <td id="T_c6ca4_row311_col4" class="data row311 col4" >288.894</td>
      <td id="T_c6ca4_row311_col5" class="data row311 col5" >274.46</td>
      <td id="T_c6ca4_row311_col6" class="data row311 col6" >0.135326</td>
      <td id="T_c6ca4_row311_col7" class="data row311 col7" >0.991676</td>
      <td id="T_c6ca4_row311_col8" class="data row311 col8" >0.152222</td>
      <td id="T_c6ca4_row311_col9" class="data row311 col9" >4194304.000000</td>
      <td id="T_c6ca4_row311_col10" class="data row311 col10" >torch.float32</td>
      <td id="T_c6ca4_row311_col11" class="data row311 col11" >(1024, 4096)</td>
      <td id="T_c6ca4_row311_col12" class="data row311 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row312" class="row_heading level0 row312" >312</th>
      <td id="T_c6ca4_row312_col0" class="data row312 col0" >encoder.layers.5.final_layer_norm.bias</td>
      <td id="T_c6ca4_row312_col1" class="data row312 col1" >encoder.layers.5.final_layer_norm.bias</td>
      <td id="T_c6ca4_row312_col2" class="data row312 col2" >av_romanizer.w2v_model.encoder.layers.5.final_layer_norm.bias</td>
      <td id="T_c6ca4_row312_col3" class="data row312 col3" >0.210115</td>
      <td id="T_c6ca4_row312_col4" class="data row312 col4" >2.30726</td>
      <td id="T_c6ca4_row312_col5" class="data row312 col5" >2.21434</td>
      <td id="T_c6ca4_row312_col6" class="data row312 col6" >0.0910668</td>
      <td id="T_c6ca4_row312_col7" class="data row312 col7" >0.996524</td>
      <td id="T_c6ca4_row312_col8" class="data row312 col8" >0.0335693</td>
      <td id="T_c6ca4_row312_col9" class="data row312 col9" >1024.000000</td>
      <td id="T_c6ca4_row312_col10" class="data row312 col10" >torch.float32</td>
      <td id="T_c6ca4_row312_col11" class="data row312 col11" >(1024,)</td>
      <td id="T_c6ca4_row312_col12" class="data row312 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row313" class="row_heading level0 row313" >313</th>
      <td id="T_c6ca4_row313_col0" class="data row313 col0" >encoder.layers.5.final_layer_norm.weight</td>
      <td id="T_c6ca4_row313_col1" class="data row313 col1" >encoder.layers.5.final_layer_norm.weight</td>
      <td id="T_c6ca4_row313_col2" class="data row313 col2" >av_romanizer.w2v_model.encoder.layers.5.final_layer_norm.weight</td>
      <td id="T_c6ca4_row313_col3" class="data row313 col3" >0.452291</td>
      <td id="T_c6ca4_row313_col4" class="data row313 col4" >5.46452</td>
      <td id="T_c6ca4_row313_col5" class="data row313 col5" >5.10635</td>
      <td id="T_c6ca4_row313_col6" class="data row313 col6" >0.0827686</td>
      <td id="T_c6ca4_row313_col7" class="data row313 col7" >0.998633</td>
      <td id="T_c6ca4_row313_col8" class="data row313 col8" >0.0421143</td>
      <td id="T_c6ca4_row313_col9" class="data row313 col9" >1024.000000</td>
      <td id="T_c6ca4_row313_col10" class="data row313 col10" >torch.float32</td>
      <td id="T_c6ca4_row313_col11" class="data row313 col11" >(1024,)</td>
      <td id="T_c6ca4_row313_col12" class="data row313 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row314" class="row_heading level0 row314" >314</th>
      <td id="T_c6ca4_row314_col0" class="data row314 col0" >encoder.layers.5.self_attn.k_proj.bias</td>
      <td id="T_c6ca4_row314_col1" class="data row314 col1" >encoder.layers.5.self_attn.k_proj.bias</td>
      <td id="T_c6ca4_row314_col2" class="data row314 col2" >av_romanizer.w2v_model.encoder.layers.5.self_attn.k_proj.bias</td>
      <td id="T_c6ca4_row314_col3" class="data row314 col3" >0.012228</td>
      <td id="T_c6ca4_row314_col4" class="data row314 col4" >0.102177</td>
      <td id="T_c6ca4_row314_col5" class="data row314 col5" >0.0981088</td>
      <td id="T_c6ca4_row314_col6" class="data row314 col6" >0.119674</td>
      <td id="T_c6ca4_row314_col7" class="data row314 col7" >0.993368</td>
      <td id="T_c6ca4_row314_col8" class="data row314 col8" >0.00236511</td>
      <td id="T_c6ca4_row314_col9" class="data row314 col9" >1024.000000</td>
      <td id="T_c6ca4_row314_col10" class="data row314 col10" >torch.float32</td>
      <td id="T_c6ca4_row314_col11" class="data row314 col11" >(1024,)</td>
      <td id="T_c6ca4_row314_col12" class="data row314 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row315" class="row_heading level0 row315" >315</th>
      <td id="T_c6ca4_row315_col0" class="data row315 col0" >encoder.layers.5.self_attn.k_proj.weight</td>
      <td id="T_c6ca4_row315_col1" class="data row315 col1" >encoder.layers.5.self_attn.k_proj.weight</td>
      <td id="T_c6ca4_row315_col2" class="data row315 col2" >av_romanizer.w2v_model.encoder.layers.5.self_attn.k_proj.weight</td>
      <td id="T_c6ca4_row315_col3" class="data row315 col3" >21.4672</td>
      <td id="T_c6ca4_row315_col4" class="data row315 col4" >155.05</td>
      <td id="T_c6ca4_row315_col5" class="data row315 col5" >149.442</td>
      <td id="T_c6ca4_row315_col6" class="data row315 col6" >0.138453</td>
      <td id="T_c6ca4_row315_col7" class="data row315 col7" >0.990734</td>
      <td id="T_c6ca4_row315_col8" class="data row315 col8" >0.174316</td>
      <td id="T_c6ca4_row315_col9" class="data row315 col9" >1048576.000000</td>
      <td id="T_c6ca4_row315_col10" class="data row315 col10" >torch.float32</td>
      <td id="T_c6ca4_row315_col11" class="data row315 col11" >(1024, 1024)</td>
      <td id="T_c6ca4_row315_col12" class="data row315 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row316" class="row_heading level0 row316" >316</th>
      <td id="T_c6ca4_row316_col0" class="data row316 col0" >encoder.layers.5.self_attn.out_proj.bias</td>
      <td id="T_c6ca4_row316_col1" class="data row316 col1" >encoder.layers.5.self_attn.out_proj.bias</td>
      <td id="T_c6ca4_row316_col2" class="data row316 col2" >av_romanizer.w2v_model.encoder.layers.5.self_attn.out_proj.bias</td>
      <td id="T_c6ca4_row316_col3" class="data row316 col3" >0.305821</td>
      <td id="T_c6ca4_row316_col4" class="data row316 col4" >3.86815</td>
      <td id="T_c6ca4_row316_col5" class="data row316 col5" >3.6467</td>
      <td id="T_c6ca4_row316_col6" class="data row316 col6" >0.0790613</td>
      <td id="T_c6ca4_row316_col7" class="data row316 col7" >0.998423</td>
      <td id="T_c6ca4_row316_col8" class="data row316 col8" >0.103516</td>
      <td id="T_c6ca4_row316_col9" class="data row316 col9" >1024.000000</td>
      <td id="T_c6ca4_row316_col10" class="data row316 col10" >torch.float32</td>
      <td id="T_c6ca4_row316_col11" class="data row316 col11" >(1024,)</td>
      <td id="T_c6ca4_row316_col12" class="data row316 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row317" class="row_heading level0 row317" >317</th>
      <td id="T_c6ca4_row317_col0" class="data row317 col0" >encoder.layers.5.self_attn.out_proj.weight</td>
      <td id="T_c6ca4_row317_col1" class="data row317 col1" >encoder.layers.5.self_attn.out_proj.weight</td>
      <td id="T_c6ca4_row317_col2" class="data row317 col2" >av_romanizer.w2v_model.encoder.layers.5.self_attn.out_proj.weight</td>
      <td id="T_c6ca4_row317_col3" class="data row317 col3" >21.2255</td>
      <td id="T_c6ca4_row317_col4" class="data row317 col4" >123.021</td>
      <td id="T_c6ca4_row317_col5" class="data row317 col5" >116.664</td>
      <td id="T_c6ca4_row317_col6" class="data row317 col6" >0.172535</td>
      <td id="T_c6ca4_row317_col7" class="data row317 col7" >0.985713</td>
      <td id="T_c6ca4_row317_col8" class="data row317 col8" >0.117676</td>
      <td id="T_c6ca4_row317_col9" class="data row317 col9" >1048576.000000</td>
      <td id="T_c6ca4_row317_col10" class="data row317 col10" >torch.float32</td>
      <td id="T_c6ca4_row317_col11" class="data row317 col11" >(1024, 1024)</td>
      <td id="T_c6ca4_row317_col12" class="data row317 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row318" class="row_heading level0 row318" >318</th>
      <td id="T_c6ca4_row318_col0" class="data row318 col0" >encoder.layers.5.self_attn.q_proj.bias</td>
      <td id="T_c6ca4_row318_col1" class="data row318 col1" >encoder.layers.5.self_attn.q_proj.bias</td>
      <td id="T_c6ca4_row318_col2" class="data row318 col2" >av_romanizer.w2v_model.encoder.layers.5.self_attn.q_proj.bias</td>
      <td id="T_c6ca4_row318_col3" class="data row318 col3" >0.852347</td>
      <td id="T_c6ca4_row318_col4" class="data row318 col4" >9.32754</td>
      <td id="T_c6ca4_row318_col5" class="data row318 col5" >9.09696</td>
      <td id="T_c6ca4_row318_col6" class="data row318 col6" >0.0913796</td>
      <td id="T_c6ca4_row318_col7" class="data row318 col7" >0.996032</td>
      <td id="T_c6ca4_row318_col8" class="data row318 col8" >0.157227</td>
      <td id="T_c6ca4_row318_col9" class="data row318 col9" >1024.000000</td>
      <td id="T_c6ca4_row318_col10" class="data row318 col10" >torch.float32</td>
      <td id="T_c6ca4_row318_col11" class="data row318 col11" >(1024,)</td>
      <td id="T_c6ca4_row318_col12" class="data row318 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row319" class="row_heading level0 row319" >319</th>
      <td id="T_c6ca4_row319_col0" class="data row319 col0" >encoder.layers.5.self_attn.q_proj.weight</td>
      <td id="T_c6ca4_row319_col1" class="data row319 col1" >encoder.layers.5.self_attn.q_proj.weight</td>
      <td id="T_c6ca4_row319_col2" class="data row319 col2" >av_romanizer.w2v_model.encoder.layers.5.self_attn.q_proj.weight</td>
      <td id="T_c6ca4_row319_col3" class="data row319 col3" >21.9935</td>
      <td id="T_c6ca4_row319_col4" class="data row319 col4" >154.456</td>
      <td id="T_c6ca4_row319_col5" class="data row319 col5" >148.767</td>
      <td id="T_c6ca4_row319_col6" class="data row319 col6" >0.142393</td>
      <td id="T_c6ca4_row319_col7" class="data row319 col7" >0.990179</td>
      <td id="T_c6ca4_row319_col8" class="data row319 col8" >0.178467</td>
      <td id="T_c6ca4_row319_col9" class="data row319 col9" >1048576.000000</td>
      <td id="T_c6ca4_row319_col10" class="data row319 col10" >torch.float32</td>
      <td id="T_c6ca4_row319_col11" class="data row319 col11" >(1024, 1024)</td>
      <td id="T_c6ca4_row319_col12" class="data row319 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row320" class="row_heading level0 row320" >320</th>
      <td id="T_c6ca4_row320_col0" class="data row320 col0" >encoder.layers.5.self_attn.v_proj.bias</td>
      <td id="T_c6ca4_row320_col1" class="data row320 col1" >encoder.layers.5.self_attn.v_proj.bias</td>
      <td id="T_c6ca4_row320_col2" class="data row320 col2" >av_romanizer.w2v_model.encoder.layers.5.self_attn.v_proj.bias</td>
      <td id="T_c6ca4_row320_col3" class="data row320 col3" >0.237682</td>
      <td id="T_c6ca4_row320_col4" class="data row320 col4" >0.973019</td>
      <td id="T_c6ca4_row320_col5" class="data row320 col5" >0.878312</td>
      <td id="T_c6ca4_row320_col6" class="data row320 col6" >0.244273</td>
      <td id="T_c6ca4_row320_col7" class="data row320 col7" >0.972196</td>
      <td id="T_c6ca4_row320_col8" class="data row320 col8" >0.0339355</td>
      <td id="T_c6ca4_row320_col9" class="data row320 col9" >1024.000000</td>
      <td id="T_c6ca4_row320_col10" class="data row320 col10" >torch.float32</td>
      <td id="T_c6ca4_row320_col11" class="data row320 col11" >(1024,)</td>
      <td id="T_c6ca4_row320_col12" class="data row320 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row321" class="row_heading level0 row321" >321</th>
      <td id="T_c6ca4_row321_col0" class="data row321 col0" >encoder.layers.5.self_attn.v_proj.weight</td>
      <td id="T_c6ca4_row321_col1" class="data row321 col1" >encoder.layers.5.self_attn.v_proj.weight</td>
      <td id="T_c6ca4_row321_col2" class="data row321 col2" >av_romanizer.w2v_model.encoder.layers.5.self_attn.v_proj.weight</td>
      <td id="T_c6ca4_row321_col3" class="data row321 col3" >19.3904</td>
      <td id="T_c6ca4_row321_col4" class="data row321 col4" >123.712</td>
      <td id="T_c6ca4_row321_col5" class="data row321 col5" >117.39</td>
      <td id="T_c6ca4_row321_col6" class="data row321 col6" >0.156738</td>
      <td id="T_c6ca4_row321_col7" class="data row321 col7" >0.988431</td>
      <td id="T_c6ca4_row321_col8" class="data row321 col8" >0.10791</td>
      <td id="T_c6ca4_row321_col9" class="data row321 col9" >1048576.000000</td>
      <td id="T_c6ca4_row321_col10" class="data row321 col10" >torch.float32</td>
      <td id="T_c6ca4_row321_col11" class="data row321 col11" >(1024, 1024)</td>
      <td id="T_c6ca4_row321_col12" class="data row321 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row322" class="row_heading level0 row322" >322</th>
      <td id="T_c6ca4_row322_col0" class="data row322 col0" >encoder.layers.5.self_attn_layer_norm.bias</td>
      <td id="T_c6ca4_row322_col1" class="data row322 col1" >encoder.layers.5.self_attn_layer_norm.bias</td>
      <td id="T_c6ca4_row322_col2" class="data row322 col2" >av_romanizer.w2v_model.encoder.layers.5.self_attn_layer_norm.bias</td>
      <td id="T_c6ca4_row322_col3" class="data row322 col3" >0.178683</td>
      <td id="T_c6ca4_row322_col4" class="data row322 col4" >0.629679</td>
      <td id="T_c6ca4_row322_col5" class="data row322 col5" >0.58906</td>
      <td id="T_c6ca4_row322_col6" class="data row322 col6" >0.283768</td>
      <td id="T_c6ca4_row322_col7" class="data row322 col7" >0.959186</td>
      <td id="T_c6ca4_row322_col8" class="data row322 col8" >0.0401611</td>
      <td id="T_c6ca4_row322_col9" class="data row322 col9" >1024.000000</td>
      <td id="T_c6ca4_row322_col10" class="data row322 col10" >torch.float32</td>
      <td id="T_c6ca4_row322_col11" class="data row322 col11" >(1024,)</td>
      <td id="T_c6ca4_row322_col12" class="data row322 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row323" class="row_heading level0 row323" >323</th>
      <td id="T_c6ca4_row323_col0" class="data row323 col0" >encoder.layers.5.self_attn_layer_norm.weight</td>
      <td id="T_c6ca4_row323_col1" class="data row323 col1" >encoder.layers.5.self_attn_layer_norm.weight</td>
      <td id="T_c6ca4_row323_col2" class="data row323 col2" >av_romanizer.w2v_model.encoder.layers.5.self_attn_layer_norm.weight</td>
      <td id="T_c6ca4_row323_col3" class="data row323 col3" >0.413114</td>
      <td id="T_c6ca4_row323_col4" class="data row323 col4" >5.34605</td>
      <td id="T_c6ca4_row323_col5" class="data row323 col5" >5.11977</td>
      <td id="T_c6ca4_row323_col6" class="data row323 col6" >0.0772746</td>
      <td id="T_c6ca4_row323_col7" class="data row323 col7" >0.997818</td>
      <td id="T_c6ca4_row323_col8" class="data row323 col8" >0.0579834</td>
      <td id="T_c6ca4_row323_col9" class="data row323 col9" >1024.000000</td>
      <td id="T_c6ca4_row323_col10" class="data row323 col10" >torch.float32</td>
      <td id="T_c6ca4_row323_col11" class="data row323 col11" >(1024,)</td>
      <td id="T_c6ca4_row323_col12" class="data row323 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row324" class="row_heading level0 row324" >324</th>
      <td id="T_c6ca4_row324_col0" class="data row324 col0" >encoder.layers.6.fc1.bias</td>
      <td id="T_c6ca4_row324_col1" class="data row324 col1" >encoder.layers.6.fc1.bias</td>
      <td id="T_c6ca4_row324_col2" class="data row324 col2" >av_romanizer.w2v_model.encoder.layers.6.fc1.bias</td>
      <td id="T_c6ca4_row324_col3" class="data row324 col3" >0.6663</td>
      <td id="T_c6ca4_row324_col4" class="data row324 col4" >3.40546</td>
      <td id="T_c6ca4_row324_col5" class="data row324 col5" >3.37256</td>
      <td id="T_c6ca4_row324_col6" class="data row324 col6" >0.195656</td>
      <td id="T_c6ca4_row324_col7" class="data row324 col7" >0.98072</td>
      <td id="T_c6ca4_row324_col8" class="data row324 col8" >0.0705566</td>
      <td id="T_c6ca4_row324_col9" class="data row324 col9" >4096.000000</td>
      <td id="T_c6ca4_row324_col10" class="data row324 col10" >torch.float32</td>
      <td id="T_c6ca4_row324_col11" class="data row324 col11" >(4096,)</td>
      <td id="T_c6ca4_row324_col12" class="data row324 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row325" class="row_heading level0 row325" >325</th>
      <td id="T_c6ca4_row325_col0" class="data row325 col0" >encoder.layers.6.fc1.weight</td>
      <td id="T_c6ca4_row325_col1" class="data row325 col1" >encoder.layers.6.fc1.weight</td>
      <td id="T_c6ca4_row325_col2" class="data row325 col2" >av_romanizer.w2v_model.encoder.layers.6.fc1.weight</td>
      <td id="T_c6ca4_row325_col3" class="data row325 col3" >45.6082</td>
      <td id="T_c6ca4_row325_col4" class="data row325 col4" >300.991</td>
      <td id="T_c6ca4_row325_col5" class="data row325 col5" >287.677</td>
      <td id="T_c6ca4_row325_col6" class="data row325 col6" >0.151527</td>
      <td id="T_c6ca4_row325_col7" class="data row325 col7" >0.989012</td>
      <td id="T_c6ca4_row325_col8" class="data row325 col8" >0.146484</td>
      <td id="T_c6ca4_row325_col9" class="data row325 col9" >4194304.000000</td>
      <td id="T_c6ca4_row325_col10" class="data row325 col10" >torch.float32</td>
      <td id="T_c6ca4_row325_col11" class="data row325 col11" >(4096, 1024)</td>
      <td id="T_c6ca4_row325_col12" class="data row325 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row326" class="row_heading level0 row326" >326</th>
      <td id="T_c6ca4_row326_col0" class="data row326 col0" >encoder.layers.6.fc2.bias</td>
      <td id="T_c6ca4_row326_col1" class="data row326 col1" >encoder.layers.6.fc2.bias</td>
      <td id="T_c6ca4_row326_col2" class="data row326 col2" >av_romanizer.w2v_model.encoder.layers.6.fc2.bias</td>
      <td id="T_c6ca4_row326_col3" class="data row326 col3" >0.151262</td>
      <td id="T_c6ca4_row326_col4" class="data row326 col4" >2.77136</td>
      <td id="T_c6ca4_row326_col5" class="data row326 col5" >2.66194</td>
      <td id="T_c6ca4_row326_col6" class="data row326 col6" >0.0545805</td>
      <td id="T_c6ca4_row326_col7" class="data row326 col7" >0.999261</td>
      <td id="T_c6ca4_row326_col8" class="data row326 col8" >0.036377</td>
      <td id="T_c6ca4_row326_col9" class="data row326 col9" >1024.000000</td>
      <td id="T_c6ca4_row326_col10" class="data row326 col10" >torch.float32</td>
      <td id="T_c6ca4_row326_col11" class="data row326 col11" >(1024,)</td>
      <td id="T_c6ca4_row326_col12" class="data row326 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row327" class="row_heading level0 row327" >327</th>
      <td id="T_c6ca4_row327_col0" class="data row327 col0" >encoder.layers.6.fc2.weight</td>
      <td id="T_c6ca4_row327_col1" class="data row327 col1" >encoder.layers.6.fc2.weight</td>
      <td id="T_c6ca4_row327_col2" class="data row327 col2" >av_romanizer.w2v_model.encoder.layers.6.fc2.weight</td>
      <td id="T_c6ca4_row327_col3" class="data row327 col3" >40.9337</td>
      <td id="T_c6ca4_row327_col4" class="data row327 col4" >285.489</td>
      <td id="T_c6ca4_row327_col5" class="data row327 col5" >271.499</td>
      <td id="T_c6ca4_row327_col6" class="data row327 col6" >0.143381</td>
      <td id="T_c6ca4_row327_col7" class="data row327 col7" >0.990454</td>
      <td id="T_c6ca4_row327_col8" class="data row327 col8" >0.168213</td>
      <td id="T_c6ca4_row327_col9" class="data row327 col9" >4194304.000000</td>
      <td id="T_c6ca4_row327_col10" class="data row327 col10" >torch.float32</td>
      <td id="T_c6ca4_row327_col11" class="data row327 col11" >(1024, 4096)</td>
      <td id="T_c6ca4_row327_col12" class="data row327 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row328" class="row_heading level0 row328" >328</th>
      <td id="T_c6ca4_row328_col0" class="data row328 col0" >encoder.layers.6.final_layer_norm.bias</td>
      <td id="T_c6ca4_row328_col1" class="data row328 col1" >encoder.layers.6.final_layer_norm.bias</td>
      <td id="T_c6ca4_row328_col2" class="data row328 col2" >av_romanizer.w2v_model.encoder.layers.6.final_layer_norm.bias</td>
      <td id="T_c6ca4_row328_col3" class="data row328 col3" >0.216904</td>
      <td id="T_c6ca4_row328_col4" class="data row328 col4" >2.20483</td>
      <td id="T_c6ca4_row328_col5" class="data row328 col5" >2.10364</td>
      <td id="T_c6ca4_row328_col6" class="data row328 col6" >0.0983767</td>
      <td id="T_c6ca4_row328_col7" class="data row328 col7" >0.996032</td>
      <td id="T_c6ca4_row328_col8" class="data row328 col8" >0.0368652</td>
      <td id="T_c6ca4_row328_col9" class="data row328 col9" >1024.000000</td>
      <td id="T_c6ca4_row328_col10" class="data row328 col10" >torch.float32</td>
      <td id="T_c6ca4_row328_col11" class="data row328 col11" >(1024,)</td>
      <td id="T_c6ca4_row328_col12" class="data row328 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row329" class="row_heading level0 row329" >329</th>
      <td id="T_c6ca4_row329_col0" class="data row329 col0" >encoder.layers.6.final_layer_norm.weight</td>
      <td id="T_c6ca4_row329_col1" class="data row329 col1" >encoder.layers.6.final_layer_norm.weight</td>
      <td id="T_c6ca4_row329_col2" class="data row329 col2" >av_romanizer.w2v_model.encoder.layers.6.final_layer_norm.weight</td>
      <td id="T_c6ca4_row329_col3" class="data row329 col3" >0.499584</td>
      <td id="T_c6ca4_row329_col4" class="data row329 col4" >5.77794</td>
      <td id="T_c6ca4_row329_col5" class="data row329 col5" >5.36492</td>
      <td id="T_c6ca4_row329_col6" class="data row329 col6" >0.0864641</td>
      <td id="T_c6ca4_row329_col7" class="data row329 col7" >0.998726</td>
      <td id="T_c6ca4_row329_col8" class="data row329 col8" >0.0430908</td>
      <td id="T_c6ca4_row329_col9" class="data row329 col9" >1024.000000</td>
      <td id="T_c6ca4_row329_col10" class="data row329 col10" >torch.float32</td>
      <td id="T_c6ca4_row329_col11" class="data row329 col11" >(1024,)</td>
      <td id="T_c6ca4_row329_col12" class="data row329 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row330" class="row_heading level0 row330" >330</th>
      <td id="T_c6ca4_row330_col0" class="data row330 col0" >encoder.layers.6.self_attn.k_proj.bias</td>
      <td id="T_c6ca4_row330_col1" class="data row330 col1" >encoder.layers.6.self_attn.k_proj.bias</td>
      <td id="T_c6ca4_row330_col2" class="data row330 col2" >av_romanizer.w2v_model.encoder.layers.6.self_attn.k_proj.bias</td>
      <td id="T_c6ca4_row330_col3" class="data row330 col3" >0.0168439</td>
      <td id="T_c6ca4_row330_col4" class="data row330 col4" >0.192515</td>
      <td id="T_c6ca4_row330_col5" class="data row330 col5" >0.190746</td>
      <td id="T_c6ca4_row330_col6" class="data row330 col6" >0.0874937</td>
      <td id="T_c6ca4_row330_col7" class="data row330 col7" >0.99618</td>
      <td id="T_c6ca4_row330_col8" class="data row330 col8" >0.00255585</td>
      <td id="T_c6ca4_row330_col9" class="data row330 col9" >1024.000000</td>
      <td id="T_c6ca4_row330_col10" class="data row330 col10" >torch.float32</td>
      <td id="T_c6ca4_row330_col11" class="data row330 col11" >(1024,)</td>
      <td id="T_c6ca4_row330_col12" class="data row330 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row331" class="row_heading level0 row331" >331</th>
      <td id="T_c6ca4_row331_col0" class="data row331 col0" >encoder.layers.6.self_attn.k_proj.weight</td>
      <td id="T_c6ca4_row331_col1" class="data row331 col1" >encoder.layers.6.self_attn.k_proj.weight</td>
      <td id="T_c6ca4_row331_col2" class="data row331 col2" >av_romanizer.w2v_model.encoder.layers.6.self_attn.k_proj.weight</td>
      <td id="T_c6ca4_row331_col3" class="data row331 col3" >21.2064</td>
      <td id="T_c6ca4_row331_col4" class="data row331 col4" >160.437</td>
      <td id="T_c6ca4_row331_col5" class="data row331 col5" >154.865</td>
      <td id="T_c6ca4_row331_col6" class="data row331 col6" >0.132179</td>
      <td id="T_c6ca4_row331_col7" class="data row331 col7" >0.991575</td>
      <td id="T_c6ca4_row331_col8" class="data row331 col8" >0.206421</td>
      <td id="T_c6ca4_row331_col9" class="data row331 col9" >1048576.000000</td>
      <td id="T_c6ca4_row331_col10" class="data row331 col10" >torch.float32</td>
      <td id="T_c6ca4_row331_col11" class="data row331 col11" >(1024, 1024)</td>
      <td id="T_c6ca4_row331_col12" class="data row331 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row332" class="row_heading level0 row332" >332</th>
      <td id="T_c6ca4_row332_col0" class="data row332 col0" >encoder.layers.6.self_attn.out_proj.bias</td>
      <td id="T_c6ca4_row332_col1" class="data row332 col1" >encoder.layers.6.self_attn.out_proj.bias</td>
      <td id="T_c6ca4_row332_col2" class="data row332 col2" >av_romanizer.w2v_model.encoder.layers.6.self_attn.out_proj.bias</td>
      <td id="T_c6ca4_row332_col3" class="data row332 col3" >0.281149</td>
      <td id="T_c6ca4_row332_col4" class="data row332 col4" >3.64053</td>
      <td id="T_c6ca4_row332_col5" class="data row332 col5" >3.44375</td>
      <td id="T_c6ca4_row332_col6" class="data row332 col6" >0.0772275</td>
      <td id="T_c6ca4_row332_col7" class="data row332 col7" >0.998392</td>
      <td id="T_c6ca4_row332_col8" class="data row332 col8" >0.0722656</td>
      <td id="T_c6ca4_row332_col9" class="data row332 col9" >1024.000000</td>
      <td id="T_c6ca4_row332_col10" class="data row332 col10" >torch.float32</td>
      <td id="T_c6ca4_row332_col11" class="data row332 col11" >(1024,)</td>
      <td id="T_c6ca4_row332_col12" class="data row332 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row333" class="row_heading level0 row333" >333</th>
      <td id="T_c6ca4_row333_col0" class="data row333 col0" >encoder.layers.6.self_attn.out_proj.weight</td>
      <td id="T_c6ca4_row333_col1" class="data row333 col1" >encoder.layers.6.self_attn.out_proj.weight</td>
      <td id="T_c6ca4_row333_col2" class="data row333 col2" >av_romanizer.w2v_model.encoder.layers.6.self_attn.out_proj.weight</td>
      <td id="T_c6ca4_row333_col3" class="data row333 col3" >21.0627</td>
      <td id="T_c6ca4_row333_col4" class="data row333 col4" >125.633</td>
      <td id="T_c6ca4_row333_col5" class="data row333 col5" >119.371</td>
      <td id="T_c6ca4_row333_col6" class="data row333 col6" >0.167653</td>
      <td id="T_c6ca4_row333_col7" class="data row333 col7" >0.986516</td>
      <td id="T_c6ca4_row333_col8" class="data row333 col8" >0.125816</td>
      <td id="T_c6ca4_row333_col9" class="data row333 col9" >1048576.000000</td>
      <td id="T_c6ca4_row333_col10" class="data row333 col10" >torch.float32</td>
      <td id="T_c6ca4_row333_col11" class="data row333 col11" >(1024, 1024)</td>
      <td id="T_c6ca4_row333_col12" class="data row333 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row334" class="row_heading level0 row334" >334</th>
      <td id="T_c6ca4_row334_col0" class="data row334 col0" >encoder.layers.6.self_attn.q_proj.bias</td>
      <td id="T_c6ca4_row334_col1" class="data row334 col1" >encoder.layers.6.self_attn.q_proj.bias</td>
      <td id="T_c6ca4_row334_col2" class="data row334 col2" >av_romanizer.w2v_model.encoder.layers.6.self_attn.q_proj.bias</td>
      <td id="T_c6ca4_row334_col3" class="data row334 col3" >0.852498</td>
      <td id="T_c6ca4_row334_col4" class="data row334 col4" >9.64744</td>
      <td id="T_c6ca4_row334_col5" class="data row334 col5" >9.2904</td>
      <td id="T_c6ca4_row334_col6" class="data row334 col6" >0.0883652</td>
      <td id="T_c6ca4_row334_col7" class="data row334 col7" >0.996657</td>
      <td id="T_c6ca4_row334_col8" class="data row334 col8" >0.107666</td>
      <td id="T_c6ca4_row334_col9" class="data row334 col9" >1024.000000</td>
      <td id="T_c6ca4_row334_col10" class="data row334 col10" >torch.float32</td>
      <td id="T_c6ca4_row334_col11" class="data row334 col11" >(1024,)</td>
      <td id="T_c6ca4_row334_col12" class="data row334 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row335" class="row_heading level0 row335" >335</th>
      <td id="T_c6ca4_row335_col0" class="data row335 col0" >encoder.layers.6.self_attn.q_proj.weight</td>
      <td id="T_c6ca4_row335_col1" class="data row335 col1" >encoder.layers.6.self_attn.q_proj.weight</td>
      <td id="T_c6ca4_row335_col2" class="data row335 col2" >av_romanizer.w2v_model.encoder.layers.6.self_attn.q_proj.weight</td>
      <td id="T_c6ca4_row335_col3" class="data row335 col3" >21.6003</td>
      <td id="T_c6ca4_row335_col4" class="data row335 col4" >159.032</td>
      <td id="T_c6ca4_row335_col5" class="data row335 col5" >153.405</td>
      <td id="T_c6ca4_row335_col6" class="data row335 col6" >0.135824</td>
      <td id="T_c6ca4_row335_col7" class="data row335 col7" >0.991087</td>
      <td id="T_c6ca4_row335_col8" class="data row335 col8" >0.224304</td>
      <td id="T_c6ca4_row335_col9" class="data row335 col9" >1048576.000000</td>
      <td id="T_c6ca4_row335_col10" class="data row335 col10" >torch.float32</td>
      <td id="T_c6ca4_row335_col11" class="data row335 col11" >(1024, 1024)</td>
      <td id="T_c6ca4_row335_col12" class="data row335 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row336" class="row_heading level0 row336" >336</th>
      <td id="T_c6ca4_row336_col0" class="data row336 col0" >encoder.layers.6.self_attn.v_proj.bias</td>
      <td id="T_c6ca4_row336_col1" class="data row336 col1" >encoder.layers.6.self_attn.v_proj.bias</td>
      <td id="T_c6ca4_row336_col2" class="data row336 col2" >av_romanizer.w2v_model.encoder.layers.6.self_attn.v_proj.bias</td>
      <td id="T_c6ca4_row336_col3" class="data row336 col3" >0.220513</td>
      <td id="T_c6ca4_row336_col4" class="data row336 col4" >1.24735</td>
      <td id="T_c6ca4_row336_col5" class="data row336 col5" >1.1653</td>
      <td id="T_c6ca4_row336_col6" class="data row336 col6" >0.176785</td>
      <td id="T_c6ca4_row336_col7" class="data row336 col7" >0.985589</td>
      <td id="T_c6ca4_row336_col8" class="data row336 col8" >0.0230713</td>
      <td id="T_c6ca4_row336_col9" class="data row336 col9" >1024.000000</td>
      <td id="T_c6ca4_row336_col10" class="data row336 col10" >torch.float32</td>
      <td id="T_c6ca4_row336_col11" class="data row336 col11" >(1024,)</td>
      <td id="T_c6ca4_row336_col12" class="data row336 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row337" class="row_heading level0 row337" >337</th>
      <td id="T_c6ca4_row337_col0" class="data row337 col0" >encoder.layers.6.self_attn.v_proj.weight</td>
      <td id="T_c6ca4_row337_col1" class="data row337 col1" >encoder.layers.6.self_attn.v_proj.weight</td>
      <td id="T_c6ca4_row337_col2" class="data row337 col2" >av_romanizer.w2v_model.encoder.layers.6.self_attn.v_proj.weight</td>
      <td id="T_c6ca4_row337_col3" class="data row337 col3" >19.5124</td>
      <td id="T_c6ca4_row337_col4" class="data row337 col4" >126.825</td>
      <td id="T_c6ca4_row337_col5" class="data row337 col5" >120.59</td>
      <td id="T_c6ca4_row337_col6" class="data row337 col6" >0.153853</td>
      <td id="T_c6ca4_row337_col7" class="data row337 col7" >0.988823</td>
      <td id="T_c6ca4_row337_col8" class="data row337 col8" >0.110168</td>
      <td id="T_c6ca4_row337_col9" class="data row337 col9" >1048576.000000</td>
      <td id="T_c6ca4_row337_col10" class="data row337 col10" >torch.float32</td>
      <td id="T_c6ca4_row337_col11" class="data row337 col11" >(1024, 1024)</td>
      <td id="T_c6ca4_row337_col12" class="data row337 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row338" class="row_heading level0 row338" >338</th>
      <td id="T_c6ca4_row338_col0" class="data row338 col0" >encoder.layers.6.self_attn_layer_norm.bias</td>
      <td id="T_c6ca4_row338_col1" class="data row338 col1" >encoder.layers.6.self_attn_layer_norm.bias</td>
      <td id="T_c6ca4_row338_col2" class="data row338 col2" >av_romanizer.w2v_model.encoder.layers.6.self_attn_layer_norm.bias</td>
      <td id="T_c6ca4_row338_col3" class="data row338 col3" >0.167762</td>
      <td id="T_c6ca4_row338_col4" class="data row338 col4" >0.595472</td>
      <td id="T_c6ca4_row338_col5" class="data row338 col5" >0.558402</td>
      <td id="T_c6ca4_row338_col6" class="data row338 col6" >0.28173</td>
      <td id="T_c6ca4_row338_col7" class="data row338 col7" >0.959746</td>
      <td id="T_c6ca4_row338_col8" class="data row338 col8" >0.0324097</td>
      <td id="T_c6ca4_row338_col9" class="data row338 col9" >1024.000000</td>
      <td id="T_c6ca4_row338_col10" class="data row338 col10" >torch.float32</td>
      <td id="T_c6ca4_row338_col11" class="data row338 col11" >(1024,)</td>
      <td id="T_c6ca4_row338_col12" class="data row338 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row339" class="row_heading level0 row339" >339</th>
      <td id="T_c6ca4_row339_col0" class="data row339 col0" >encoder.layers.6.self_attn_layer_norm.weight</td>
      <td id="T_c6ca4_row339_col1" class="data row339 col1" >encoder.layers.6.self_attn_layer_norm.weight</td>
      <td id="T_c6ca4_row339_col2" class="data row339 col2" >av_romanizer.w2v_model.encoder.layers.6.self_attn_layer_norm.weight</td>
      <td id="T_c6ca4_row339_col3" class="data row339 col3" >0.316595</td>
      <td id="T_c6ca4_row339_col4" class="data row339 col4" >5.39423</td>
      <td id="T_c6ca4_row339_col5" class="data row339 col5" >5.32398</td>
      <td id="T_c6ca4_row339_col6" class="data row339 col6" >0.0586913</td>
      <td id="T_c6ca4_row339_col7" class="data row339 col7" >0.998341</td>
      <td id="T_c6ca4_row339_col8" class="data row339 col8" >0.0482178</td>
      <td id="T_c6ca4_row339_col9" class="data row339 col9" >1024.000000</td>
      <td id="T_c6ca4_row339_col10" class="data row339 col10" >torch.float32</td>
      <td id="T_c6ca4_row339_col11" class="data row339 col11" >(1024,)</td>
      <td id="T_c6ca4_row339_col12" class="data row339 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row340" class="row_heading level0 row340" >340</th>
      <td id="T_c6ca4_row340_col0" class="data row340 col0" >encoder.layers.7.fc1.bias</td>
      <td id="T_c6ca4_row340_col1" class="data row340 col1" >encoder.layers.7.fc1.bias</td>
      <td id="T_c6ca4_row340_col2" class="data row340 col2" >av_romanizer.w2v_model.encoder.layers.7.fc1.bias</td>
      <td id="T_c6ca4_row340_col3" class="data row340 col3" >0.655494</td>
      <td id="T_c6ca4_row340_col4" class="data row340 col4" >3.15371</td>
      <td id="T_c6ca4_row340_col5" class="data row340 col5" >3.12518</td>
      <td id="T_c6ca4_row340_col6" class="data row340 col6" >0.207849</td>
      <td id="T_c6ca4_row340_col7" class="data row340 col7" >0.978244</td>
      <td id="T_c6ca4_row340_col8" class="data row340 col8" >0.0783005</td>
      <td id="T_c6ca4_row340_col9" class="data row340 col9" >4096.000000</td>
      <td id="T_c6ca4_row340_col10" class="data row340 col10" >torch.float32</td>
      <td id="T_c6ca4_row340_col11" class="data row340 col11" >(4096,)</td>
      <td id="T_c6ca4_row340_col12" class="data row340 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row341" class="row_heading level0 row341" >341</th>
      <td id="T_c6ca4_row341_col0" class="data row341 col0" >encoder.layers.7.fc1.weight</td>
      <td id="T_c6ca4_row341_col1" class="data row341 col1" >encoder.layers.7.fc1.weight</td>
      <td id="T_c6ca4_row341_col2" class="data row341 col2" >av_romanizer.w2v_model.encoder.layers.7.fc1.weight</td>
      <td id="T_c6ca4_row341_col3" class="data row341 col3" >47.6631</td>
      <td id="T_c6ca4_row341_col4" class="data row341 col4" >299.444</td>
      <td id="T_c6ca4_row341_col5" class="data row341 col5" >286.106</td>
      <td id="T_c6ca4_row341_col6" class="data row341 col6" >0.159172</td>
      <td id="T_c6ca4_row341_col7" class="data row341 col7" >0.98778</td>
      <td id="T_c6ca4_row341_col8" class="data row341 col8" >0.167114</td>
      <td id="T_c6ca4_row341_col9" class="data row341 col9" >4194304.000000</td>
      <td id="T_c6ca4_row341_col10" class="data row341 col10" >torch.float32</td>
      <td id="T_c6ca4_row341_col11" class="data row341 col11" >(4096, 1024)</td>
      <td id="T_c6ca4_row341_col12" class="data row341 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row342" class="row_heading level0 row342" >342</th>
      <td id="T_c6ca4_row342_col0" class="data row342 col0" >encoder.layers.7.fc2.bias</td>
      <td id="T_c6ca4_row342_col1" class="data row342 col1" >encoder.layers.7.fc2.bias</td>
      <td id="T_c6ca4_row342_col2" class="data row342 col2" >av_romanizer.w2v_model.encoder.layers.7.fc2.bias</td>
      <td id="T_c6ca4_row342_col3" class="data row342 col3" >0.179871</td>
      <td id="T_c6ca4_row342_col4" class="data row342 col4" >3.01579</td>
      <td id="T_c6ca4_row342_col5" class="data row342 col5" >2.87163</td>
      <td id="T_c6ca4_row342_col6" class="data row342 col6" >0.0596431</td>
      <td id="T_c6ca4_row342_col7" class="data row342 col7" >0.999332</td>
      <td id="T_c6ca4_row342_col8" class="data row342 col8" >0.0444336</td>
      <td id="T_c6ca4_row342_col9" class="data row342 col9" >1024.000000</td>
      <td id="T_c6ca4_row342_col10" class="data row342 col10" >torch.float32</td>
      <td id="T_c6ca4_row342_col11" class="data row342 col11" >(1024,)</td>
      <td id="T_c6ca4_row342_col12" class="data row342 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row343" class="row_heading level0 row343" >343</th>
      <td id="T_c6ca4_row343_col0" class="data row343 col0" >encoder.layers.7.fc2.weight</td>
      <td id="T_c6ca4_row343_col1" class="data row343 col1" >encoder.layers.7.fc2.weight</td>
      <td id="T_c6ca4_row343_col2" class="data row343 col2" >av_romanizer.w2v_model.encoder.layers.7.fc2.weight</td>
      <td id="T_c6ca4_row343_col3" class="data row343 col3" >43.6298</td>
      <td id="T_c6ca4_row343_col4" class="data row343 col4" >289.806</td>
      <td id="T_c6ca4_row343_col5" class="data row343 col5" >275.691</td>
      <td id="T_c6ca4_row343_col6" class="data row343 col6" >0.150548</td>
      <td id="T_c6ca4_row343_col7" class="data row343 col7" >0.989334</td>
      <td id="T_c6ca4_row343_col8" class="data row343 col8" >0.186768</td>
      <td id="T_c6ca4_row343_col9" class="data row343 col9" >4194304.000000</td>
      <td id="T_c6ca4_row343_col10" class="data row343 col10" >torch.float32</td>
      <td id="T_c6ca4_row343_col11" class="data row343 col11" >(1024, 4096)</td>
      <td id="T_c6ca4_row343_col12" class="data row343 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row344" class="row_heading level0 row344" >344</th>
      <td id="T_c6ca4_row344_col0" class="data row344 col0" >encoder.layers.7.final_layer_norm.bias</td>
      <td id="T_c6ca4_row344_col1" class="data row344 col1" >encoder.layers.7.final_layer_norm.bias</td>
      <td id="T_c6ca4_row344_col2" class="data row344 col2" >av_romanizer.w2v_model.encoder.layers.7.final_layer_norm.bias</td>
      <td id="T_c6ca4_row344_col3" class="data row344 col3" >0.193832</td>
      <td id="T_c6ca4_row344_col4" class="data row344 col4" >2.09645</td>
      <td id="T_c6ca4_row344_col5" class="data row344 col5" >1.99948</td>
      <td id="T_c6ca4_row344_col6" class="data row344 col6" >0.0924573</td>
      <td id="T_c6ca4_row344_col7" class="data row344 col7" >0.99664</td>
      <td id="T_c6ca4_row344_col8" class="data row344 col8" >0.0300903</td>
      <td id="T_c6ca4_row344_col9" class="data row344 col9" >1024.000000</td>
      <td id="T_c6ca4_row344_col10" class="data row344 col10" >torch.float32</td>
      <td id="T_c6ca4_row344_col11" class="data row344 col11" >(1024,)</td>
      <td id="T_c6ca4_row344_col12" class="data row344 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row345" class="row_heading level0 row345" >345</th>
      <td id="T_c6ca4_row345_col0" class="data row345 col0" >encoder.layers.7.final_layer_norm.weight</td>
      <td id="T_c6ca4_row345_col1" class="data row345 col1" >encoder.layers.7.final_layer_norm.weight</td>
      <td id="T_c6ca4_row345_col2" class="data row345 col2" >av_romanizer.w2v_model.encoder.layers.7.final_layer_norm.weight</td>
      <td id="T_c6ca4_row345_col3" class="data row345 col3" >0.581098</td>
      <td id="T_c6ca4_row345_col4" class="data row345 col4" >6.13151</td>
      <td id="T_c6ca4_row345_col5" class="data row345 col5" >5.63606</td>
      <td id="T_c6ca4_row345_col6" class="data row345 col6" >0.0947725</td>
      <td id="T_c6ca4_row345_col7" class="data row345 col7" >0.998666</td>
      <td id="T_c6ca4_row345_col8" class="data row345 col8" >0.0465088</td>
      <td id="T_c6ca4_row345_col9" class="data row345 col9" >1024.000000</td>
      <td id="T_c6ca4_row345_col10" class="data row345 col10" >torch.float32</td>
      <td id="T_c6ca4_row345_col11" class="data row345 col11" >(1024,)</td>
      <td id="T_c6ca4_row345_col12" class="data row345 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row346" class="row_heading level0 row346" >346</th>
      <td id="T_c6ca4_row346_col0" class="data row346 col0" >encoder.layers.7.self_attn.k_proj.bias</td>
      <td id="T_c6ca4_row346_col1" class="data row346 col1" >encoder.layers.7.self_attn.k_proj.bias</td>
      <td id="T_c6ca4_row346_col2" class="data row346 col2" >av_romanizer.w2v_model.encoder.layers.7.self_attn.k_proj.bias</td>
      <td id="T_c6ca4_row346_col3" class="data row346 col3" >0.0170856</td>
      <td id="T_c6ca4_row346_col4" class="data row346 col4" >0.121389</td>
      <td id="T_c6ca4_row346_col5" class="data row346 col5" >0.116956</td>
      <td id="T_c6ca4_row346_col6" class="data row346 col6" >0.14075</td>
      <td id="T_c6ca4_row346_col7" class="data row346 col7" >0.990411</td>
      <td id="T_c6ca4_row346_col8" class="data row346 col8" >0.00265694</td>
      <td id="T_c6ca4_row346_col9" class="data row346 col9" >1024.000000</td>
      <td id="T_c6ca4_row346_col10" class="data row346 col10" >torch.float32</td>
      <td id="T_c6ca4_row346_col11" class="data row346 col11" >(1024,)</td>
      <td id="T_c6ca4_row346_col12" class="data row346 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row347" class="row_heading level0 row347" >347</th>
      <td id="T_c6ca4_row347_col0" class="data row347 col0" >encoder.layers.7.self_attn.k_proj.weight</td>
      <td id="T_c6ca4_row347_col1" class="data row347 col1" >encoder.layers.7.self_attn.k_proj.weight</td>
      <td id="T_c6ca4_row347_col2" class="data row347 col2" >av_romanizer.w2v_model.encoder.layers.7.self_attn.k_proj.weight</td>
      <td id="T_c6ca4_row347_col3" class="data row347 col3" >22.7644</td>
      <td id="T_c6ca4_row347_col4" class="data row347 col4" >152.297</td>
      <td id="T_c6ca4_row347_col5" class="data row347 col5" >147.171</td>
      <td id="T_c6ca4_row347_col6" class="data row347 col6" >0.149474</td>
      <td id="T_c6ca4_row347_col7" class="data row347 col7" >0.989026</td>
      <td id="T_c6ca4_row347_col8" class="data row347 col8" >0.223145</td>
      <td id="T_c6ca4_row347_col9" class="data row347 col9" >1048576.000000</td>
      <td id="T_c6ca4_row347_col10" class="data row347 col10" >torch.float32</td>
      <td id="T_c6ca4_row347_col11" class="data row347 col11" >(1024, 1024)</td>
      <td id="T_c6ca4_row347_col12" class="data row347 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row348" class="row_heading level0 row348" >348</th>
      <td id="T_c6ca4_row348_col0" class="data row348 col0" >encoder.layers.7.self_attn.out_proj.bias</td>
      <td id="T_c6ca4_row348_col1" class="data row348 col1" >encoder.layers.7.self_attn.out_proj.bias</td>
      <td id="T_c6ca4_row348_col2" class="data row348 col2" >av_romanizer.w2v_model.encoder.layers.7.self_attn.out_proj.bias</td>
      <td id="T_c6ca4_row348_col3" class="data row348 col3" >0.276692</td>
      <td id="T_c6ca4_row348_col4" class="data row348 col4" >3.48962</td>
      <td id="T_c6ca4_row348_col5" class="data row348 col5" >3.30016</td>
      <td id="T_c6ca4_row348_col6" class="data row348 col6" >0.07929</td>
      <td id="T_c6ca4_row348_col7" class="data row348 col7" >0.998234</td>
      <td id="T_c6ca4_row348_col8" class="data row348 col8" >0.0800781</td>
      <td id="T_c6ca4_row348_col9" class="data row348 col9" >1024.000000</td>
      <td id="T_c6ca4_row348_col10" class="data row348 col10" >torch.float32</td>
      <td id="T_c6ca4_row348_col11" class="data row348 col11" >(1024,)</td>
      <td id="T_c6ca4_row348_col12" class="data row348 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row349" class="row_heading level0 row349" >349</th>
      <td id="T_c6ca4_row349_col0" class="data row349 col0" >encoder.layers.7.self_attn.out_proj.weight</td>
      <td id="T_c6ca4_row349_col1" class="data row349 col1" >encoder.layers.7.self_attn.out_proj.weight</td>
      <td id="T_c6ca4_row349_col2" class="data row349 col2" >av_romanizer.w2v_model.encoder.layers.7.self_attn.out_proj.weight</td>
      <td id="T_c6ca4_row349_col3" class="data row349 col3" >22.0123</td>
      <td id="T_c6ca4_row349_col4" class="data row349 col4" >129.668</td>
      <td id="T_c6ca4_row349_col5" class="data row349 col5" >122.945</td>
      <td id="T_c6ca4_row349_col6" class="data row349 col6" >0.169759</td>
      <td id="T_c6ca4_row349_col7" class="data row349 col7" >0.98622</td>
      <td id="T_c6ca4_row349_col8" class="data row349 col8" >0.114594</td>
      <td id="T_c6ca4_row349_col9" class="data row349 col9" >1048576.000000</td>
      <td id="T_c6ca4_row349_col10" class="data row349 col10" >torch.float32</td>
      <td id="T_c6ca4_row349_col11" class="data row349 col11" >(1024, 1024)</td>
      <td id="T_c6ca4_row349_col12" class="data row349 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row350" class="row_heading level0 row350" >350</th>
      <td id="T_c6ca4_row350_col0" class="data row350 col0" >encoder.layers.7.self_attn.q_proj.bias</td>
      <td id="T_c6ca4_row350_col1" class="data row350 col1" >encoder.layers.7.self_attn.q_proj.bias</td>
      <td id="T_c6ca4_row350_col2" class="data row350 col2" >av_romanizer.w2v_model.encoder.layers.7.self_attn.q_proj.bias</td>
      <td id="T_c6ca4_row350_col3" class="data row350 col3" >0.810758</td>
      <td id="T_c6ca4_row350_col4" class="data row350 col4" >9.25898</td>
      <td id="T_c6ca4_row350_col5" class="data row350 col5" >9.00841</td>
      <td id="T_c6ca4_row350_col6" class="data row350 col6" >0.0875644</td>
      <td id="T_c6ca4_row350_col7" class="data row350 col7" >0.996436</td>
      <td id="T_c6ca4_row350_col8" class="data row350 col8" >0.118164</td>
      <td id="T_c6ca4_row350_col9" class="data row350 col9" >1024.000000</td>
      <td id="T_c6ca4_row350_col10" class="data row350 col10" >torch.float32</td>
      <td id="T_c6ca4_row350_col11" class="data row350 col11" >(1024,)</td>
      <td id="T_c6ca4_row350_col12" class="data row350 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row351" class="row_heading level0 row351" >351</th>
      <td id="T_c6ca4_row351_col0" class="data row351 col0" >encoder.layers.7.self_attn.q_proj.weight</td>
      <td id="T_c6ca4_row351_col1" class="data row351 col1" >encoder.layers.7.self_attn.q_proj.weight</td>
      <td id="T_c6ca4_row351_col2" class="data row351 col2" >av_romanizer.w2v_model.encoder.layers.7.self_attn.q_proj.weight</td>
      <td id="T_c6ca4_row351_col3" class="data row351 col3" >23.3058</td>
      <td id="T_c6ca4_row351_col4" class="data row351 col4" >151.28</td>
      <td id="T_c6ca4_row351_col5" class="data row351 col5" >146.237</td>
      <td id="T_c6ca4_row351_col6" class="data row351 col6" >0.154058</td>
      <td id="T_c6ca4_row351_col7" class="data row351 col7" >0.988299</td>
      <td id="T_c6ca4_row351_col8" class="data row351 col8" >0.181152</td>
      <td id="T_c6ca4_row351_col9" class="data row351 col9" >1048576.000000</td>
      <td id="T_c6ca4_row351_col10" class="data row351 col10" >torch.float32</td>
      <td id="T_c6ca4_row351_col11" class="data row351 col11" >(1024, 1024)</td>
      <td id="T_c6ca4_row351_col12" class="data row351 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row352" class="row_heading level0 row352" >352</th>
      <td id="T_c6ca4_row352_col0" class="data row352 col0" >encoder.layers.7.self_attn.v_proj.bias</td>
      <td id="T_c6ca4_row352_col1" class="data row352 col1" >encoder.layers.7.self_attn.v_proj.bias</td>
      <td id="T_c6ca4_row352_col2" class="data row352 col2" >av_romanizer.w2v_model.encoder.layers.7.self_attn.v_proj.bias</td>
      <td id="T_c6ca4_row352_col3" class="data row352 col3" >0.225594</td>
      <td id="T_c6ca4_row352_col4" class="data row352 col4" >1.36456</td>
      <td id="T_c6ca4_row352_col5" class="data row352 col5" >1.26157</td>
      <td id="T_c6ca4_row352_col6" class="data row352 col6" >0.165323</td>
      <td id="T_c6ca4_row352_col7" class="data row352 col7" >0.988299</td>
      <td id="T_c6ca4_row352_col8" class="data row352 col8" >0.0351562</td>
      <td id="T_c6ca4_row352_col9" class="data row352 col9" >1024.000000</td>
      <td id="T_c6ca4_row352_col10" class="data row352 col10" >torch.float32</td>
      <td id="T_c6ca4_row352_col11" class="data row352 col11" >(1024,)</td>
      <td id="T_c6ca4_row352_col12" class="data row352 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row353" class="row_heading level0 row353" >353</th>
      <td id="T_c6ca4_row353_col0" class="data row353 col0" >encoder.layers.7.self_attn.v_proj.weight</td>
      <td id="T_c6ca4_row353_col1" class="data row353 col1" >encoder.layers.7.self_attn.v_proj.weight</td>
      <td id="T_c6ca4_row353_col2" class="data row353 col2" >av_romanizer.w2v_model.encoder.layers.7.self_attn.v_proj.weight</td>
      <td id="T_c6ca4_row353_col3" class="data row353 col3" >20.269</td>
      <td id="T_c6ca4_row353_col4" class="data row353 col4" >132.521</td>
      <td id="T_c6ca4_row353_col5" class="data row353 col5" >125.826</td>
      <td id="T_c6ca4_row353_col6" class="data row353 col6" >0.152949</td>
      <td id="T_c6ca4_row353_col7" class="data row353 col7" >0.989025</td>
      <td id="T_c6ca4_row353_col8" class="data row353 col8" >0.106323</td>
      <td id="T_c6ca4_row353_col9" class="data row353 col9" >1048576.000000</td>
      <td id="T_c6ca4_row353_col10" class="data row353 col10" >torch.float32</td>
      <td id="T_c6ca4_row353_col11" class="data row353 col11" >(1024, 1024)</td>
      <td id="T_c6ca4_row353_col12" class="data row353 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row354" class="row_heading level0 row354" >354</th>
      <td id="T_c6ca4_row354_col0" class="data row354 col0" >encoder.layers.7.self_attn_layer_norm.bias</td>
      <td id="T_c6ca4_row354_col1" class="data row354 col1" >encoder.layers.7.self_attn_layer_norm.bias</td>
      <td id="T_c6ca4_row354_col2" class="data row354 col2" >av_romanizer.w2v_model.encoder.layers.7.self_attn_layer_norm.bias</td>
      <td id="T_c6ca4_row354_col3" class="data row354 col3" >0.166185</td>
      <td id="T_c6ca4_row354_col4" class="data row354 col4" >0.795596</td>
      <td id="T_c6ca4_row354_col5" class="data row354 col5" >0.781529</td>
      <td id="T_c6ca4_row354_col6" class="data row354 col6" >0.208881</td>
      <td id="T_c6ca4_row354_col7" class="data row354 col7" >0.977951</td>
      <td id="T_c6ca4_row354_col8" class="data row354 col8" >0.0306549</td>
      <td id="T_c6ca4_row354_col9" class="data row354 col9" >1024.000000</td>
      <td id="T_c6ca4_row354_col10" class="data row354 col10" >torch.float32</td>
      <td id="T_c6ca4_row354_col11" class="data row354 col11" >(1024,)</td>
      <td id="T_c6ca4_row354_col12" class="data row354 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row355" class="row_heading level0 row355" >355</th>
      <td id="T_c6ca4_row355_col0" class="data row355 col0" >encoder.layers.7.self_attn_layer_norm.weight</td>
      <td id="T_c6ca4_row355_col1" class="data row355 col1" >encoder.layers.7.self_attn_layer_norm.weight</td>
      <td id="T_c6ca4_row355_col2" class="data row355 col2" >av_romanizer.w2v_model.encoder.layers.7.self_attn_layer_norm.weight</td>
      <td id="T_c6ca4_row355_col3" class="data row355 col3" >0.297255</td>
      <td id="T_c6ca4_row355_col4" class="data row355 col4" >5.96077</td>
      <td id="T_c6ca4_row355_col5" class="data row355 col5" >5.96347</td>
      <td id="T_c6ca4_row355_col6" class="data row355 col6" >0.0498686</td>
      <td id="T_c6ca4_row355_col7" class="data row355 col7" >0.998757</td>
      <td id="T_c6ca4_row355_col8" class="data row355 col8" >0.0405273</td>
      <td id="T_c6ca4_row355_col9" class="data row355 col9" >1024.000000</td>
      <td id="T_c6ca4_row355_col10" class="data row355 col10" >torch.float32</td>
      <td id="T_c6ca4_row355_col11" class="data row355 col11" >(1024,)</td>
      <td id="T_c6ca4_row355_col12" class="data row355 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row356" class="row_heading level0 row356" >356</th>
      <td id="T_c6ca4_row356_col0" class="data row356 col0" >encoder.layers.8.fc1.bias</td>
      <td id="T_c6ca4_row356_col1" class="data row356 col1" >encoder.layers.8.fc1.bias</td>
      <td id="T_c6ca4_row356_col2" class="data row356 col2" >av_romanizer.w2v_model.encoder.layers.8.fc1.bias</td>
      <td id="T_c6ca4_row356_col3" class="data row356 col3" >0.633011</td>
      <td id="T_c6ca4_row356_col4" class="data row356 col4" >3.33843</td>
      <td id="T_c6ca4_row356_col5" class="data row356 col5" >3.26849</td>
      <td id="T_c6ca4_row356_col6" class="data row356 col6" >0.189613</td>
      <td id="T_c6ca4_row356_col7" class="data row356 col7" >0.981863</td>
      <td id="T_c6ca4_row356_col8" class="data row356 col8" >0.0841322</td>
      <td id="T_c6ca4_row356_col9" class="data row356 col9" >4096.000000</td>
      <td id="T_c6ca4_row356_col10" class="data row356 col10" >torch.float32</td>
      <td id="T_c6ca4_row356_col11" class="data row356 col11" >(4096,)</td>
      <td id="T_c6ca4_row356_col12" class="data row356 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row357" class="row_heading level0 row357" >357</th>
      <td id="T_c6ca4_row357_col0" class="data row357 col0" >encoder.layers.8.fc1.weight</td>
      <td id="T_c6ca4_row357_col1" class="data row357 col1" >encoder.layers.8.fc1.weight</td>
      <td id="T_c6ca4_row357_col2" class="data row357 col2" >av_romanizer.w2v_model.encoder.layers.8.fc1.weight</td>
      <td id="T_c6ca4_row357_col3" class="data row357 col3" >50.029</td>
      <td id="T_c6ca4_row357_col4" class="data row357 col4" >300.384</td>
      <td id="T_c6ca4_row357_col5" class="data row357 col5" >286.991</td>
      <td id="T_c6ca4_row357_col6" class="data row357 col6" >0.16655</td>
      <td id="T_c6ca4_row357_col7" class="data row357 col7" >0.986524</td>
      <td id="T_c6ca4_row357_col8" class="data row357 col8" >0.175247</td>
      <td id="T_c6ca4_row357_col9" class="data row357 col9" >4194304.000000</td>
      <td id="T_c6ca4_row357_col10" class="data row357 col10" >torch.float32</td>
      <td id="T_c6ca4_row357_col11" class="data row357 col11" >(4096, 1024)</td>
      <td id="T_c6ca4_row357_col12" class="data row357 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row358" class="row_heading level0 row358" >358</th>
      <td id="T_c6ca4_row358_col0" class="data row358 col0" >encoder.layers.8.fc2.bias</td>
      <td id="T_c6ca4_row358_col1" class="data row358 col1" >encoder.layers.8.fc2.bias</td>
      <td id="T_c6ca4_row358_col2" class="data row358 col2" >av_romanizer.w2v_model.encoder.layers.8.fc2.bias</td>
      <td id="T_c6ca4_row358_col3" class="data row358 col3" >0.181776</td>
      <td id="T_c6ca4_row358_col4" class="data row358 col4" >2.82162</td>
      <td id="T_c6ca4_row358_col5" class="data row358 col5" >2.68314</td>
      <td id="T_c6ca4_row358_col6" class="data row358 col6" >0.0644225</td>
      <td id="T_c6ca4_row358_col7" class="data row358 col7" >0.999084</td>
      <td id="T_c6ca4_row358_col8" class="data row358 col8" >0.0349121</td>
      <td id="T_c6ca4_row358_col9" class="data row358 col9" >1024.000000</td>
      <td id="T_c6ca4_row358_col10" class="data row358 col10" >torch.float32</td>
      <td id="T_c6ca4_row358_col11" class="data row358 col11" >(1024,)</td>
      <td id="T_c6ca4_row358_col12" class="data row358 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row359" class="row_heading level0 row359" >359</th>
      <td id="T_c6ca4_row359_col0" class="data row359 col0" >encoder.layers.8.fc2.weight</td>
      <td id="T_c6ca4_row359_col1" class="data row359 col1" >encoder.layers.8.fc2.weight</td>
      <td id="T_c6ca4_row359_col2" class="data row359 col2" >av_romanizer.w2v_model.encoder.layers.8.fc2.weight</td>
      <td id="T_c6ca4_row359_col3" class="data row359 col3" >46.3899</td>
      <td id="T_c6ca4_row359_col4" class="data row359 col4" >295.639</td>
      <td id="T_c6ca4_row359_col5" class="data row359 col5" >281.464</td>
      <td id="T_c6ca4_row359_col6" class="data row359 col6" >0.156914</td>
      <td id="T_c6ca4_row359_col7" class="data row359 col7" >0.988276</td>
      <td id="T_c6ca4_row359_col8" class="data row359 col8" >0.156342</td>
      <td id="T_c6ca4_row359_col9" class="data row359 col9" >4194304.000000</td>
      <td id="T_c6ca4_row359_col10" class="data row359 col10" >torch.float32</td>
      <td id="T_c6ca4_row359_col11" class="data row359 col11" >(1024, 4096)</td>
      <td id="T_c6ca4_row359_col12" class="data row359 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row360" class="row_heading level0 row360" >360</th>
      <td id="T_c6ca4_row360_col0" class="data row360 col0" >encoder.layers.8.final_layer_norm.bias</td>
      <td id="T_c6ca4_row360_col1" class="data row360 col1" >encoder.layers.8.final_layer_norm.bias</td>
      <td id="T_c6ca4_row360_col2" class="data row360 col2" >av_romanizer.w2v_model.encoder.layers.8.final_layer_norm.bias</td>
      <td id="T_c6ca4_row360_col3" class="data row360 col3" >0.205581</td>
      <td id="T_c6ca4_row360_col4" class="data row360 col4" >1.9293</td>
      <td id="T_c6ca4_row360_col5" class="data row360 col5" >1.84093</td>
      <td id="T_c6ca4_row360_col6" class="data row360 col6" >0.106557</td>
      <td id="T_c6ca4_row360_col7" class="data row360 col7" >0.99515</td>
      <td id="T_c6ca4_row360_col8" class="data row360 col8" >0.0441895</td>
      <td id="T_c6ca4_row360_col9" class="data row360 col9" >1024.000000</td>
      <td id="T_c6ca4_row360_col10" class="data row360 col10" >torch.float32</td>
      <td id="T_c6ca4_row360_col11" class="data row360 col11" >(1024,)</td>
      <td id="T_c6ca4_row360_col12" class="data row360 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row361" class="row_heading level0 row361" >361</th>
      <td id="T_c6ca4_row361_col0" class="data row361 col0" >encoder.layers.8.final_layer_norm.weight</td>
      <td id="T_c6ca4_row361_col1" class="data row361 col1" >encoder.layers.8.final_layer_norm.weight</td>
      <td id="T_c6ca4_row361_col2" class="data row361 col2" >av_romanizer.w2v_model.encoder.layers.8.final_layer_norm.weight</td>
      <td id="T_c6ca4_row361_col3" class="data row361 col3" >0.575196</td>
      <td id="T_c6ca4_row361_col4" class="data row361 col4" >6.35985</td>
      <td id="T_c6ca4_row361_col5" class="data row361 col5" >5.86874</td>
      <td id="T_c6ca4_row361_col6" class="data row361 col6" >0.0904418</td>
      <td id="T_c6ca4_row361_col7" class="data row361 col7" >0.998799</td>
      <td id="T_c6ca4_row361_col8" class="data row361 col8" >0.0456543</td>
      <td id="T_c6ca4_row361_col9" class="data row361 col9" >1024.000000</td>
      <td id="T_c6ca4_row361_col10" class="data row361 col10" >torch.float32</td>
      <td id="T_c6ca4_row361_col11" class="data row361 col11" >(1024,)</td>
      <td id="T_c6ca4_row361_col12" class="data row361 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row362" class="row_heading level0 row362" >362</th>
      <td id="T_c6ca4_row362_col0" class="data row362 col0" >encoder.layers.8.self_attn.k_proj.bias</td>
      <td id="T_c6ca4_row362_col1" class="data row362 col1" >encoder.layers.8.self_attn.k_proj.bias</td>
      <td id="T_c6ca4_row362_col2" class="data row362 col2" >av_romanizer.w2v_model.encoder.layers.8.self_attn.k_proj.bias</td>
      <td id="T_c6ca4_row362_col3" class="data row362 col3" >0.0195423</td>
      <td id="T_c6ca4_row362_col4" class="data row362 col4" >0.381358</td>
      <td id="T_c6ca4_row362_col5" class="data row362 col5" >0.381259</td>
      <td id="T_c6ca4_row362_col6" class="data row362 col6" >0.0512438</td>
      <td id="T_c6ca4_row362_col7" class="data row362 col7" >0.998687</td>
      <td id="T_c6ca4_row362_col8" class="data row362 col8" >0.00398254</td>
      <td id="T_c6ca4_row362_col9" class="data row362 col9" >1024.000000</td>
      <td id="T_c6ca4_row362_col10" class="data row362 col10" >torch.float32</td>
      <td id="T_c6ca4_row362_col11" class="data row362 col11" >(1024,)</td>
      <td id="T_c6ca4_row362_col12" class="data row362 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row363" class="row_heading level0 row363" >363</th>
      <td id="T_c6ca4_row363_col0" class="data row363 col0" >encoder.layers.8.self_attn.k_proj.weight</td>
      <td id="T_c6ca4_row363_col1" class="data row363 col1" >encoder.layers.8.self_attn.k_proj.weight</td>
      <td id="T_c6ca4_row363_col2" class="data row363 col2" >av_romanizer.w2v_model.encoder.layers.8.self_attn.k_proj.weight</td>
      <td id="T_c6ca4_row363_col3" class="data row363 col3" >22.2376</td>
      <td id="T_c6ca4_row363_col4" class="data row363 col4" >153.198</td>
      <td id="T_c6ca4_row363_col5" class="data row363 col5" >147.69</td>
      <td id="T_c6ca4_row363_col6" class="data row363 col6" >0.145155</td>
      <td id="T_c6ca4_row363_col7" class="data row363 col7" >0.989743</td>
      <td id="T_c6ca4_row363_col8" class="data row363 col8" >0.198578</td>
      <td id="T_c6ca4_row363_col9" class="data row363 col9" >1048576.000000</td>
      <td id="T_c6ca4_row363_col10" class="data row363 col10" >torch.float32</td>
      <td id="T_c6ca4_row363_col11" class="data row363 col11" >(1024, 1024)</td>
      <td id="T_c6ca4_row363_col12" class="data row363 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row364" class="row_heading level0 row364" >364</th>
      <td id="T_c6ca4_row364_col0" class="data row364 col0" >encoder.layers.8.self_attn.out_proj.bias</td>
      <td id="T_c6ca4_row364_col1" class="data row364 col1" >encoder.layers.8.self_attn.out_proj.bias</td>
      <td id="T_c6ca4_row364_col2" class="data row364 col2" >av_romanizer.w2v_model.encoder.layers.8.self_attn.out_proj.bias</td>
      <td id="T_c6ca4_row364_col3" class="data row364 col3" >0.251494</td>
      <td id="T_c6ca4_row364_col4" class="data row364 col4" >3.32042</td>
      <td id="T_c6ca4_row364_col5" class="data row364 col5" >3.16587</td>
      <td id="T_c6ca4_row364_col6" class="data row364 col6" >0.0757417</td>
      <td id="T_c6ca4_row364_col7" class="data row364 col7" >0.998128</td>
      <td id="T_c6ca4_row364_col8" class="data row364 col8" >0.0385742</td>
      <td id="T_c6ca4_row364_col9" class="data row364 col9" >1024.000000</td>
      <td id="T_c6ca4_row364_col10" class="data row364 col10" >torch.float32</td>
      <td id="T_c6ca4_row364_col11" class="data row364 col11" >(1024,)</td>
      <td id="T_c6ca4_row364_col12" class="data row364 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row365" class="row_heading level0 row365" >365</th>
      <td id="T_c6ca4_row365_col0" class="data row365 col0" >encoder.layers.8.self_attn.out_proj.weight</td>
      <td id="T_c6ca4_row365_col1" class="data row365 col1" >encoder.layers.8.self_attn.out_proj.weight</td>
      <td id="T_c6ca4_row365_col2" class="data row365 col2" >av_romanizer.w2v_model.encoder.layers.8.self_attn.out_proj.weight</td>
      <td id="T_c6ca4_row365_col3" class="data row365 col3" >23.7494</td>
      <td id="T_c6ca4_row365_col4" class="data row365 col4" >133.98</td>
      <td id="T_c6ca4_row365_col5" class="data row365 col5" >127.458</td>
      <td id="T_c6ca4_row365_col6" class="data row365 col6" >0.17726</td>
      <td id="T_c6ca4_row365_col7" class="data row365 col7" >0.984731</td>
      <td id="T_c6ca4_row365_col8" class="data row365 col8" >0.157227</td>
      <td id="T_c6ca4_row365_col9" class="data row365 col9" >1048576.000000</td>
      <td id="T_c6ca4_row365_col10" class="data row365 col10" >torch.float32</td>
      <td id="T_c6ca4_row365_col11" class="data row365 col11" >(1024, 1024)</td>
      <td id="T_c6ca4_row365_col12" class="data row365 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row366" class="row_heading level0 row366" >366</th>
      <td id="T_c6ca4_row366_col0" class="data row366 col0" >encoder.layers.8.self_attn.q_proj.bias</td>
      <td id="T_c6ca4_row366_col1" class="data row366 col1" >encoder.layers.8.self_attn.q_proj.bias</td>
      <td id="T_c6ca4_row366_col2" class="data row366 col2" >av_romanizer.w2v_model.encoder.layers.8.self_attn.q_proj.bias</td>
      <td id="T_c6ca4_row366_col3" class="data row366 col3" >0.909336</td>
      <td id="T_c6ca4_row366_col4" class="data row366 col4" >10.3034</td>
      <td id="T_c6ca4_row366_col5" class="data row366 col5" >9.97972</td>
      <td id="T_c6ca4_row366_col6" class="data row366 col6" >0.0882555</td>
      <td id="T_c6ca4_row366_col7" class="data row366 col7" >0.996489</td>
      <td id="T_c6ca4_row366_col8" class="data row366 col8" >0.115234</td>
      <td id="T_c6ca4_row366_col9" class="data row366 col9" >1024.000000</td>
      <td id="T_c6ca4_row366_col10" class="data row366 col10" >torch.float32</td>
      <td id="T_c6ca4_row366_col11" class="data row366 col11" >(1024,)</td>
      <td id="T_c6ca4_row366_col12" class="data row366 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row367" class="row_heading level0 row367" >367</th>
      <td id="T_c6ca4_row367_col0" class="data row367 col0" >encoder.layers.8.self_attn.q_proj.weight</td>
      <td id="T_c6ca4_row367_col1" class="data row367 col1" >encoder.layers.8.self_attn.q_proj.weight</td>
      <td id="T_c6ca4_row367_col2" class="data row367 col2" >av_romanizer.w2v_model.encoder.layers.8.self_attn.q_proj.weight</td>
      <td id="T_c6ca4_row367_col3" class="data row367 col3" >22.9061</td>
      <td id="T_c6ca4_row367_col4" class="data row367 col4" >151.762</td>
      <td id="T_c6ca4_row367_col5" class="data row367 col5" >146.227</td>
      <td id="T_c6ca4_row367_col6" class="data row367 col6" >0.150934</td>
      <td id="T_c6ca4_row367_col7" class="data row367 col7" >0.988869</td>
      <td id="T_c6ca4_row367_col8" class="data row367 col8" >0.199066</td>
      <td id="T_c6ca4_row367_col9" class="data row367 col9" >1048576.000000</td>
      <td id="T_c6ca4_row367_col10" class="data row367 col10" >torch.float32</td>
      <td id="T_c6ca4_row367_col11" class="data row367 col11" >(1024, 1024)</td>
      <td id="T_c6ca4_row367_col12" class="data row367 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row368" class="row_heading level0 row368" >368</th>
      <td id="T_c6ca4_row368_col0" class="data row368 col0" >encoder.layers.8.self_attn.v_proj.bias</td>
      <td id="T_c6ca4_row368_col1" class="data row368 col1" >encoder.layers.8.self_attn.v_proj.bias</td>
      <td id="T_c6ca4_row368_col2" class="data row368 col2" >av_romanizer.w2v_model.encoder.layers.8.self_attn.v_proj.bias</td>
      <td id="T_c6ca4_row368_col3" class="data row368 col3" >0.236191</td>
      <td id="T_c6ca4_row368_col4" class="data row368 col4" >1.63664</td>
      <td id="T_c6ca4_row368_col5" class="data row368 col5" >1.54263</td>
      <td id="T_c6ca4_row368_col6" class="data row368 col6" >0.144315</td>
      <td id="T_c6ca4_row368_col7" class="data row368 col7" >0.990702</td>
      <td id="T_c6ca4_row368_col8" class="data row368 col8" >0.0288696</td>
      <td id="T_c6ca4_row368_col9" class="data row368 col9" >1024.000000</td>
      <td id="T_c6ca4_row368_col10" class="data row368 col10" >torch.float32</td>
      <td id="T_c6ca4_row368_col11" class="data row368 col11" >(1024,)</td>
      <td id="T_c6ca4_row368_col12" class="data row368 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row369" class="row_heading level0 row369" >369</th>
      <td id="T_c6ca4_row369_col0" class="data row369 col0" >encoder.layers.8.self_attn.v_proj.weight</td>
      <td id="T_c6ca4_row369_col1" class="data row369 col1" >encoder.layers.8.self_attn.v_proj.weight</td>
      <td id="T_c6ca4_row369_col2" class="data row369 col2" >av_romanizer.w2v_model.encoder.layers.8.self_attn.v_proj.weight</td>
      <td id="T_c6ca4_row369_col3" class="data row369 col3" >21.3075</td>
      <td id="T_c6ca4_row369_col4" class="data row369 col4" >137.641</td>
      <td id="T_c6ca4_row369_col5" class="data row369 col5" >131.08</td>
      <td id="T_c6ca4_row369_col6" class="data row369 col6" >0.154805</td>
      <td id="T_c6ca4_row369_col7" class="data row369 col7" >0.988611</td>
      <td id="T_c6ca4_row369_col8" class="data row369 col8" >0.116577</td>
      <td id="T_c6ca4_row369_col9" class="data row369 col9" >1048576.000000</td>
      <td id="T_c6ca4_row369_col10" class="data row369 col10" >torch.float32</td>
      <td id="T_c6ca4_row369_col11" class="data row369 col11" >(1024, 1024)</td>
      <td id="T_c6ca4_row369_col12" class="data row369 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row370" class="row_heading level0 row370" >370</th>
      <td id="T_c6ca4_row370_col0" class="data row370 col0" >encoder.layers.8.self_attn_layer_norm.bias</td>
      <td id="T_c6ca4_row370_col1" class="data row370 col1" >encoder.layers.8.self_attn_layer_norm.bias</td>
      <td id="T_c6ca4_row370_col2" class="data row370 col2" >av_romanizer.w2v_model.encoder.layers.8.self_attn_layer_norm.bias</td>
      <td id="T_c6ca4_row370_col3" class="data row370 col3" >0.180291</td>
      <td id="T_c6ca4_row370_col4" class="data row370 col4" >0.764572</td>
      <td id="T_c6ca4_row370_col5" class="data row370 col5" >0.721087</td>
      <td id="T_c6ca4_row370_col6" class="data row370 col6" >0.235807</td>
      <td id="T_c6ca4_row370_col7" class="data row370 col7" >0.972236</td>
      <td id="T_c6ca4_row370_col8" class="data row370 col8" >0.0378647</td>
      <td id="T_c6ca4_row370_col9" class="data row370 col9" >1024.000000</td>
      <td id="T_c6ca4_row370_col10" class="data row370 col10" >torch.float32</td>
      <td id="T_c6ca4_row370_col11" class="data row370 col11" >(1024,)</td>
      <td id="T_c6ca4_row370_col12" class="data row370 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row371" class="row_heading level0 row371" >371</th>
      <td id="T_c6ca4_row371_col0" class="data row371 col0" >encoder.layers.8.self_attn_layer_norm.weight</td>
      <td id="T_c6ca4_row371_col1" class="data row371 col1" >encoder.layers.8.self_attn_layer_norm.weight</td>
      <td id="T_c6ca4_row371_col2" class="data row371 col2" >av_romanizer.w2v_model.encoder.layers.8.self_attn_layer_norm.weight</td>
      <td id="T_c6ca4_row371_col3" class="data row371 col3" >0.398323</td>
      <td id="T_c6ca4_row371_col4" class="data row371 col4" >5.99435</td>
      <td id="T_c6ca4_row371_col5" class="data row371 col5" >5.78175</td>
      <td id="T_c6ca4_row371_col6" class="data row371 col6" >0.0664497</td>
      <td id="T_c6ca4_row371_col7" class="data row371 col7" >0.998363</td>
      <td id="T_c6ca4_row371_col8" class="data row371 col8" >0.0561523</td>
      <td id="T_c6ca4_row371_col9" class="data row371 col9" >1024.000000</td>
      <td id="T_c6ca4_row371_col10" class="data row371 col10" >torch.float32</td>
      <td id="T_c6ca4_row371_col11" class="data row371 col11" >(1024,)</td>
      <td id="T_c6ca4_row371_col12" class="data row371 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row372" class="row_heading level0 row372" >372</th>
      <td id="T_c6ca4_row372_col0" class="data row372 col0" >encoder.layers.9.fc1.bias</td>
      <td id="T_c6ca4_row372_col1" class="data row372 col1" >encoder.layers.9.fc1.bias</td>
      <td id="T_c6ca4_row372_col2" class="data row372 col2" >av_romanizer.w2v_model.encoder.layers.9.fc1.bias</td>
      <td id="T_c6ca4_row372_col3" class="data row372 col3" >0.620609</td>
      <td id="T_c6ca4_row372_col4" class="data row372 col4" >3.0128</td>
      <td id="T_c6ca4_row372_col5" class="data row372 col5" >3.01686</td>
      <td id="T_c6ca4_row372_col6" class="data row372 col6" >0.205991</td>
      <td id="T_c6ca4_row372_col7" class="data row372 col7" >0.978813</td>
      <td id="T_c6ca4_row372_col8" class="data row372 col8" >0.0672913</td>
      <td id="T_c6ca4_row372_col9" class="data row372 col9" >4096.000000</td>
      <td id="T_c6ca4_row372_col10" class="data row372 col10" >torch.float32</td>
      <td id="T_c6ca4_row372_col11" class="data row372 col11" >(4096,)</td>
      <td id="T_c6ca4_row372_col12" class="data row372 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row373" class="row_heading level0 row373" >373</th>
      <td id="T_c6ca4_row373_col0" class="data row373 col0" >encoder.layers.9.fc1.weight</td>
      <td id="T_c6ca4_row373_col1" class="data row373 col1" >encoder.layers.9.fc1.weight</td>
      <td id="T_c6ca4_row373_col2" class="data row373 col2" >av_romanizer.w2v_model.encoder.layers.9.fc1.weight</td>
      <td id="T_c6ca4_row373_col3" class="data row373 col3" >51.4865</td>
      <td id="T_c6ca4_row373_col4" class="data row373 col4" >300.157</td>
      <td id="T_c6ca4_row373_col5" class="data row373 col5" >286.864</td>
      <td id="T_c6ca4_row373_col6" class="data row373 col6" >0.171532</td>
      <td id="T_c6ca4_row373_col7" class="data row373 col7" >0.985633</td>
      <td id="T_c6ca4_row373_col8" class="data row373 col8" >0.201416</td>
      <td id="T_c6ca4_row373_col9" class="data row373 col9" >4194304.000000</td>
      <td id="T_c6ca4_row373_col10" class="data row373 col10" >torch.float32</td>
      <td id="T_c6ca4_row373_col11" class="data row373 col11" >(4096, 1024)</td>
      <td id="T_c6ca4_row373_col12" class="data row373 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row374" class="row_heading level0 row374" >374</th>
      <td id="T_c6ca4_row374_col0" class="data row374 col0" >encoder.layers.9.fc2.bias</td>
      <td id="T_c6ca4_row374_col1" class="data row374 col1" >encoder.layers.9.fc2.bias</td>
      <td id="T_c6ca4_row374_col2" class="data row374 col2" >av_romanizer.w2v_model.encoder.layers.9.fc2.bias</td>
      <td id="T_c6ca4_row374_col3" class="data row374 col3" >0.201136</td>
      <td id="T_c6ca4_row374_col4" class="data row374 col4" >3.00044</td>
      <td id="T_c6ca4_row374_col5" class="data row374 col5" >2.84916</td>
      <td id="T_c6ca4_row374_col6" class="data row374 col6" >0.0670354</td>
      <td id="T_c6ca4_row374_col7" class="data row374 col7" >0.998972</td>
      <td id="T_c6ca4_row374_col8" class="data row374 col8" >0.0341797</td>
      <td id="T_c6ca4_row374_col9" class="data row374 col9" >1024.000000</td>
      <td id="T_c6ca4_row374_col10" class="data row374 col10" >torch.float32</td>
      <td id="T_c6ca4_row374_col11" class="data row374 col11" >(1024,)</td>
      <td id="T_c6ca4_row374_col12" class="data row374 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row375" class="row_heading level0 row375" >375</th>
      <td id="T_c6ca4_row375_col0" class="data row375 col0" >encoder.layers.9.fc2.weight</td>
      <td id="T_c6ca4_row375_col1" class="data row375 col1" >encoder.layers.9.fc2.weight</td>
      <td id="T_c6ca4_row375_col2" class="data row375 col2" >av_romanizer.w2v_model.encoder.layers.9.fc2.weight</td>
      <td id="T_c6ca4_row375_col3" class="data row375 col3" >49.2303</td>
      <td id="T_c6ca4_row375_col4" class="data row375 col4" >299.142</td>
      <td id="T_c6ca4_row375_col5" class="data row375 col5" >284.932</td>
      <td id="T_c6ca4_row375_col6" class="data row375 col6" >0.164572</td>
      <td id="T_c6ca4_row375_col7" class="data row375 col7" >0.986967</td>
      <td id="T_c6ca4_row375_col8" class="data row375 col8" >0.155609</td>
      <td id="T_c6ca4_row375_col9" class="data row375 col9" >4194304.000000</td>
      <td id="T_c6ca4_row375_col10" class="data row375 col10" >torch.float32</td>
      <td id="T_c6ca4_row375_col11" class="data row375 col11" >(1024, 4096)</td>
      <td id="T_c6ca4_row375_col12" class="data row375 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row376" class="row_heading level0 row376" >376</th>
      <td id="T_c6ca4_row376_col0" class="data row376 col0" >encoder.layers.9.final_layer_norm.bias</td>
      <td id="T_c6ca4_row376_col1" class="data row376 col1" >encoder.layers.9.final_layer_norm.bias</td>
      <td id="T_c6ca4_row376_col2" class="data row376 col2" >av_romanizer.w2v_model.encoder.layers.9.final_layer_norm.bias</td>
      <td id="T_c6ca4_row376_col3" class="data row376 col3" >0.186849</td>
      <td id="T_c6ca4_row376_col4" class="data row376 col4" >2.05027</td>
      <td id="T_c6ca4_row376_col5" class="data row376 col5" >1.9726</td>
      <td id="T_c6ca4_row376_col6" class="data row376 col6" >0.0911335</td>
      <td id="T_c6ca4_row376_col7" class="data row376 col7" >0.99643</td>
      <td id="T_c6ca4_row376_col8" class="data row376 col8" >0.0314941</td>
      <td id="T_c6ca4_row376_col9" class="data row376 col9" >1024.000000</td>
      <td id="T_c6ca4_row376_col10" class="data row376 col10" >torch.float32</td>
      <td id="T_c6ca4_row376_col11" class="data row376 col11" >(1024,)</td>
      <td id="T_c6ca4_row376_col12" class="data row376 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row377" class="row_heading level0 row377" >377</th>
      <td id="T_c6ca4_row377_col0" class="data row377 col0" >encoder.layers.9.final_layer_norm.weight</td>
      <td id="T_c6ca4_row377_col1" class="data row377 col1" >encoder.layers.9.final_layer_norm.weight</td>
      <td id="T_c6ca4_row377_col2" class="data row377 col2" >av_romanizer.w2v_model.encoder.layers.9.final_layer_norm.weight</td>
      <td id="T_c6ca4_row377_col3" class="data row377 col3" >0.566503</td>
      <td id="T_c6ca4_row377_col4" class="data row377 col4" >6.72884</td>
      <td id="T_c6ca4_row377_col5" class="data row377 col5" >6.27158</td>
      <td id="T_c6ca4_row377_col6" class="data row377 col6" >0.0841904</td>
      <td id="T_c6ca4_row377_col7" class="data row377 col7" >0.998675</td>
      <td id="T_c6ca4_row377_col8" class="data row377 col8" >0.0484619</td>
      <td id="T_c6ca4_row377_col9" class="data row377 col9" >1024.000000</td>
      <td id="T_c6ca4_row377_col10" class="data row377 col10" >torch.float32</td>
      <td id="T_c6ca4_row377_col11" class="data row377 col11" >(1024,)</td>
      <td id="T_c6ca4_row377_col12" class="data row377 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row378" class="row_heading level0 row378" >378</th>
      <td id="T_c6ca4_row378_col0" class="data row378 col0" >encoder.layers.9.self_attn.k_proj.bias</td>
      <td id="T_c6ca4_row378_col1" class="data row378 col1" >encoder.layers.9.self_attn.k_proj.bias</td>
      <td id="T_c6ca4_row378_col2" class="data row378 col2" >av_romanizer.w2v_model.encoder.layers.9.self_attn.k_proj.bias</td>
      <td id="T_c6ca4_row378_col3" class="data row378 col3" >0.0171389</td>
      <td id="T_c6ca4_row378_col4" class="data row378 col4" >0.27472</td>
      <td id="T_c6ca4_row378_col5" class="data row378 col5" >0.277216</td>
      <td id="T_c6ca4_row378_col6" class="data row378 col6" >0.0623868</td>
      <td id="T_c6ca4_row378_col7" class="data row378 col7" >0.998112</td>
      <td id="T_c6ca4_row378_col8" class="data row378 col8" >0.00330353</td>
      <td id="T_c6ca4_row378_col9" class="data row378 col9" >1024.000000</td>
      <td id="T_c6ca4_row378_col10" class="data row378 col10" >torch.float32</td>
      <td id="T_c6ca4_row378_col11" class="data row378 col11" >(1024,)</td>
      <td id="T_c6ca4_row378_col12" class="data row378 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row379" class="row_heading level0 row379" >379</th>
      <td id="T_c6ca4_row379_col0" class="data row379 col0" >encoder.layers.9.self_attn.k_proj.weight</td>
      <td id="T_c6ca4_row379_col1" class="data row379 col1" >encoder.layers.9.self_attn.k_proj.weight</td>
      <td id="T_c6ca4_row379_col2" class="data row379 col2" >av_romanizer.w2v_model.encoder.layers.9.self_attn.k_proj.weight</td>
      <td id="T_c6ca4_row379_col3" class="data row379 col3" >23.1122</td>
      <td id="T_c6ca4_row379_col4" class="data row379 col4" >151.468</td>
      <td id="T_c6ca4_row379_col5" class="data row379 col5" >145.896</td>
      <td id="T_c6ca4_row379_col6" class="data row379 col6" >0.152588</td>
      <td id="T_c6ca4_row379_col7" class="data row379 col7" >0.988616</td>
      <td id="T_c6ca4_row379_col8" class="data row379 col8" >0.12207</td>
      <td id="T_c6ca4_row379_col9" class="data row379 col9" >1048576.000000</td>
      <td id="T_c6ca4_row379_col10" class="data row379 col10" >torch.float32</td>
      <td id="T_c6ca4_row379_col11" class="data row379 col11" >(1024, 1024)</td>
      <td id="T_c6ca4_row379_col12" class="data row379 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row380" class="row_heading level0 row380" >380</th>
      <td id="T_c6ca4_row380_col0" class="data row380 col0" >encoder.layers.9.self_attn.out_proj.bias</td>
      <td id="T_c6ca4_row380_col1" class="data row380 col1" >encoder.layers.9.self_attn.out_proj.bias</td>
      <td id="T_c6ca4_row380_col2" class="data row380 col2" >av_romanizer.w2v_model.encoder.layers.9.self_attn.out_proj.bias</td>
      <td id="T_c6ca4_row380_col3" class="data row380 col3" >0.252828</td>
      <td id="T_c6ca4_row380_col4" class="data row380 col4" >3.19074</td>
      <td id="T_c6ca4_row380_col5" class="data row380 col5" >3.03359</td>
      <td id="T_c6ca4_row380_col6" class="data row380 col6" >0.0792381</td>
      <td id="T_c6ca4_row380_col7" class="data row380 col7" >0.997974</td>
      <td id="T_c6ca4_row380_col8" class="data row380 col8" >0.0341797</td>
      <td id="T_c6ca4_row380_col9" class="data row380 col9" >1024.000000</td>
      <td id="T_c6ca4_row380_col10" class="data row380 col10" >torch.float32</td>
      <td id="T_c6ca4_row380_col11" class="data row380 col11" >(1024,)</td>
      <td id="T_c6ca4_row380_col12" class="data row380 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row381" class="row_heading level0 row381" >381</th>
      <td id="T_c6ca4_row381_col0" class="data row381 col0" >encoder.layers.9.self_attn.out_proj.weight</td>
      <td id="T_c6ca4_row381_col1" class="data row381 col1" >encoder.layers.9.self_attn.out_proj.weight</td>
      <td id="T_c6ca4_row381_col2" class="data row381 col2" >av_romanizer.w2v_model.encoder.layers.9.self_attn.out_proj.weight</td>
      <td id="T_c6ca4_row381_col3" class="data row381 col3" >24.497</td>
      <td id="T_c6ca4_row381_col4" class="data row381 col4" >135.031</td>
      <td id="T_c6ca4_row381_col5" class="data row381 col5" >128.743</td>
      <td id="T_c6ca4_row381_col6" class="data row381 col6" >0.181417</td>
      <td id="T_c6ca4_row381_col7" class="data row381 col7" >0.983877</td>
      <td id="T_c6ca4_row381_col8" class="data row381 col8" >0.137547</td>
      <td id="T_c6ca4_row381_col9" class="data row381 col9" >1048576.000000</td>
      <td id="T_c6ca4_row381_col10" class="data row381 col10" >torch.float32</td>
      <td id="T_c6ca4_row381_col11" class="data row381 col11" >(1024, 1024)</td>
      <td id="T_c6ca4_row381_col12" class="data row381 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row382" class="row_heading level0 row382" >382</th>
      <td id="T_c6ca4_row382_col0" class="data row382 col0" >encoder.layers.9.self_attn.q_proj.bias</td>
      <td id="T_c6ca4_row382_col1" class="data row382 col1" >encoder.layers.9.self_attn.q_proj.bias</td>
      <td id="T_c6ca4_row382_col2" class="data row382 col2" >av_romanizer.w2v_model.encoder.layers.9.self_attn.q_proj.bias</td>
      <td id="T_c6ca4_row382_col3" class="data row382 col3" >0.834552</td>
      <td id="T_c6ca4_row382_col4" class="data row382 col4" >9.26809</td>
      <td id="T_c6ca4_row382_col5" class="data row382 col5" >9.03383</td>
      <td id="T_c6ca4_row382_col6" class="data row382 col6" >0.0900457</td>
      <td id="T_c6ca4_row382_col7" class="data row382 col7" >0.996168</td>
      <td id="T_c6ca4_row382_col8" class="data row382 col8" >0.0986328</td>
      <td id="T_c6ca4_row382_col9" class="data row382 col9" >1024.000000</td>
      <td id="T_c6ca4_row382_col10" class="data row382 col10" >torch.float32</td>
      <td id="T_c6ca4_row382_col11" class="data row382 col11" >(1024,)</td>
      <td id="T_c6ca4_row382_col12" class="data row382 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row383" class="row_heading level0 row383" >383</th>
      <td id="T_c6ca4_row383_col0" class="data row383 col0" >encoder.layers.9.self_attn.q_proj.weight</td>
      <td id="T_c6ca4_row383_col1" class="data row383 col1" >encoder.layers.9.self_attn.q_proj.weight</td>
      <td id="T_c6ca4_row383_col2" class="data row383 col2" >av_romanizer.w2v_model.encoder.layers.9.self_attn.q_proj.weight</td>
      <td id="T_c6ca4_row383_col3" class="data row383 col3" >23.3528</td>
      <td id="T_c6ca4_row383_col4" class="data row383 col4" >150.388</td>
      <td id="T_c6ca4_row383_col5" class="data row383 col5" >144.816</td>
      <td id="T_c6ca4_row383_col6" class="data row383 col6" >0.155284</td>
      <td id="T_c6ca4_row383_col7" class="data row383 col7" >0.988192</td>
      <td id="T_c6ca4_row383_col8" class="data row383 col8" >0.146484</td>
      <td id="T_c6ca4_row383_col9" class="data row383 col9" >1048576.000000</td>
      <td id="T_c6ca4_row383_col10" class="data row383 col10" >torch.float32</td>
      <td id="T_c6ca4_row383_col11" class="data row383 col11" >(1024, 1024)</td>
      <td id="T_c6ca4_row383_col12" class="data row383 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row384" class="row_heading level0 row384" >384</th>
      <td id="T_c6ca4_row384_col0" class="data row384 col0" >encoder.layers.9.self_attn.v_proj.bias</td>
      <td id="T_c6ca4_row384_col1" class="data row384 col1" >encoder.layers.9.self_attn.v_proj.bias</td>
      <td id="T_c6ca4_row384_col2" class="data row384 col2" >av_romanizer.w2v_model.encoder.layers.9.self_attn.v_proj.bias</td>
      <td id="T_c6ca4_row384_col3" class="data row384 col3" >0.225604</td>
      <td id="T_c6ca4_row384_col4" class="data row384 col4" >1.5168</td>
      <td id="T_c6ca4_row384_col5" class="data row384 col5" >1.44355</td>
      <td id="T_c6ca4_row384_col6" class="data row384 col6" >0.148737</td>
      <td id="T_c6ca4_row384_col7" class="data row384 col7" >0.989603</td>
      <td id="T_c6ca4_row384_col8" class="data row384 col8" >0.0250397</td>
      <td id="T_c6ca4_row384_col9" class="data row384 col9" >1024.000000</td>
      <td id="T_c6ca4_row384_col10" class="data row384 col10" >torch.float32</td>
      <td id="T_c6ca4_row384_col11" class="data row384 col11" >(1024,)</td>
      <td id="T_c6ca4_row384_col12" class="data row384 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row385" class="row_heading level0 row385" >385</th>
      <td id="T_c6ca4_row385_col0" class="data row385 col0" >encoder.layers.9.self_attn.v_proj.weight</td>
      <td id="T_c6ca4_row385_col1" class="data row385 col1" >encoder.layers.9.self_attn.v_proj.weight</td>
      <td id="T_c6ca4_row385_col2" class="data row385 col2" >av_romanizer.w2v_model.encoder.layers.9.self_attn.v_proj.weight</td>
      <td id="T_c6ca4_row385_col3" class="data row385 col3" >22.1509</td>
      <td id="T_c6ca4_row385_col4" class="data row385 col4" >138.867</td>
      <td id="T_c6ca4_row385_col5" class="data row385 col5" >132.496</td>
      <td id="T_c6ca4_row385_col6" class="data row385 col6" >0.159511</td>
      <td id="T_c6ca4_row385_col7" class="data row385 col7" >0.987769</td>
      <td id="T_c6ca4_row385_col8" class="data row385 col8" >0.123535</td>
      <td id="T_c6ca4_row385_col9" class="data row385 col9" >1048576.000000</td>
      <td id="T_c6ca4_row385_col10" class="data row385 col10" >torch.float32</td>
      <td id="T_c6ca4_row385_col11" class="data row385 col11" >(1024, 1024)</td>
      <td id="T_c6ca4_row385_col12" class="data row385 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row386" class="row_heading level0 row386" >386</th>
      <td id="T_c6ca4_row386_col0" class="data row386 col0" >encoder.layers.9.self_attn_layer_norm.bias</td>
      <td id="T_c6ca4_row386_col1" class="data row386 col1" >encoder.layers.9.self_attn_layer_norm.bias</td>
      <td id="T_c6ca4_row386_col2" class="data row386 col2" >av_romanizer.w2v_model.encoder.layers.9.self_attn_layer_norm.bias</td>
      <td id="T_c6ca4_row386_col3" class="data row386 col3" >0.163557</td>
      <td id="T_c6ca4_row386_col4" class="data row386 col4" >0.662656</td>
      <td id="T_c6ca4_row386_col5" class="data row386 col5" >0.650426</td>
      <td id="T_c6ca4_row386_col6" class="data row386 col6" >0.246821</td>
      <td id="T_c6ca4_row386_col7" class="data row386 col7" >0.96914</td>
      <td id="T_c6ca4_row386_col8" class="data row386 col8" >0.0352173</td>
      <td id="T_c6ca4_row386_col9" class="data row386 col9" >1024.000000</td>
      <td id="T_c6ca4_row386_col10" class="data row386 col10" >torch.float32</td>
      <td id="T_c6ca4_row386_col11" class="data row386 col11" >(1024,)</td>
      <td id="T_c6ca4_row386_col12" class="data row386 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row387" class="row_heading level0 row387" >387</th>
      <td id="T_c6ca4_row387_col0" class="data row387 col0" >encoder.layers.9.self_attn_layer_norm.weight</td>
      <td id="T_c6ca4_row387_col1" class="data row387 col1" >encoder.layers.9.self_attn_layer_norm.weight</td>
      <td id="T_c6ca4_row387_col2" class="data row387 col2" >av_romanizer.w2v_model.encoder.layers.9.self_attn_layer_norm.weight</td>
      <td id="T_c6ca4_row387_col3" class="data row387 col3" >0.355148</td>
      <td id="T_c6ca4_row387_col4" class="data row387 col4" >5.86626</td>
      <td id="T_c6ca4_row387_col5" class="data row387 col5" >5.83402</td>
      <td id="T_c6ca4_row387_col6" class="data row387 col6" >0.0605407</td>
      <td id="T_c6ca4_row387_col7" class="data row387 col7" >0.998172</td>
      <td id="T_c6ca4_row387_col8" class="data row387 col8" >0.0383301</td>
      <td id="T_c6ca4_row387_col9" class="data row387 col9" >1024.000000</td>
      <td id="T_c6ca4_row387_col10" class="data row387 col10" >torch.float32</td>
      <td id="T_c6ca4_row387_col11" class="data row387 col11" >(1024,)</td>
      <td id="T_c6ca4_row387_col12" class="data row387 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row388" class="row_heading level0 row388" >388</th>
      <td id="T_c6ca4_row388_col0" class="data row388 col0" >encoder.pos_conv.0.bias</td>
      <td id="T_c6ca4_row388_col1" class="data row388 col1" >encoder.pos_conv.0.bias</td>
      <td id="T_c6ca4_row388_col2" class="data row388 col2" >av_romanizer.w2v_model.encoder.pos_conv.0.bias</td>
      <td id="T_c6ca4_row388_col3" class="data row388 col3" >0.725031</td>
      <td id="T_c6ca4_row388_col4" class="data row388 col4" >9.75832</td>
      <td id="T_c6ca4_row388_col5" class="data row388 col5" >9.34813</td>
      <td id="T_c6ca4_row388_col6" class="data row388 col6" >0.0742988</td>
      <td id="T_c6ca4_row388_col7" class="data row388 col7" >0.998041</td>
      <td id="T_c6ca4_row388_col8" class="data row388 col8" >0.207031</td>
      <td id="T_c6ca4_row388_col9" class="data row388 col9" >1024.000000</td>
      <td id="T_c6ca4_row388_col10" class="data row388 col10" >torch.float32</td>
      <td id="T_c6ca4_row388_col11" class="data row388 col11" >(1024,)</td>
      <td id="T_c6ca4_row388_col12" class="data row388 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row389" class="row_heading level0 row389" >389</th>
      <td id="T_c6ca4_row389_col0" class="data row389 col0" >encoder.pos_conv.0.weight_g</td>
      <td id="T_c6ca4_row389_col1" class="data row389 col1" >encoder.pos_conv.0.weight_g</td>
      <td id="T_c6ca4_row389_col2" class="data row389 col2" >av_romanizer.w2v_model.encoder.pos_conv.0.weight_g</td>
      <td id="T_c6ca4_row389_col3" class="data row389 col3" >0.461196</td>
      <td id="T_c6ca4_row389_col4" class="data row389 col4" >15.3562</td>
      <td id="T_c6ca4_row389_col5" class="data row389 col5" >15.0476</td>
      <td id="T_c6ca4_row389_col6" class="data row389 col6" >0.0300333</td>
      <td id="T_c6ca4_row389_col7" class="data row389 col7" >0.999746</td>
      <td id="T_c6ca4_row389_col8" class="data row389 col8" >0.265625</td>
      <td id="T_c6ca4_row389_col9" class="data row389 col9" >128.000000</td>
      <td id="T_c6ca4_row389_col10" class="data row389 col10" >torch.float32</td>
      <td id="T_c6ca4_row389_col11" class="data row389 col11" >(1, 1, 128)</td>
      <td id="T_c6ca4_row389_col12" class="data row389 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row390" class="row_heading level0 row390" >390</th>
      <td id="T_c6ca4_row390_col0" class="data row390 col0" >encoder.pos_conv.0.weight_v</td>
      <td id="T_c6ca4_row390_col1" class="data row390 col1" >encoder.pos_conv.0.weight_v</td>
      <td id="T_c6ca4_row390_col2" class="data row390 col2" >av_romanizer.w2v_model.encoder.pos_conv.0.weight_v</td>
      <td id="T_c6ca4_row390_col3" class="data row390 col3" >46.9325</td>
      <td id="T_c6ca4_row390_col4" class="data row390 col4" >295.565</td>
      <td id="T_c6ca4_row390_col5" class="data row390 col5" >281.435</td>
      <td id="T_c6ca4_row390_col6" class="data row390 col6" >0.158789</td>
      <td id="T_c6ca4_row390_col7" class="data row390 col7" >0.98796</td>
      <td id="T_c6ca4_row390_col8" class="data row390 col8" >0.305664</td>
      <td id="T_c6ca4_row390_col9" class="data row390 col9" >8388608.000000</td>
      <td id="T_c6ca4_row390_col10" class="data row390 col10" >torch.float32</td>
      <td id="T_c6ca4_row390_col11" class="data row390 col11" >(1024, 64, 128)</td>
      <td id="T_c6ca4_row390_col12" class="data row390 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row391" class="row_heading level0 row391" >391</th>
      <td id="T_c6ca4_row391_col0" class="data row391 col0" >feature_extractor_audio.proj.bias</td>
      <td id="T_c6ca4_row391_col1" class="data row391 col1" >feature_extractor_audio.proj.bias</td>
      <td id="T_c6ca4_row391_col2" class="data row391 col2" >av_romanizer.w2v_model.feature_extractor_audio.proj.bias</td>
      <td id="T_c6ca4_row391_col3" class="data row391 col3" >0.342192</td>
      <td id="T_c6ca4_row391_col4" class="data row391 col4" >0.227</td>
      <td id="T_c6ca4_row391_col5" class="data row391 col5" >0.447608</td>
      <td id="T_c6ca4_row391_col6" class="data row391 col6" >1.50745</td>
      <td id="T_c6ca4_row391_col7" class="data row391 col7" >0.663276</td>
      <td id="T_c6ca4_row391_col8" class="data row391 col8" >0.103363</td>
      <td id="T_c6ca4_row391_col9" class="data row391 col9" >1024.000000</td>
      <td id="T_c6ca4_row391_col10" class="data row391 col10" >torch.float32</td>
      <td id="T_c6ca4_row391_col11" class="data row391 col11" >(1024,)</td>
      <td id="T_c6ca4_row391_col12" class="data row391 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row392" class="row_heading level0 row392" >392</th>
      <td id="T_c6ca4_row392_col0" class="data row392 col0" >feature_extractor_audio.proj.weight</td>
      <td id="T_c6ca4_row392_col1" class="data row392 col1" >feature_extractor_audio.proj.weight</td>
      <td id="T_c6ca4_row392_col2" class="data row392 col2" >av_romanizer.w2v_model.feature_extractor_audio.proj.weight</td>
      <td id="T_c6ca4_row392_col3" class="data row392 col3" >2.4267</td>
      <td id="T_c6ca4_row392_col4" class="data row392 col4" >2.98944</td>
      <td id="T_c6ca4_row392_col5" class="data row392 col5" >4.15326</td>
      <td id="T_c6ca4_row392_col6" class="data row392 col6" >0.811758</td>
      <td id="T_c6ca4_row392_col7" class="data row392 col7" >0.817397</td>
      <td id="T_c6ca4_row392_col8" class="data row392 col8" >0.0620422</td>
      <td id="T_c6ca4_row392_col9" class="data row392 col9" >106496.000000</td>
      <td id="T_c6ca4_row392_col10" class="data row392 col10" >torch.float32</td>
      <td id="T_c6ca4_row392_col11" class="data row392 col11" >(1024, 104)</td>
      <td id="T_c6ca4_row392_col12" class="data row392 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row393" class="row_heading level0 row393" >393</th>
      <td id="T_c6ca4_row393_col0" class="data row393 col0" >feature_extractor_video.proj.bias</td>
      <td id="T_c6ca4_row393_col1" class="data row393 col1" >feature_extractor_video.proj.bias</td>
      <td id="T_c6ca4_row393_col2" class="data row393 col2" >av_romanizer.w2v_model.feature_extractor_video.proj.bias</td>
      <td id="T_c6ca4_row393_col3" class="data row393 col3" >0.613534</td>
      <td id="T_c6ca4_row393_col4" class="data row393 col4" >0.258944</td>
      <td id="T_c6ca4_row393_col5" class="data row393 col5" >0.74179</td>
      <td id="T_c6ca4_row393_col6" class="data row393 col6" >2.36938</td>
      <td id="T_c6ca4_row393_col7" class="data row393 col7" >0.627025</td>
      <td id="T_c6ca4_row393_col8" class="data row393 col8" >0.0396729</td>
      <td id="T_c6ca4_row393_col9" class="data row393 col9" >1024.000000</td>
      <td id="T_c6ca4_row393_col10" class="data row393 col10" >torch.float32</td>
      <td id="T_c6ca4_row393_col11" class="data row393 col11" >(1024,)</td>
      <td id="T_c6ca4_row393_col12" class="data row393 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row394" class="row_heading level0 row394" >394</th>
      <td id="T_c6ca4_row394_col0" class="data row394 col0" >feature_extractor_video.proj.weight</td>
      <td id="T_c6ca4_row394_col1" class="data row394 col1" >feature_extractor_video.proj.weight</td>
      <td id="T_c6ca4_row394_col2" class="data row394 col2" >av_romanizer.w2v_model.feature_extractor_video.proj.weight</td>
      <td id="T_c6ca4_row394_col3" class="data row394 col3" >6.53474</td>
      <td id="T_c6ca4_row394_col4" class="data row394 col4" >29.4521</td>
      <td id="T_c6ca4_row394_col5" class="data row394 col5" >28.9795</td>
      <td id="T_c6ca4_row394_col6" class="data row394 col6" >0.221877</td>
      <td id="T_c6ca4_row394_col7" class="data row394 col7" >0.975115</td>
      <td id="T_c6ca4_row394_col8" class="data row394 col8" >0.109253</td>
      <td id="T_c6ca4_row394_col9" class="data row394 col9" >524288.000000</td>
      <td id="T_c6ca4_row394_col10" class="data row394 col10" >torch.float32</td>
      <td id="T_c6ca4_row394_col11" class="data row394 col11" >(1024, 512)</td>
      <td id="T_c6ca4_row394_col12" class="data row394 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row395" class="row_heading level0 row395" >395</th>
      <td id="T_c6ca4_row395_col0" class="data row395 col0" >feature_extractor_video.resnet.frontend3D.0.weight</td>
      <td id="T_c6ca4_row395_col1" class="data row395 col1" >feature_extractor_video.resnet.frontend3D.0.weight</td>
      <td id="T_c6ca4_row395_col2" class="data row395 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.frontend3D.0.weight</td>
      <td id="T_c6ca4_row395_col3" class="data row395 col3" >1.32021</td>
      <td id="T_c6ca4_row395_col4" class="data row395 col4" >21.3311</td>
      <td id="T_c6ca4_row395_col5" class="data row395 col5" >20.3523</td>
      <td id="T_c6ca4_row395_col6" class="data row395 col6" >0.0618912</td>
      <td id="T_c6ca4_row395_col7" class="data row395 col7" >0.999096</td>
      <td id="T_c6ca4_row395_col8" class="data row395 col8" >0.136719</td>
      <td id="T_c6ca4_row395_col9" class="data row395 col9" >15680.000000</td>
      <td id="T_c6ca4_row395_col10" class="data row395 col10" >torch.float32</td>
      <td id="T_c6ca4_row395_col11" class="data row395 col11" >(64, 1, 5, 7, 7)</td>
      <td id="T_c6ca4_row395_col12" class="data row395 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row396" class="row_heading level0 row396" >396</th>
      <td id="T_c6ca4_row396_col0" class="data row396 col0" >feature_extractor_video.resnet.frontend3D.1.bias</td>
      <td id="T_c6ca4_row396_col1" class="data row396 col1" >feature_extractor_video.resnet.frontend3D.1.bias</td>
      <td id="T_c6ca4_row396_col2" class="data row396 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.frontend3D.1.bias</td>
      <td id="T_c6ca4_row396_col3" class="data row396 col3" >0.0786259</td>
      <td id="T_c6ca4_row396_col4" class="data row396 col4" >0.701386</td>
      <td id="T_c6ca4_row396_col5" class="data row396 col5" >0.688269</td>
      <td id="T_c6ca4_row396_col6" class="data row396 col6" >0.112101</td>
      <td id="T_c6ca4_row396_col7" class="data row396 col7" >0.993775</td>
      <td id="T_c6ca4_row396_col8" class="data row396 col8" >0.0340881</td>
      <td id="T_c6ca4_row396_col9" class="data row396 col9" >64.000000</td>
      <td id="T_c6ca4_row396_col10" class="data row396 col10" >torch.float32</td>
      <td id="T_c6ca4_row396_col11" class="data row396 col11" >(64,)</td>
      <td id="T_c6ca4_row396_col12" class="data row396 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row397" class="row_heading level0 row397" >397</th>
      <td id="T_c6ca4_row397_col0" class="data row397 col0" >feature_extractor_video.resnet.frontend3D.1.num_batches_tracked</td>
      <td id="T_c6ca4_row397_col1" class="data row397 col1" >feature_extractor_video.resnet.frontend3D.1.num_batches_tracked</td>
      <td id="T_c6ca4_row397_col2" class="data row397 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.frontend3D.1.num_batches_tracked</td>
      <td id="T_c6ca4_row397_col3" class="data row397 col3" >880066</td>
      <td id="T_c6ca4_row397_col4" class="data row397 col4" >601803</td>
      <td id="T_c6ca4_row397_col5" class="data row397 col5" >1.48187e+06</td>
      <td id="T_c6ca4_row397_col6" class="data row397 col6" >1.46238</td>
      <td id="T_c6ca4_row397_col7" class="data row397 col7" >1</td>
      <td id="T_c6ca4_row397_col8" class="data row397 col8" >880066</td>
      <td id="T_c6ca4_row397_col9" class="data row397 col9" >1.000000</td>
      <td id="T_c6ca4_row397_col10" class="data row397 col10" >torch.int64</td>
      <td id="T_c6ca4_row397_col11" class="data row397 col11" >()</td>
      <td id="T_c6ca4_row397_col12" class="data row397 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row398" class="row_heading level0 row398" >398</th>
      <td id="T_c6ca4_row398_col0" class="data row398 col0" >feature_extractor_video.resnet.frontend3D.1.running_mean</td>
      <td id="T_c6ca4_row398_col1" class="data row398 col1" >feature_extractor_video.resnet.frontend3D.1.running_mean</td>
      <td id="T_c6ca4_row398_col2" class="data row398 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.frontend3D.1.running_mean</td>
      <td id="T_c6ca4_row398_col3" class="data row398 col3" >4.7057</td>
      <td id="T_c6ca4_row398_col4" class="data row398 col4" >5.18209</td>
      <td id="T_c6ca4_row398_col5" class="data row398 col5" >0.778618</td>
      <td id="T_c6ca4_row398_col6" class="data row398 col6" >0.908069</td>
      <td id="T_c6ca4_row398_col7" class="data row398 col7" >0.658847</td>
      <td id="T_c6ca4_row398_col8" class="data row398 col8" >2.20239</td>
      <td id="T_c6ca4_row398_col9" class="data row398 col9" >64.000000</td>
      <td id="T_c6ca4_row398_col10" class="data row398 col10" >torch.float32</td>
      <td id="T_c6ca4_row398_col11" class="data row398 col11" >(64,)</td>
      <td id="T_c6ca4_row398_col12" class="data row398 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row399" class="row_heading level0 row399" >399</th>
      <td id="T_c6ca4_row399_col0" class="data row399 col0" >feature_extractor_video.resnet.frontend3D.1.running_var</td>
      <td id="T_c6ca4_row399_col1" class="data row399 col1" >feature_extractor_video.resnet.frontend3D.1.running_var</td>
      <td id="T_c6ca4_row399_col2" class="data row399 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.frontend3D.1.running_var</td>
      <td id="T_c6ca4_row399_col3" class="data row399 col3" >68.5075</td>
      <td id="T_c6ca4_row399_col4" class="data row399 col4" >453.176</td>
      <td id="T_c6ca4_row399_col5" class="data row399 col5" >398.184</td>
      <td id="T_c6ca4_row399_col6" class="data row399 col6" >0.151172</td>
      <td id="T_c6ca4_row399_col7" class="data row399 col7" >0.995375</td>
      <td id="T_c6ca4_row399_col8" class="data row399 col8" >29.375</td>
      <td id="T_c6ca4_row399_col9" class="data row399 col9" >64.000000</td>
      <td id="T_c6ca4_row399_col10" class="data row399 col10" >torch.float32</td>
      <td id="T_c6ca4_row399_col11" class="data row399 col11" >(64,)</td>
      <td id="T_c6ca4_row399_col12" class="data row399 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row400" class="row_heading level0 row400" >400</th>
      <td id="T_c6ca4_row400_col0" class="data row400 col0" >feature_extractor_video.resnet.frontend3D.1.weight</td>
      <td id="T_c6ca4_row400_col1" class="data row400 col1" >feature_extractor_video.resnet.frontend3D.1.weight</td>
      <td id="T_c6ca4_row400_col2" class="data row400 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.frontend3D.1.weight</td>
      <td id="T_c6ca4_row400_col3" class="data row400 col3" >0.0883185</td>
      <td id="T_c6ca4_row400_col4" class="data row400 col4" >1.12142</td>
      <td id="T_c6ca4_row400_col5" class="data row400 col5" >1.06491</td>
      <td id="T_c6ca4_row400_col6" class="data row400 col6" >0.0787559</td>
      <td id="T_c6ca4_row400_col7" class="data row400 col7" >0.998071</td>
      <td id="T_c6ca4_row400_col8" class="data row400 col8" >0.0293579</td>
      <td id="T_c6ca4_row400_col9" class="data row400 col9" >64.000000</td>
      <td id="T_c6ca4_row400_col10" class="data row400 col10" >torch.float32</td>
      <td id="T_c6ca4_row400_col11" class="data row400 col11" >(64,)</td>
      <td id="T_c6ca4_row400_col12" class="data row400 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row401" class="row_heading level0 row401" >401</th>
      <td id="T_c6ca4_row401_col0" class="data row401 col0" >feature_extractor_video.resnet.frontend3D.2.weight</td>
      <td id="T_c6ca4_row401_col1" class="data row401 col1" >feature_extractor_video.resnet.frontend3D.2.weight</td>
      <td id="T_c6ca4_row401_col2" class="data row401 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.frontend3D.2.weight</td>
      <td id="T_c6ca4_row401_col3" class="data row401 col3" >0.193564</td>
      <td id="T_c6ca4_row401_col4" class="data row401 col4" >4.98665</td>
      <td id="T_c6ca4_row401_col5" class="data row401 col5" >4.86831</td>
      <td id="T_c6ca4_row401_col6" class="data row401 col6" >0.0388164</td>
      <td id="T_c6ca4_row401_col7" class="data row401 col7" >0.999517</td>
      <td id="T_c6ca4_row401_col8" class="data row401 col8" >0.0559082</td>
      <td id="T_c6ca4_row401_col9" class="data row401 col9" >64.000000</td>
      <td id="T_c6ca4_row401_col10" class="data row401 col10" >torch.float32</td>
      <td id="T_c6ca4_row401_col11" class="data row401 col11" >(64,)</td>
      <td id="T_c6ca4_row401_col12" class="data row401 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row402" class="row_heading level0 row402" >402</th>
      <td id="T_c6ca4_row402_col0" class="data row402 col0" >feature_extractor_video.resnet.trunk.layer1.0.bn1.bias</td>
      <td id="T_c6ca4_row402_col1" class="data row402 col1" >feature_extractor_video.resnet.trunk.layer1.0.bn1.bias</td>
      <td id="T_c6ca4_row402_col2" class="data row402 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer1.0.bn1.bias</td>
      <td id="T_c6ca4_row402_col3" class="data row402 col3" >0.186303</td>
      <td id="T_c6ca4_row402_col4" class="data row402 col4" >0.632965</td>
      <td id="T_c6ca4_row402_col5" class="data row402 col5" >0.643761</td>
      <td id="T_c6ca4_row402_col6" class="data row402 col6" >0.294334</td>
      <td id="T_c6ca4_row402_col7" class="data row402 col7" >0.957553</td>
      <td id="T_c6ca4_row402_col8" class="data row402 col8" >0.0806427</td>
      <td id="T_c6ca4_row402_col9" class="data row402 col9" >64.000000</td>
      <td id="T_c6ca4_row402_col10" class="data row402 col10" >torch.float32</td>
      <td id="T_c6ca4_row402_col11" class="data row402 col11" >(64,)</td>
      <td id="T_c6ca4_row402_col12" class="data row402 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row403" class="row_heading level0 row403" >403</th>
      <td id="T_c6ca4_row403_col0" class="data row403 col0" >feature_extractor_video.resnet.trunk.layer1.0.bn1.num_batches_tracked</td>
      <td id="T_c6ca4_row403_col1" class="data row403 col1" >feature_extractor_video.resnet.trunk.layer1.0.bn1.num_batches_tracked</td>
      <td id="T_c6ca4_row403_col2" class="data row403 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer1.0.bn1.num_batches_tracked</td>
      <td id="T_c6ca4_row403_col3" class="data row403 col3" >880066</td>
      <td id="T_c6ca4_row403_col4" class="data row403 col4" >601803</td>
      <td id="T_c6ca4_row403_col5" class="data row403 col5" >1.48187e+06</td>
      <td id="T_c6ca4_row403_col6" class="data row403 col6" >1.46238</td>
      <td id="T_c6ca4_row403_col7" class="data row403 col7" >1</td>
      <td id="T_c6ca4_row403_col8" class="data row403 col8" >880066</td>
      <td id="T_c6ca4_row403_col9" class="data row403 col9" >1.000000</td>
      <td id="T_c6ca4_row403_col10" class="data row403 col10" >torch.int64</td>
      <td id="T_c6ca4_row403_col11" class="data row403 col11" >()</td>
      <td id="T_c6ca4_row403_col12" class="data row403 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row404" class="row_heading level0 row404" >404</th>
      <td id="T_c6ca4_row404_col0" class="data row404 col0" >feature_extractor_video.resnet.trunk.layer1.0.bn1.running_mean</td>
      <td id="T_c6ca4_row404_col1" class="data row404 col1" >feature_extractor_video.resnet.trunk.layer1.0.bn1.running_mean</td>
      <td id="T_c6ca4_row404_col2" class="data row404 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer1.0.bn1.running_mean</td>
      <td id="T_c6ca4_row404_col3" class="data row404 col3" >0.652484</td>
      <td id="T_c6ca4_row404_col4" class="data row404 col4" >4.82071</td>
      <td id="T_c6ca4_row404_col5" class="data row404 col5" >4.59208</td>
      <td id="T_c6ca4_row404_col6" class="data row404 col6" >0.13535</td>
      <td id="T_c6ca4_row404_col7" class="data row404 col7" >0.991565</td>
      <td id="T_c6ca4_row404_col8" class="data row404 col8" >0.252686</td>
      <td id="T_c6ca4_row404_col9" class="data row404 col9" >64.000000</td>
      <td id="T_c6ca4_row404_col10" class="data row404 col10" >torch.float32</td>
      <td id="T_c6ca4_row404_col11" class="data row404 col11" >(64,)</td>
      <td id="T_c6ca4_row404_col12" class="data row404 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row405" class="row_heading level0 row405" >405</th>
      <td id="T_c6ca4_row405_col0" class="data row405 col0" >feature_extractor_video.resnet.trunk.layer1.0.bn1.running_var</td>
      <td id="T_c6ca4_row405_col1" class="data row405 col1" >feature_extractor_video.resnet.trunk.layer1.0.bn1.running_var</td>
      <td id="T_c6ca4_row405_col2" class="data row405 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer1.0.bn1.running_var</td>
      <td id="T_c6ca4_row405_col3" class="data row405 col3" >2.07078</td>
      <td id="T_c6ca4_row405_col4" class="data row405 col4" >7.72994</td>
      <td id="T_c6ca4_row405_col5" class="data row405 col5" >6.1529</td>
      <td id="T_c6ca4_row405_col6" class="data row405 col6" >0.267891</td>
      <td id="T_c6ca4_row405_col7" class="data row405 col7" >0.981066</td>
      <td id="T_c6ca4_row405_col8" class="data row405 col8" >1.1582</td>
      <td id="T_c6ca4_row405_col9" class="data row405 col9" >64.000000</td>
      <td id="T_c6ca4_row405_col10" class="data row405 col10" >torch.float32</td>
      <td id="T_c6ca4_row405_col11" class="data row405 col11" >(64,)</td>
      <td id="T_c6ca4_row405_col12" class="data row405 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row406" class="row_heading level0 row406" >406</th>
      <td id="T_c6ca4_row406_col0" class="data row406 col0" >feature_extractor_video.resnet.trunk.layer1.0.bn1.weight</td>
      <td id="T_c6ca4_row406_col1" class="data row406 col1" >feature_extractor_video.resnet.trunk.layer1.0.bn1.weight</td>
      <td id="T_c6ca4_row406_col2" class="data row406 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer1.0.bn1.weight</td>
      <td id="T_c6ca4_row406_col3" class="data row406 col3" >0.130779</td>
      <td id="T_c6ca4_row406_col4" class="data row406 col4" >1.77058</td>
      <td id="T_c6ca4_row406_col5" class="data row406 col5" >1.68511</td>
      <td id="T_c6ca4_row406_col6" class="data row406 col6" >0.0738624</td>
      <td id="T_c6ca4_row406_col7" class="data row406 col7" >0.998358</td>
      <td id="T_c6ca4_row406_col8" class="data row406 col8" >0.0393677</td>
      <td id="T_c6ca4_row406_col9" class="data row406 col9" >64.000000</td>
      <td id="T_c6ca4_row406_col10" class="data row406 col10" >torch.float32</td>
      <td id="T_c6ca4_row406_col11" class="data row406 col11" >(64,)</td>
      <td id="T_c6ca4_row406_col12" class="data row406 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row407" class="row_heading level0 row407" >407</th>
      <td id="T_c6ca4_row407_col0" class="data row407 col0" >feature_extractor_video.resnet.trunk.layer1.0.bn2.bias</td>
      <td id="T_c6ca4_row407_col1" class="data row407 col1" >feature_extractor_video.resnet.trunk.layer1.0.bn2.bias</td>
      <td id="T_c6ca4_row407_col2" class="data row407 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer1.0.bn2.bias</td>
      <td id="T_c6ca4_row407_col3" class="data row407 col3" >0.10504</td>
      <td id="T_c6ca4_row407_col4" class="data row407 col4" >1.17166</td>
      <td id="T_c6ca4_row407_col5" class="data row407 col5" >1.11931</td>
      <td id="T_c6ca4_row407_col6" class="data row407 col6" >0.0896506</td>
      <td id="T_c6ca4_row407_col7" class="data row407 col7" >0.996838</td>
      <td id="T_c6ca4_row407_col8" class="data row407 col8" >0.0367203</td>
      <td id="T_c6ca4_row407_col9" class="data row407 col9" >64.000000</td>
      <td id="T_c6ca4_row407_col10" class="data row407 col10" >torch.float32</td>
      <td id="T_c6ca4_row407_col11" class="data row407 col11" >(64,)</td>
      <td id="T_c6ca4_row407_col12" class="data row407 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row408" class="row_heading level0 row408" >408</th>
      <td id="T_c6ca4_row408_col0" class="data row408 col0" >feature_extractor_video.resnet.trunk.layer1.0.bn2.num_batches_tracked</td>
      <td id="T_c6ca4_row408_col1" class="data row408 col1" >feature_extractor_video.resnet.trunk.layer1.0.bn2.num_batches_tracked</td>
      <td id="T_c6ca4_row408_col2" class="data row408 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer1.0.bn2.num_batches_tracked</td>
      <td id="T_c6ca4_row408_col3" class="data row408 col3" >880066</td>
      <td id="T_c6ca4_row408_col4" class="data row408 col4" >601803</td>
      <td id="T_c6ca4_row408_col5" class="data row408 col5" >1.48187e+06</td>
      <td id="T_c6ca4_row408_col6" class="data row408 col6" >1.46238</td>
      <td id="T_c6ca4_row408_col7" class="data row408 col7" >1</td>
      <td id="T_c6ca4_row408_col8" class="data row408 col8" >880066</td>
      <td id="T_c6ca4_row408_col9" class="data row408 col9" >1.000000</td>
      <td id="T_c6ca4_row408_col10" class="data row408 col10" >torch.int64</td>
      <td id="T_c6ca4_row408_col11" class="data row408 col11" >()</td>
      <td id="T_c6ca4_row408_col12" class="data row408 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row409" class="row_heading level0 row409" >409</th>
      <td id="T_c6ca4_row409_col0" class="data row409 col0" >feature_extractor_video.resnet.trunk.layer1.0.bn2.running_mean</td>
      <td id="T_c6ca4_row409_col1" class="data row409 col1" >feature_extractor_video.resnet.trunk.layer1.0.bn2.running_mean</td>
      <td id="T_c6ca4_row409_col2" class="data row409 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer1.0.bn2.running_mean</td>
      <td id="T_c6ca4_row409_col3" class="data row409 col3" >0.780059</td>
      <td id="T_c6ca4_row409_col4" class="data row409 col4" >5.82047</td>
      <td id="T_c6ca4_row409_col5" class="data row409 col5" >5.77876</td>
      <td id="T_c6ca4_row409_col6" class="data row409 col6" >0.13402</td>
      <td id="T_c6ca4_row409_col7" class="data row409 col7" >0.99098</td>
      <td id="T_c6ca4_row409_col8" class="data row409 col8" >0.346069</td>
      <td id="T_c6ca4_row409_col9" class="data row409 col9" >64.000000</td>
      <td id="T_c6ca4_row409_col10" class="data row409 col10" >torch.float32</td>
      <td id="T_c6ca4_row409_col11" class="data row409 col11" >(64,)</td>
      <td id="T_c6ca4_row409_col12" class="data row409 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row410" class="row_heading level0 row410" >410</th>
      <td id="T_c6ca4_row410_col0" class="data row410 col0" >feature_extractor_video.resnet.trunk.layer1.0.bn2.running_var</td>
      <td id="T_c6ca4_row410_col1" class="data row410 col1" >feature_extractor_video.resnet.trunk.layer1.0.bn2.running_var</td>
      <td id="T_c6ca4_row410_col2" class="data row410 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer1.0.bn2.running_var</td>
      <td id="T_c6ca4_row410_col3" class="data row410 col3" >1.07337</td>
      <td id="T_c6ca4_row410_col4" class="data row410 col4" >7.79068</td>
      <td id="T_c6ca4_row410_col5" class="data row410 col5" >6.91104</td>
      <td id="T_c6ca4_row410_col6" class="data row410 col6" >0.137777</td>
      <td id="T_c6ca4_row410_col7" class="data row410 col7" >0.996486</td>
      <td id="T_c6ca4_row410_col8" class="data row410 col8" >0.685547</td>
      <td id="T_c6ca4_row410_col9" class="data row410 col9" >64.000000</td>
      <td id="T_c6ca4_row410_col10" class="data row410 col10" >torch.float32</td>
      <td id="T_c6ca4_row410_col11" class="data row410 col11" >(64,)</td>
      <td id="T_c6ca4_row410_col12" class="data row410 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row411" class="row_heading level0 row411" >411</th>
      <td id="T_c6ca4_row411_col0" class="data row411 col0" >feature_extractor_video.resnet.trunk.layer1.0.bn2.weight</td>
      <td id="T_c6ca4_row411_col1" class="data row411 col1" >feature_extractor_video.resnet.trunk.layer1.0.bn2.weight</td>
      <td id="T_c6ca4_row411_col2" class="data row411 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer1.0.bn2.weight</td>
      <td id="T_c6ca4_row411_col3" class="data row411 col3" >0.104939</td>
      <td id="T_c6ca4_row411_col4" class="data row411 col4" >1.63182</td>
      <td id="T_c6ca4_row411_col5" class="data row411 col5" >1.5728</td>
      <td id="T_c6ca4_row411_col6" class="data row411 col6" >0.0643082</td>
      <td id="T_c6ca4_row411_col7" class="data row411 col7" >0.998533</td>
      <td id="T_c6ca4_row411_col8" class="data row411 col8" >0.0319824</td>
      <td id="T_c6ca4_row411_col9" class="data row411 col9" >64.000000</td>
      <td id="T_c6ca4_row411_col10" class="data row411 col10" >torch.float32</td>
      <td id="T_c6ca4_row411_col11" class="data row411 col11" >(64,)</td>
      <td id="T_c6ca4_row411_col12" class="data row411 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row412" class="row_heading level0 row412" >412</th>
      <td id="T_c6ca4_row412_col0" class="data row412 col0" >feature_extractor_video.resnet.trunk.layer1.0.conv1.weight</td>
      <td id="T_c6ca4_row412_col1" class="data row412 col1" >feature_extractor_video.resnet.trunk.layer1.0.conv1.weight</td>
      <td id="T_c6ca4_row412_col2" class="data row412 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer1.0.conv1.weight</td>
      <td id="T_c6ca4_row412_col3" class="data row412 col3" >2.49205</td>
      <td id="T_c6ca4_row412_col4" class="data row412 col4" >32.2414</td>
      <td id="T_c6ca4_row412_col5" class="data row412 col5" >30.809</td>
      <td id="T_c6ca4_row412_col6" class="data row412 col6" >0.0772935</td>
      <td id="T_c6ca4_row412_col7" class="data row412 col7" >0.997907</td>
      <td id="T_c6ca4_row412_col8" class="data row412 col8" >0.138672</td>
      <td id="T_c6ca4_row412_col9" class="data row412 col9" >36864.000000</td>
      <td id="T_c6ca4_row412_col10" class="data row412 col10" >torch.float32</td>
      <td id="T_c6ca4_row412_col11" class="data row412 col11" >(64, 64, 3, 3)</td>
      <td id="T_c6ca4_row412_col12" class="data row412 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row413" class="row_heading level0 row413" >413</th>
      <td id="T_c6ca4_row413_col0" class="data row413 col0" >feature_extractor_video.resnet.trunk.layer1.0.conv2.weight</td>
      <td id="T_c6ca4_row413_col1" class="data row413 col1" >feature_extractor_video.resnet.trunk.layer1.0.conv2.weight</td>
      <td id="T_c6ca4_row413_col2" class="data row413 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer1.0.conv2.weight</td>
      <td id="T_c6ca4_row413_col3" class="data row413 col3" >2.77393</td>
      <td id="T_c6ca4_row413_col4" class="data row413 col4" >29.0646</td>
      <td id="T_c6ca4_row413_col5" class="data row413 col5" >27.8072</td>
      <td id="T_c6ca4_row413_col6" class="data row413 col6" >0.0954399</td>
      <td id="T_c6ca4_row413_col7" class="data row413 col7" >0.996218</td>
      <td id="T_c6ca4_row413_col8" class="data row413 col8" >0.107422</td>
      <td id="T_c6ca4_row413_col9" class="data row413 col9" >36864.000000</td>
      <td id="T_c6ca4_row413_col10" class="data row413 col10" >torch.float32</td>
      <td id="T_c6ca4_row413_col11" class="data row413 col11" >(64, 64, 3, 3)</td>
      <td id="T_c6ca4_row413_col12" class="data row413 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row414" class="row_heading level0 row414" >414</th>
      <td id="T_c6ca4_row414_col0" class="data row414 col0" >feature_extractor_video.resnet.trunk.layer1.0.relu1.weight</td>
      <td id="T_c6ca4_row414_col1" class="data row414 col1" >feature_extractor_video.resnet.trunk.layer1.0.relu1.weight</td>
      <td id="T_c6ca4_row414_col2" class="data row414 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer1.0.relu1.weight</td>
      <td id="T_c6ca4_row414_col3" class="data row414 col3" >0.136873</td>
      <td id="T_c6ca4_row414_col4" class="data row414 col4" >2.03837</td>
      <td id="T_c6ca4_row414_col5" class="data row414 col5" >1.94833</td>
      <td id="T_c6ca4_row414_col6" class="data row414 col6" >0.0671482</td>
      <td id="T_c6ca4_row414_col7" class="data row414 col7" >0.998662</td>
      <td id="T_c6ca4_row414_col8" class="data row414 col8" >0.0454102</td>
      <td id="T_c6ca4_row414_col9" class="data row414 col9" >64.000000</td>
      <td id="T_c6ca4_row414_col10" class="data row414 col10" >torch.float32</td>
      <td id="T_c6ca4_row414_col11" class="data row414 col11" >(64,)</td>
      <td id="T_c6ca4_row414_col12" class="data row414 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row415" class="row_heading level0 row415" >415</th>
      <td id="T_c6ca4_row415_col0" class="data row415 col0" >feature_extractor_video.resnet.trunk.layer1.0.relu2.weight</td>
      <td id="T_c6ca4_row415_col1" class="data row415 col1" >feature_extractor_video.resnet.trunk.layer1.0.relu2.weight</td>
      <td id="T_c6ca4_row415_col2" class="data row415 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer1.0.relu2.weight</td>
      <td id="T_c6ca4_row415_col3" class="data row415 col3" >0.236185</td>
      <td id="T_c6ca4_row415_col4" class="data row415 col4" >4.16851</td>
      <td id="T_c6ca4_row415_col5" class="data row415 col5" >3.98363</td>
      <td id="T_c6ca4_row415_col6" class="data row415 col6" >0.0566594</td>
      <td id="T_c6ca4_row415_col7" class="data row415 col7" >0.99935</td>
      <td id="T_c6ca4_row415_col8" class="data row415 col8" >0.0825195</td>
      <td id="T_c6ca4_row415_col9" class="data row415 col9" >64.000000</td>
      <td id="T_c6ca4_row415_col10" class="data row415 col10" >torch.float32</td>
      <td id="T_c6ca4_row415_col11" class="data row415 col11" >(64,)</td>
      <td id="T_c6ca4_row415_col12" class="data row415 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row416" class="row_heading level0 row416" >416</th>
      <td id="T_c6ca4_row416_col0" class="data row416 col0" >feature_extractor_video.resnet.trunk.layer1.1.bn1.bias</td>
      <td id="T_c6ca4_row416_col1" class="data row416 col1" >feature_extractor_video.resnet.trunk.layer1.1.bn1.bias</td>
      <td id="T_c6ca4_row416_col2" class="data row416 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer1.1.bn1.bias</td>
      <td id="T_c6ca4_row416_col3" class="data row416 col3" >0.0846251</td>
      <td id="T_c6ca4_row416_col4" class="data row416 col4" >0.753761</td>
      <td id="T_c6ca4_row416_col5" class="data row416 col5" >0.728843</td>
      <td id="T_c6ca4_row416_col6" class="data row416 col6" >0.11227</td>
      <td id="T_c6ca4_row416_col7" class="data row416 col7" >0.994047</td>
      <td id="T_c6ca4_row416_col8" class="data row416 col8" >0.0377808</td>
      <td id="T_c6ca4_row416_col9" class="data row416 col9" >64.000000</td>
      <td id="T_c6ca4_row416_col10" class="data row416 col10" >torch.float32</td>
      <td id="T_c6ca4_row416_col11" class="data row416 col11" >(64,)</td>
      <td id="T_c6ca4_row416_col12" class="data row416 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row417" class="row_heading level0 row417" >417</th>
      <td id="T_c6ca4_row417_col0" class="data row417 col0" >feature_extractor_video.resnet.trunk.layer1.1.bn1.num_batches_tracked</td>
      <td id="T_c6ca4_row417_col1" class="data row417 col1" >feature_extractor_video.resnet.trunk.layer1.1.bn1.num_batches_tracked</td>
      <td id="T_c6ca4_row417_col2" class="data row417 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer1.1.bn1.num_batches_tracked</td>
      <td id="T_c6ca4_row417_col3" class="data row417 col3" >880066</td>
      <td id="T_c6ca4_row417_col4" class="data row417 col4" >601803</td>
      <td id="T_c6ca4_row417_col5" class="data row417 col5" >1.48187e+06</td>
      <td id="T_c6ca4_row417_col6" class="data row417 col6" >1.46238</td>
      <td id="T_c6ca4_row417_col7" class="data row417 col7" >1</td>
      <td id="T_c6ca4_row417_col8" class="data row417 col8" >880066</td>
      <td id="T_c6ca4_row417_col9" class="data row417 col9" >1.000000</td>
      <td id="T_c6ca4_row417_col10" class="data row417 col10" >torch.int64</td>
      <td id="T_c6ca4_row417_col11" class="data row417 col11" >()</td>
      <td id="T_c6ca4_row417_col12" class="data row417 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row418" class="row_heading level0 row418" >418</th>
      <td id="T_c6ca4_row418_col0" class="data row418 col0" >feature_extractor_video.resnet.trunk.layer1.1.bn1.running_mean</td>
      <td id="T_c6ca4_row418_col1" class="data row418 col1" >feature_extractor_video.resnet.trunk.layer1.1.bn1.running_mean</td>
      <td id="T_c6ca4_row418_col2" class="data row418 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer1.1.bn1.running_mean</td>
      <td id="T_c6ca4_row418_col3" class="data row418 col3" >1.11327</td>
      <td id="T_c6ca4_row418_col4" class="data row418 col4" >10.608</td>
      <td id="T_c6ca4_row418_col5" class="data row418 col5" >9.94421</td>
      <td id="T_c6ca4_row418_col6" class="data row418 col6" >0.104946</td>
      <td id="T_c6ca4_row418_col7" class="data row418 col7" >0.996214</td>
      <td id="T_c6ca4_row418_col8" class="data row418 col8" >0.353516</td>
      <td id="T_c6ca4_row418_col9" class="data row418 col9" >64.000000</td>
      <td id="T_c6ca4_row418_col10" class="data row418 col10" >torch.float32</td>
      <td id="T_c6ca4_row418_col11" class="data row418 col11" >(64,)</td>
      <td id="T_c6ca4_row418_col12" class="data row418 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row419" class="row_heading level0 row419" >419</th>
      <td id="T_c6ca4_row419_col0" class="data row419 col0" >feature_extractor_video.resnet.trunk.layer1.1.bn1.running_var</td>
      <td id="T_c6ca4_row419_col1" class="data row419 col1" >feature_extractor_video.resnet.trunk.layer1.1.bn1.running_var</td>
      <td id="T_c6ca4_row419_col2" class="data row419 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer1.1.bn1.running_var</td>
      <td id="T_c6ca4_row419_col3" class="data row419 col3" >2.35843</td>
      <td id="T_c6ca4_row419_col4" class="data row419 col4" >10.8962</td>
      <td id="T_c6ca4_row419_col5" class="data row419 col5" >8.79719</td>
      <td id="T_c6ca4_row419_col6" class="data row419 col6" >0.216446</td>
      <td id="T_c6ca4_row419_col7" class="data row419 col7" >0.993967</td>
      <td id="T_c6ca4_row419_col8" class="data row419 col8" >0.813477</td>
      <td id="T_c6ca4_row419_col9" class="data row419 col9" >64.000000</td>
      <td id="T_c6ca4_row419_col10" class="data row419 col10" >torch.float32</td>
      <td id="T_c6ca4_row419_col11" class="data row419 col11" >(64,)</td>
      <td id="T_c6ca4_row419_col12" class="data row419 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row420" class="row_heading level0 row420" >420</th>
      <td id="T_c6ca4_row420_col0" class="data row420 col0" >feature_extractor_video.resnet.trunk.layer1.1.bn1.weight</td>
      <td id="T_c6ca4_row420_col1" class="data row420 col1" >feature_extractor_video.resnet.trunk.layer1.1.bn1.weight</td>
      <td id="T_c6ca4_row420_col2" class="data row420 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer1.1.bn1.weight</td>
      <td id="T_c6ca4_row420_col3" class="data row420 col3" >0.0932387</td>
      <td id="T_c6ca4_row420_col4" class="data row420 col4" >1.69703</td>
      <td id="T_c6ca4_row420_col5" class="data row420 col5" >1.62371</td>
      <td id="T_c6ca4_row420_col6" class="data row420 col6" >0.0549424</td>
      <td id="T_c6ca4_row420_col7" class="data row420 col7" >0.999398</td>
      <td id="T_c6ca4_row420_col8" class="data row420 col8" >0.0251465</td>
      <td id="T_c6ca4_row420_col9" class="data row420 col9" >64.000000</td>
      <td id="T_c6ca4_row420_col10" class="data row420 col10" >torch.float32</td>
      <td id="T_c6ca4_row420_col11" class="data row420 col11" >(64,)</td>
      <td id="T_c6ca4_row420_col12" class="data row420 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row421" class="row_heading level0 row421" >421</th>
      <td id="T_c6ca4_row421_col0" class="data row421 col0" >feature_extractor_video.resnet.trunk.layer1.1.bn2.bias</td>
      <td id="T_c6ca4_row421_col1" class="data row421 col1" >feature_extractor_video.resnet.trunk.layer1.1.bn2.bias</td>
      <td id="T_c6ca4_row421_col2" class="data row421 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer1.1.bn2.bias</td>
      <td id="T_c6ca4_row421_col3" class="data row421 col3" >0.121681</td>
      <td id="T_c6ca4_row421_col4" class="data row421 col4" >1.64962</td>
      <td id="T_c6ca4_row421_col5" class="data row421 col5" >1.59335</td>
      <td id="T_c6ca4_row421_col6" class="data row421 col6" >0.0737627</td>
      <td id="T_c6ca4_row421_col7" class="data row421 col7" >0.997786</td>
      <td id="T_c6ca4_row421_col8" class="data row421 col8" >0.0603485</td>
      <td id="T_c6ca4_row421_col9" class="data row421 col9" >64.000000</td>
      <td id="T_c6ca4_row421_col10" class="data row421 col10" >torch.float32</td>
      <td id="T_c6ca4_row421_col11" class="data row421 col11" >(64,)</td>
      <td id="T_c6ca4_row421_col12" class="data row421 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row422" class="row_heading level0 row422" >422</th>
      <td id="T_c6ca4_row422_col0" class="data row422 col0" >feature_extractor_video.resnet.trunk.layer1.1.bn2.num_batches_tracked</td>
      <td id="T_c6ca4_row422_col1" class="data row422 col1" >feature_extractor_video.resnet.trunk.layer1.1.bn2.num_batches_tracked</td>
      <td id="T_c6ca4_row422_col2" class="data row422 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer1.1.bn2.num_batches_tracked</td>
      <td id="T_c6ca4_row422_col3" class="data row422 col3" >880066</td>
      <td id="T_c6ca4_row422_col4" class="data row422 col4" >601803</td>
      <td id="T_c6ca4_row422_col5" class="data row422 col5" >1.48187e+06</td>
      <td id="T_c6ca4_row422_col6" class="data row422 col6" >1.46238</td>
      <td id="T_c6ca4_row422_col7" class="data row422 col7" >1</td>
      <td id="T_c6ca4_row422_col8" class="data row422 col8" >880066</td>
      <td id="T_c6ca4_row422_col9" class="data row422 col9" >1.000000</td>
      <td id="T_c6ca4_row422_col10" class="data row422 col10" >torch.int64</td>
      <td id="T_c6ca4_row422_col11" class="data row422 col11" >()</td>
      <td id="T_c6ca4_row422_col12" class="data row422 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row423" class="row_heading level0 row423" >423</th>
      <td id="T_c6ca4_row423_col0" class="data row423 col0" >feature_extractor_video.resnet.trunk.layer1.1.bn2.running_mean</td>
      <td id="T_c6ca4_row423_col1" class="data row423 col1" >feature_extractor_video.resnet.trunk.layer1.1.bn2.running_mean</td>
      <td id="T_c6ca4_row423_col2" class="data row423 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer1.1.bn2.running_mean</td>
      <td id="T_c6ca4_row423_col3" class="data row423 col3" >0.635193</td>
      <td id="T_c6ca4_row423_col4" class="data row423 col4" >4.55578</td>
      <td id="T_c6ca4_row423_col5" class="data row423 col5" >4.38795</td>
      <td id="T_c6ca4_row423_col6" class="data row423 col6" >0.139426</td>
      <td id="T_c6ca4_row423_col7" class="data row423 col7" >0.990613</td>
      <td id="T_c6ca4_row423_col8" class="data row423 col8" >0.280029</td>
      <td id="T_c6ca4_row423_col9" class="data row423 col9" >64.000000</td>
      <td id="T_c6ca4_row423_col10" class="data row423 col10" >torch.float32</td>
      <td id="T_c6ca4_row423_col11" class="data row423 col11" >(64,)</td>
      <td id="T_c6ca4_row423_col12" class="data row423 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row424" class="row_heading level0 row424" >424</th>
      <td id="T_c6ca4_row424_col0" class="data row424 col0" >feature_extractor_video.resnet.trunk.layer1.1.bn2.running_var</td>
      <td id="T_c6ca4_row424_col1" class="data row424 col1" >feature_extractor_video.resnet.trunk.layer1.1.bn2.running_var</td>
      <td id="T_c6ca4_row424_col2" class="data row424 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer1.1.bn2.running_var</td>
      <td id="T_c6ca4_row424_col3" class="data row424 col3" >0.468549</td>
      <td id="T_c6ca4_row424_col4" class="data row424 col4" >2.64123</td>
      <td id="T_c6ca4_row424_col5" class="data row424 col5" >2.20257</td>
      <td id="T_c6ca4_row424_col6" class="data row424 col6" >0.177398</td>
      <td id="T_c6ca4_row424_col7" class="data row424 col7" >0.997669</td>
      <td id="T_c6ca4_row424_col8" class="data row424 col8" >0.147949</td>
      <td id="T_c6ca4_row424_col9" class="data row424 col9" >64.000000</td>
      <td id="T_c6ca4_row424_col10" class="data row424 col10" >torch.float32</td>
      <td id="T_c6ca4_row424_col11" class="data row424 col11" >(64,)</td>
      <td id="T_c6ca4_row424_col12" class="data row424 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row425" class="row_heading level0 row425" >425</th>
      <td id="T_c6ca4_row425_col0" class="data row425 col0" >feature_extractor_video.resnet.trunk.layer1.1.bn2.weight</td>
      <td id="T_c6ca4_row425_col1" class="data row425 col1" >feature_extractor_video.resnet.trunk.layer1.1.bn2.weight</td>
      <td id="T_c6ca4_row425_col2" class="data row425 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer1.1.bn2.weight</td>
      <td id="T_c6ca4_row425_col3" class="data row425 col3" >0.0969749</td>
      <td id="T_c6ca4_row425_col4" class="data row425 col4" >1.84804</td>
      <td id="T_c6ca4_row425_col5" class="data row425 col5" >1.79511</td>
      <td id="T_c6ca4_row425_col6" class="data row425 col6" >0.0524744</td>
      <td id="T_c6ca4_row425_col7" class="data row425 col7" >0.999005</td>
      <td id="T_c6ca4_row425_col8" class="data row425 col8" >0.0339355</td>
      <td id="T_c6ca4_row425_col9" class="data row425 col9" >64.000000</td>
      <td id="T_c6ca4_row425_col10" class="data row425 col10" >torch.float32</td>
      <td id="T_c6ca4_row425_col11" class="data row425 col11" >(64,)</td>
      <td id="T_c6ca4_row425_col12" class="data row425 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row426" class="row_heading level0 row426" >426</th>
      <td id="T_c6ca4_row426_col0" class="data row426 col0" >feature_extractor_video.resnet.trunk.layer1.1.conv1.weight</td>
      <td id="T_c6ca4_row426_col1" class="data row426 col1" >feature_extractor_video.resnet.trunk.layer1.1.conv1.weight</td>
      <td id="T_c6ca4_row426_col2" class="data row426 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer1.1.conv1.weight</td>
      <td id="T_c6ca4_row426_col3" class="data row426 col3" >2.91969</td>
      <td id="T_c6ca4_row426_col4" class="data row426 col4" >29.4254</td>
      <td id="T_c6ca4_row426_col5" class="data row426 col5" >28.1662</td>
      <td id="T_c6ca4_row426_col6" class="data row426 col6" >0.0992233</td>
      <td id="T_c6ca4_row426_col7" class="data row426 col7" >0.995814</td>
      <td id="T_c6ca4_row426_col8" class="data row426 col8" >0.119141</td>
      <td id="T_c6ca4_row426_col9" class="data row426 col9" >36864.000000</td>
      <td id="T_c6ca4_row426_col10" class="data row426 col10" >torch.float32</td>
      <td id="T_c6ca4_row426_col11" class="data row426 col11" >(64, 64, 3, 3)</td>
      <td id="T_c6ca4_row426_col12" class="data row426 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row427" class="row_heading level0 row427" >427</th>
      <td id="T_c6ca4_row427_col0" class="data row427 col0" >feature_extractor_video.resnet.trunk.layer1.1.conv2.weight</td>
      <td id="T_c6ca4_row427_col1" class="data row427 col1" >feature_extractor_video.resnet.trunk.layer1.1.conv2.weight</td>
      <td id="T_c6ca4_row427_col2" class="data row427 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer1.1.conv2.weight</td>
      <td id="T_c6ca4_row427_col3" class="data row427 col3" >3.05513</td>
      <td id="T_c6ca4_row427_col4" class="data row427 col4" >28.0431</td>
      <td id="T_c6ca4_row427_col5" class="data row427 col5" >26.8434</td>
      <td id="T_c6ca4_row427_col6" class="data row427 col6" >0.108944</td>
      <td id="T_c6ca4_row427_col7" class="data row427 col7" >0.994756</td>
      <td id="T_c6ca4_row427_col8" class="data row427 col8" >0.0901478</td>
      <td id="T_c6ca4_row427_col9" class="data row427 col9" >36864.000000</td>
      <td id="T_c6ca4_row427_col10" class="data row427 col10" >torch.float32</td>
      <td id="T_c6ca4_row427_col11" class="data row427 col11" >(64, 64, 3, 3)</td>
      <td id="T_c6ca4_row427_col12" class="data row427 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row428" class="row_heading level0 row428" >428</th>
      <td id="T_c6ca4_row428_col0" class="data row428 col0" >feature_extractor_video.resnet.trunk.layer1.1.relu1.weight</td>
      <td id="T_c6ca4_row428_col1" class="data row428 col1" >feature_extractor_video.resnet.trunk.layer1.1.relu1.weight</td>
      <td id="T_c6ca4_row428_col2" class="data row428 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer1.1.relu1.weight</td>
      <td id="T_c6ca4_row428_col3" class="data row428 col3" >0.108353</td>
      <td id="T_c6ca4_row428_col4" class="data row428 col4" >1.85981</td>
      <td id="T_c6ca4_row428_col5" class="data row428 col5" >1.83977</td>
      <td id="T_c6ca4_row428_col6" class="data row428 col6" >0.0582601</td>
      <td id="T_c6ca4_row428_col7" class="data row428 col7" >0.998343</td>
      <td id="T_c6ca4_row428_col8" class="data row428 col8" >0.0404053</td>
      <td id="T_c6ca4_row428_col9" class="data row428 col9" >64.000000</td>
      <td id="T_c6ca4_row428_col10" class="data row428 col10" >torch.float32</td>
      <td id="T_c6ca4_row428_col11" class="data row428 col11" >(64,)</td>
      <td id="T_c6ca4_row428_col12" class="data row428 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row429" class="row_heading level0 row429" >429</th>
      <td id="T_c6ca4_row429_col0" class="data row429 col0" >feature_extractor_video.resnet.trunk.layer1.1.relu2.weight</td>
      <td id="T_c6ca4_row429_col1" class="data row429 col1" >feature_extractor_video.resnet.trunk.layer1.1.relu2.weight</td>
      <td id="T_c6ca4_row429_col2" class="data row429 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer1.1.relu2.weight</td>
      <td id="T_c6ca4_row429_col3" class="data row429 col3" >0.207868</td>
      <td id="T_c6ca4_row429_col4" class="data row429 col4" >4.56617</td>
      <td id="T_c6ca4_row429_col5" class="data row429 col5" >4.41652</td>
      <td id="T_c6ca4_row429_col6" class="data row429 col6" >0.0455235</td>
      <td id="T_c6ca4_row429_col7" class="data row429 col7" >0.999484</td>
      <td id="T_c6ca4_row429_col8" class="data row429 col8" >0.0634766</td>
      <td id="T_c6ca4_row429_col9" class="data row429 col9" >64.000000</td>
      <td id="T_c6ca4_row429_col10" class="data row429 col10" >torch.float32</td>
      <td id="T_c6ca4_row429_col11" class="data row429 col11" >(64,)</td>
      <td id="T_c6ca4_row429_col12" class="data row429 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row430" class="row_heading level0 row430" >430</th>
      <td id="T_c6ca4_row430_col0" class="data row430 col0" >feature_extractor_video.resnet.trunk.layer2.0.bn1.bias</td>
      <td id="T_c6ca4_row430_col1" class="data row430 col1" >feature_extractor_video.resnet.trunk.layer2.0.bn1.bias</td>
      <td id="T_c6ca4_row430_col2" class="data row430 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer2.0.bn1.bias</td>
      <td id="T_c6ca4_row430_col3" class="data row430 col3" >0.122769</td>
      <td id="T_c6ca4_row430_col4" class="data row430 col4" >1.20254</td>
      <td id="T_c6ca4_row430_col5" class="data row430 col5" >1.17624</td>
      <td id="T_c6ca4_row430_col6" class="data row430 col6" >0.102091</td>
      <td id="T_c6ca4_row430_col7" class="data row430 col7" >0.994917</td>
      <td id="T_c6ca4_row430_col8" class="data row430 col8" >0.0350342</td>
      <td id="T_c6ca4_row430_col9" class="data row430 col9" >128.000000</td>
      <td id="T_c6ca4_row430_col10" class="data row430 col10" >torch.float32</td>
      <td id="T_c6ca4_row430_col11" class="data row430 col11" >(128,)</td>
      <td id="T_c6ca4_row430_col12" class="data row430 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row431" class="row_heading level0 row431" >431</th>
      <td id="T_c6ca4_row431_col0" class="data row431 col0" >feature_extractor_video.resnet.trunk.layer2.0.bn1.num_batches_tracked</td>
      <td id="T_c6ca4_row431_col1" class="data row431 col1" >feature_extractor_video.resnet.trunk.layer2.0.bn1.num_batches_tracked</td>
      <td id="T_c6ca4_row431_col2" class="data row431 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer2.0.bn1.num_batches_tracked</td>
      <td id="T_c6ca4_row431_col3" class="data row431 col3" >880066</td>
      <td id="T_c6ca4_row431_col4" class="data row431 col4" >601803</td>
      <td id="T_c6ca4_row431_col5" class="data row431 col5" >1.48187e+06</td>
      <td id="T_c6ca4_row431_col6" class="data row431 col6" >1.46238</td>
      <td id="T_c6ca4_row431_col7" class="data row431 col7" >1</td>
      <td id="T_c6ca4_row431_col8" class="data row431 col8" >880066</td>
      <td id="T_c6ca4_row431_col9" class="data row431 col9" >1.000000</td>
      <td id="T_c6ca4_row431_col10" class="data row431 col10" >torch.int64</td>
      <td id="T_c6ca4_row431_col11" class="data row431 col11" >()</td>
      <td id="T_c6ca4_row431_col12" class="data row431 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row432" class="row_heading level0 row432" >432</th>
      <td id="T_c6ca4_row432_col0" class="data row432 col0" >feature_extractor_video.resnet.trunk.layer2.0.bn1.running_mean</td>
      <td id="T_c6ca4_row432_col1" class="data row432 col1" >feature_extractor_video.resnet.trunk.layer2.0.bn1.running_mean</td>
      <td id="T_c6ca4_row432_col2" class="data row432 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer2.0.bn1.running_mean</td>
      <td id="T_c6ca4_row432_col3" class="data row432 col3" >2.21472</td>
      <td id="T_c6ca4_row432_col4" class="data row432 col4" >12.6926</td>
      <td id="T_c6ca4_row432_col5" class="data row432 col5" >11.8043</td>
      <td id="T_c6ca4_row432_col6" class="data row432 col6" >0.174489</td>
      <td id="T_c6ca4_row432_col7" class="data row432 col7" >0.986265</td>
      <td id="T_c6ca4_row432_col8" class="data row432 col8" >0.671875</td>
      <td id="T_c6ca4_row432_col9" class="data row432 col9" >128.000000</td>
      <td id="T_c6ca4_row432_col10" class="data row432 col10" >torch.float32</td>
      <td id="T_c6ca4_row432_col11" class="data row432 col11" >(128,)</td>
      <td id="T_c6ca4_row432_col12" class="data row432 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row433" class="row_heading level0 row433" >433</th>
      <td id="T_c6ca4_row433_col0" class="data row433 col0" >feature_extractor_video.resnet.trunk.layer2.0.bn1.running_var</td>
      <td id="T_c6ca4_row433_col1" class="data row433 col1" >feature_extractor_video.resnet.trunk.layer2.0.bn1.running_var</td>
      <td id="T_c6ca4_row433_col2" class="data row433 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer2.0.bn1.running_var</td>
      <td id="T_c6ca4_row433_col3" class="data row433 col3" >5.51079</td>
      <td id="T_c6ca4_row433_col4" class="data row433 col4" >23.0577</td>
      <td id="T_c6ca4_row433_col5" class="data row433 col5" >18.137</td>
      <td id="T_c6ca4_row433_col6" class="data row433 col6" >0.239</td>
      <td id="T_c6ca4_row433_col7" class="data row433 col7" >0.99264</td>
      <td id="T_c6ca4_row433_col8" class="data row433 col8" >1.19531</td>
      <td id="T_c6ca4_row433_col9" class="data row433 col9" >128.000000</td>
      <td id="T_c6ca4_row433_col10" class="data row433 col10" >torch.float32</td>
      <td id="T_c6ca4_row433_col11" class="data row433 col11" >(128,)</td>
      <td id="T_c6ca4_row433_col12" class="data row433 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row434" class="row_heading level0 row434" >434</th>
      <td id="T_c6ca4_row434_col0" class="data row434 col0" >feature_extractor_video.resnet.trunk.layer2.0.bn1.weight</td>
      <td id="T_c6ca4_row434_col1" class="data row434 col1" >feature_extractor_video.resnet.trunk.layer2.0.bn1.weight</td>
      <td id="T_c6ca4_row434_col2" class="data row434 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer2.0.bn1.weight</td>
      <td id="T_c6ca4_row434_col3" class="data row434 col3" >0.173784</td>
      <td id="T_c6ca4_row434_col4" class="data row434 col4" >2.26288</td>
      <td id="T_c6ca4_row434_col5" class="data row434 col5" >2.14361</td>
      <td id="T_c6ca4_row434_col6" class="data row434 col6" >0.0767977</td>
      <td id="T_c6ca4_row434_col7" class="data row434 col7" >0.998353</td>
      <td id="T_c6ca4_row434_col8" class="data row434 col8" >0.0389404</td>
      <td id="T_c6ca4_row434_col9" class="data row434 col9" >128.000000</td>
      <td id="T_c6ca4_row434_col10" class="data row434 col10" >torch.float32</td>
      <td id="T_c6ca4_row434_col11" class="data row434 col11" >(128,)</td>
      <td id="T_c6ca4_row434_col12" class="data row434 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row435" class="row_heading level0 row435" >435</th>
      <td id="T_c6ca4_row435_col0" class="data row435 col0" >feature_extractor_video.resnet.trunk.layer2.0.bn2.bias</td>
      <td id="T_c6ca4_row435_col1" class="data row435 col1" >feature_extractor_video.resnet.trunk.layer2.0.bn2.bias</td>
      <td id="T_c6ca4_row435_col2" class="data row435 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer2.0.bn2.bias</td>
      <td id="T_c6ca4_row435_col3" class="data row435 col3" >0.0931827</td>
      <td id="T_c6ca4_row435_col4" class="data row435 col4" >1.01532</td>
      <td id="T_c6ca4_row435_col5" class="data row435 col5" >0.978563</td>
      <td id="T_c6ca4_row435_col6" class="data row435 col6" >0.0917763</td>
      <td id="T_c6ca4_row435_col7" class="data row435 col7" >0.99631</td>
      <td id="T_c6ca4_row435_col8" class="data row435 col8" >0.0292969</td>
      <td id="T_c6ca4_row435_col9" class="data row435 col9" >128.000000</td>
      <td id="T_c6ca4_row435_col10" class="data row435 col10" >torch.float32</td>
      <td id="T_c6ca4_row435_col11" class="data row435 col11" >(128,)</td>
      <td id="T_c6ca4_row435_col12" class="data row435 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row436" class="row_heading level0 row436" >436</th>
      <td id="T_c6ca4_row436_col0" class="data row436 col0" >feature_extractor_video.resnet.trunk.layer2.0.bn2.num_batches_tracked</td>
      <td id="T_c6ca4_row436_col1" class="data row436 col1" >feature_extractor_video.resnet.trunk.layer2.0.bn2.num_batches_tracked</td>
      <td id="T_c6ca4_row436_col2" class="data row436 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer2.0.bn2.num_batches_tracked</td>
      <td id="T_c6ca4_row436_col3" class="data row436 col3" >880066</td>
      <td id="T_c6ca4_row436_col4" class="data row436 col4" >601803</td>
      <td id="T_c6ca4_row436_col5" class="data row436 col5" >1.48187e+06</td>
      <td id="T_c6ca4_row436_col6" class="data row436 col6" >1.46238</td>
      <td id="T_c6ca4_row436_col7" class="data row436 col7" >1</td>
      <td id="T_c6ca4_row436_col8" class="data row436 col8" >880066</td>
      <td id="T_c6ca4_row436_col9" class="data row436 col9" >1.000000</td>
      <td id="T_c6ca4_row436_col10" class="data row436 col10" >torch.int64</td>
      <td id="T_c6ca4_row436_col11" class="data row436 col11" >()</td>
      <td id="T_c6ca4_row436_col12" class="data row436 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row437" class="row_heading level0 row437" >437</th>
      <td id="T_c6ca4_row437_col0" class="data row437 col0" >feature_extractor_video.resnet.trunk.layer2.0.bn2.running_mean</td>
      <td id="T_c6ca4_row437_col1" class="data row437 col1" >feature_extractor_video.resnet.trunk.layer2.0.bn2.running_mean</td>
      <td id="T_c6ca4_row437_col2" class="data row437 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer2.0.bn2.running_mean</td>
      <td id="T_c6ca4_row437_col3" class="data row437 col3" >1.45757</td>
      <td id="T_c6ca4_row437_col4" class="data row437 col4" >10.019</td>
      <td id="T_c6ca4_row437_col5" class="data row437 col5" >9.43841</td>
      <td id="T_c6ca4_row437_col6" class="data row437 col6" >0.145481</td>
      <td id="T_c6ca4_row437_col7" class="data row437 col7" >0.990549</td>
      <td id="T_c6ca4_row437_col8" class="data row437 col8" >0.39093</td>
      <td id="T_c6ca4_row437_col9" class="data row437 col9" >128.000000</td>
      <td id="T_c6ca4_row437_col10" class="data row437 col10" >torch.float32</td>
      <td id="T_c6ca4_row437_col11" class="data row437 col11" >(128,)</td>
      <td id="T_c6ca4_row437_col12" class="data row437 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row438" class="row_heading level0 row438" >438</th>
      <td id="T_c6ca4_row438_col0" class="data row438 col0" >feature_extractor_video.resnet.trunk.layer2.0.bn2.running_var</td>
      <td id="T_c6ca4_row438_col1" class="data row438 col1" >feature_extractor_video.resnet.trunk.layer2.0.bn2.running_var</td>
      <td id="T_c6ca4_row438_col2" class="data row438 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer2.0.bn2.running_var</td>
      <td id="T_c6ca4_row438_col3" class="data row438 col3" >2.4731</td>
      <td id="T_c6ca4_row438_col4" class="data row438 col4" >9.72412</td>
      <td id="T_c6ca4_row438_col5" class="data row438 col5" >7.32321</td>
      <td id="T_c6ca4_row438_col6" class="data row438 col6" >0.254326</td>
      <td id="T_c6ca4_row438_col7" class="data row438 col7" >0.99753</td>
      <td id="T_c6ca4_row438_col8" class="data row438 col8" >0.460938</td>
      <td id="T_c6ca4_row438_col9" class="data row438 col9" >128.000000</td>
      <td id="T_c6ca4_row438_col10" class="data row438 col10" >torch.float32</td>
      <td id="T_c6ca4_row438_col11" class="data row438 col11" >(128,)</td>
      <td id="T_c6ca4_row438_col12" class="data row438 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row439" class="row_heading level0 row439" >439</th>
      <td id="T_c6ca4_row439_col0" class="data row439 col0" >feature_extractor_video.resnet.trunk.layer2.0.bn2.weight</td>
      <td id="T_c6ca4_row439_col1" class="data row439 col1" >feature_extractor_video.resnet.trunk.layer2.0.bn2.weight</td>
      <td id="T_c6ca4_row439_col2" class="data row439 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer2.0.bn2.weight</td>
      <td id="T_c6ca4_row439_col3" class="data row439 col3" >0.137072</td>
      <td id="T_c6ca4_row439_col4" class="data row439 col4" >2.48037</td>
      <td id="T_c6ca4_row439_col5" class="data row439 col5" >2.38436</td>
      <td id="T_c6ca4_row439_col6" class="data row439 col6" >0.0552628</td>
      <td id="T_c6ca4_row439_col7" class="data row439 col7" >0.999191</td>
      <td id="T_c6ca4_row439_col8" class="data row439 col8" >0.0385742</td>
      <td id="T_c6ca4_row439_col9" class="data row439 col9" >128.000000</td>
      <td id="T_c6ca4_row439_col10" class="data row439 col10" >torch.float32</td>
      <td id="T_c6ca4_row439_col11" class="data row439 col11" >(128,)</td>
      <td id="T_c6ca4_row439_col12" class="data row439 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row440" class="row_heading level0 row440" >440</th>
      <td id="T_c6ca4_row440_col0" class="data row440 col0" >feature_extractor_video.resnet.trunk.layer2.0.conv1.weight</td>
      <td id="T_c6ca4_row440_col1" class="data row440 col1" >feature_extractor_video.resnet.trunk.layer2.0.conv1.weight</td>
      <td id="T_c6ca4_row440_col2" class="data row440 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer2.0.conv1.weight</td>
      <td id="T_c6ca4_row440_col3" class="data row440 col3" >4.36008</td>
      <td id="T_c6ca4_row440_col4" class="data row440 col4" >38.4521</td>
      <td id="T_c6ca4_row440_col5" class="data row440 col5" >36.8218</td>
      <td id="T_c6ca4_row440_col6" class="data row440 col6" >0.11339</td>
      <td id="T_c6ca4_row440_col7" class="data row440 col7" >0.994225</td>
      <td id="T_c6ca4_row440_col8" class="data row440 col8" >0.101486</td>
      <td id="T_c6ca4_row440_col9" class="data row440 col9" >73728.000000</td>
      <td id="T_c6ca4_row440_col10" class="data row440 col10" >torch.float32</td>
      <td id="T_c6ca4_row440_col11" class="data row440 col11" >(128, 64, 3, 3)</td>
      <td id="T_c6ca4_row440_col12" class="data row440 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row441" class="row_heading level0 row441" >441</th>
      <td id="T_c6ca4_row441_col0" class="data row441 col0" >feature_extractor_video.resnet.trunk.layer2.0.conv2.weight</td>
      <td id="T_c6ca4_row441_col1" class="data row441 col1" >feature_extractor_video.resnet.trunk.layer2.0.conv2.weight</td>
      <td id="T_c6ca4_row441_col2" class="data row441 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer2.0.conv2.weight</td>
      <td id="T_c6ca4_row441_col3" class="data row441 col3" >6.23719</td>
      <td id="T_c6ca4_row441_col4" class="data row441 col4" >52.7307</td>
      <td id="T_c6ca4_row441_col5" class="data row441 col5" >50.5691</td>
      <td id="T_c6ca4_row441_col6" class="data row441 col6" >0.118284</td>
      <td id="T_c6ca4_row441_col7" class="data row441 col7" >0.993582</td>
      <td id="T_c6ca4_row441_col8" class="data row441 col8" >0.103516</td>
      <td id="T_c6ca4_row441_col9" class="data row441 col9" >147456.000000</td>
      <td id="T_c6ca4_row441_col10" class="data row441 col10" >torch.float32</td>
      <td id="T_c6ca4_row441_col11" class="data row441 col11" >(128, 128, 3, 3)</td>
      <td id="T_c6ca4_row441_col12" class="data row441 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row442" class="row_heading level0 row442" >442</th>
      <td id="T_c6ca4_row442_col0" class="data row442 col0" >feature_extractor_video.resnet.trunk.layer2.0.downsample.0.weight</td>
      <td id="T_c6ca4_row442_col1" class="data row442 col1" >feature_extractor_video.resnet.trunk.layer2.0.downsample.0.weight</td>
      <td id="T_c6ca4_row442_col2" class="data row442 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer2.0.downsample.0.weight</td>
      <td id="T_c6ca4_row442_col3" class="data row442 col3" >1.41997</td>
      <td id="T_c6ca4_row442_col4" class="data row442 col4" >13.8063</td>
      <td id="T_c6ca4_row442_col5" class="data row442 col5" >13.2041</td>
      <td id="T_c6ca4_row442_col6" class="data row442 col6" >0.102849</td>
      <td id="T_c6ca4_row442_col7" class="data row442 col7" >0.995465</td>
      <td id="T_c6ca4_row442_col8" class="data row442 col8" >0.130859</td>
      <td id="T_c6ca4_row442_col9" class="data row442 col9" >8192.000000</td>
      <td id="T_c6ca4_row442_col10" class="data row442 col10" >torch.float32</td>
      <td id="T_c6ca4_row442_col11" class="data row442 col11" >(128, 64, 1, 1)</td>
      <td id="T_c6ca4_row442_col12" class="data row442 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row443" class="row_heading level0 row443" >443</th>
      <td id="T_c6ca4_row443_col0" class="data row443 col0" >feature_extractor_video.resnet.trunk.layer2.0.downsample.1.bias</td>
      <td id="T_c6ca4_row443_col1" class="data row443 col1" >feature_extractor_video.resnet.trunk.layer2.0.downsample.1.bias</td>
      <td id="T_c6ca4_row443_col2" class="data row443 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer2.0.downsample.1.bias</td>
      <td id="T_c6ca4_row443_col3" class="data row443 col3" >0.0931827</td>
      <td id="T_c6ca4_row443_col4" class="data row443 col4" >1.01532</td>
      <td id="T_c6ca4_row443_col5" class="data row443 col5" >0.978563</td>
      <td id="T_c6ca4_row443_col6" class="data row443 col6" >0.0917763</td>
      <td id="T_c6ca4_row443_col7" class="data row443 col7" >0.99631</td>
      <td id="T_c6ca4_row443_col8" class="data row443 col8" >0.0292969</td>
      <td id="T_c6ca4_row443_col9" class="data row443 col9" >128.000000</td>
      <td id="T_c6ca4_row443_col10" class="data row443 col10" >torch.float32</td>
      <td id="T_c6ca4_row443_col11" class="data row443 col11" >(128,)</td>
      <td id="T_c6ca4_row443_col12" class="data row443 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row444" class="row_heading level0 row444" >444</th>
      <td id="T_c6ca4_row444_col0" class="data row444 col0" >feature_extractor_video.resnet.trunk.layer2.0.downsample.1.num_batches_tracked</td>
      <td id="T_c6ca4_row444_col1" class="data row444 col1" >feature_extractor_video.resnet.trunk.layer2.0.downsample.1.num_batches_tracked</td>
      <td id="T_c6ca4_row444_col2" class="data row444 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer2.0.downsample.1.num_batches_tracked</td>
      <td id="T_c6ca4_row444_col3" class="data row444 col3" >880066</td>
      <td id="T_c6ca4_row444_col4" class="data row444 col4" >601803</td>
      <td id="T_c6ca4_row444_col5" class="data row444 col5" >1.48187e+06</td>
      <td id="T_c6ca4_row444_col6" class="data row444 col6" >1.46238</td>
      <td id="T_c6ca4_row444_col7" class="data row444 col7" >1</td>
      <td id="T_c6ca4_row444_col8" class="data row444 col8" >880066</td>
      <td id="T_c6ca4_row444_col9" class="data row444 col9" >1.000000</td>
      <td id="T_c6ca4_row444_col10" class="data row444 col10" >torch.int64</td>
      <td id="T_c6ca4_row444_col11" class="data row444 col11" >()</td>
      <td id="T_c6ca4_row444_col12" class="data row444 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row445" class="row_heading level0 row445" >445</th>
      <td id="T_c6ca4_row445_col0" class="data row445 col0" >feature_extractor_video.resnet.trunk.layer2.0.downsample.1.running_mean</td>
      <td id="T_c6ca4_row445_col1" class="data row445 col1" >feature_extractor_video.resnet.trunk.layer2.0.downsample.1.running_mean</td>
      <td id="T_c6ca4_row445_col2" class="data row445 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer2.0.downsample.1.running_mean</td>
      <td id="T_c6ca4_row445_col3" class="data row445 col3" >0.657864</td>
      <td id="T_c6ca4_row445_col4" class="data row445 col4" >3.98746</td>
      <td id="T_c6ca4_row445_col5" class="data row445 col5" >3.56053</td>
      <td id="T_c6ca4_row445_col6" class="data row445 col6" >0.164983</td>
      <td id="T_c6ca4_row445_col7" class="data row445 col7" >0.991177</td>
      <td id="T_c6ca4_row445_col8" class="data row445 col8" >0.280273</td>
      <td id="T_c6ca4_row445_col9" class="data row445 col9" >128.000000</td>
      <td id="T_c6ca4_row445_col10" class="data row445 col10" >torch.float32</td>
      <td id="T_c6ca4_row445_col11" class="data row445 col11" >(128,)</td>
      <td id="T_c6ca4_row445_col12" class="data row445 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row446" class="row_heading level0 row446" >446</th>
      <td id="T_c6ca4_row446_col0" class="data row446 col0" >feature_extractor_video.resnet.trunk.layer2.0.downsample.1.running_var</td>
      <td id="T_c6ca4_row446_col1" class="data row446 col1" >feature_extractor_video.resnet.trunk.layer2.0.downsample.1.running_var</td>
      <td id="T_c6ca4_row446_col2" class="data row446 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer2.0.downsample.1.running_var</td>
      <td id="T_c6ca4_row446_col3" class="data row446 col3" >0.398258</td>
      <td id="T_c6ca4_row446_col4" class="data row446 col4" >1.47042</td>
      <td id="T_c6ca4_row446_col5" class="data row446 col5" >1.09739</td>
      <td id="T_c6ca4_row446_col6" class="data row446 col6" >0.270846</td>
      <td id="T_c6ca4_row446_col7" class="data row446 col7" >0.993971</td>
      <td id="T_c6ca4_row446_col8" class="data row446 col8" >0.205322</td>
      <td id="T_c6ca4_row446_col9" class="data row446 col9" >128.000000</td>
      <td id="T_c6ca4_row446_col10" class="data row446 col10" >torch.float32</td>
      <td id="T_c6ca4_row446_col11" class="data row446 col11" >(128,)</td>
      <td id="T_c6ca4_row446_col12" class="data row446 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row447" class="row_heading level0 row447" >447</th>
      <td id="T_c6ca4_row447_col0" class="data row447 col0" >feature_extractor_video.resnet.trunk.layer2.0.downsample.1.weight</td>
      <td id="T_c6ca4_row447_col1" class="data row447 col1" >feature_extractor_video.resnet.trunk.layer2.0.downsample.1.weight</td>
      <td id="T_c6ca4_row447_col2" class="data row447 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer2.0.downsample.1.weight</td>
      <td id="T_c6ca4_row447_col3" class="data row447 col3" >0.137859</td>
      <td id="T_c6ca4_row447_col4" class="data row447 col4" >1.59804</td>
      <td id="T_c6ca4_row447_col5" class="data row447 col5" >1.50422</td>
      <td id="T_c6ca4_row447_col6" class="data row447 col6" >0.0862677</td>
      <td id="T_c6ca4_row447_col7" class="data row447 col7" >0.997877</td>
      <td id="T_c6ca4_row447_col8" class="data row447 col8" >0.0686035</td>
      <td id="T_c6ca4_row447_col9" class="data row447 col9" >128.000000</td>
      <td id="T_c6ca4_row447_col10" class="data row447 col10" >torch.float32</td>
      <td id="T_c6ca4_row447_col11" class="data row447 col11" >(128,)</td>
      <td id="T_c6ca4_row447_col12" class="data row447 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row448" class="row_heading level0 row448" >448</th>
      <td id="T_c6ca4_row448_col0" class="data row448 col0" >feature_extractor_video.resnet.trunk.layer2.0.relu1.weight</td>
      <td id="T_c6ca4_row448_col1" class="data row448 col1" >feature_extractor_video.resnet.trunk.layer2.0.relu1.weight</td>
      <td id="T_c6ca4_row448_col2" class="data row448 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer2.0.relu1.weight</td>
      <td id="T_c6ca4_row448_col3" class="data row448 col3" >0.209235</td>
      <td id="T_c6ca4_row448_col4" class="data row448 col4" >4.50728</td>
      <td id="T_c6ca4_row448_col5" class="data row448 col5" >4.36211</td>
      <td id="T_c6ca4_row448_col6" class="data row448 col6" >0.0464214</td>
      <td id="T_c6ca4_row448_col7" class="data row448 col7" >0.999423</td>
      <td id="T_c6ca4_row448_col8" class="data row448 col8" >0.0463867</td>
      <td id="T_c6ca4_row448_col9" class="data row448 col9" >128.000000</td>
      <td id="T_c6ca4_row448_col10" class="data row448 col10" >torch.float32</td>
      <td id="T_c6ca4_row448_col11" class="data row448 col11" >(128,)</td>
      <td id="T_c6ca4_row448_col12" class="data row448 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row449" class="row_heading level0 row449" >449</th>
      <td id="T_c6ca4_row449_col0" class="data row449 col0" >feature_extractor_video.resnet.trunk.layer2.0.relu2.weight</td>
      <td id="T_c6ca4_row449_col1" class="data row449 col1" >feature_extractor_video.resnet.trunk.layer2.0.relu2.weight</td>
      <td id="T_c6ca4_row449_col2" class="data row449 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer2.0.relu2.weight</td>
      <td id="T_c6ca4_row449_col3" class="data row449 col3" >0.238318</td>
      <td id="T_c6ca4_row449_col4" class="data row449 col4" >4.73662</td>
      <td id="T_c6ca4_row449_col5" class="data row449 col5" >4.56986</td>
      <td id="T_c6ca4_row449_col6" class="data row449 col6" >0.050314</td>
      <td id="T_c6ca4_row449_col7" class="data row449 col7" >0.99933</td>
      <td id="T_c6ca4_row449_col8" class="data row449 col8" >0.0541992</td>
      <td id="T_c6ca4_row449_col9" class="data row449 col9" >128.000000</td>
      <td id="T_c6ca4_row449_col10" class="data row449 col10" >torch.float32</td>
      <td id="T_c6ca4_row449_col11" class="data row449 col11" >(128,)</td>
      <td id="T_c6ca4_row449_col12" class="data row449 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row450" class="row_heading level0 row450" >450</th>
      <td id="T_c6ca4_row450_col0" class="data row450 col0" >feature_extractor_video.resnet.trunk.layer2.1.bn1.bias</td>
      <td id="T_c6ca4_row450_col1" class="data row450 col1" >feature_extractor_video.resnet.trunk.layer2.1.bn1.bias</td>
      <td id="T_c6ca4_row450_col2" class="data row450 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer2.1.bn1.bias</td>
      <td id="T_c6ca4_row450_col3" class="data row450 col3" >0.12916</td>
      <td id="T_c6ca4_row450_col4" class="data row450 col4" >1.41337</td>
      <td id="T_c6ca4_row450_col5" class="data row450 col5" >1.36478</td>
      <td id="T_c6ca4_row450_col6" class="data row450 col6" >0.0913843</td>
      <td id="T_c6ca4_row450_col7" class="data row450 col7" >0.996288</td>
      <td id="T_c6ca4_row450_col8" class="data row450 col8" >0.0507812</td>
      <td id="T_c6ca4_row450_col9" class="data row450 col9" >128.000000</td>
      <td id="T_c6ca4_row450_col10" class="data row450 col10" >torch.float32</td>
      <td id="T_c6ca4_row450_col11" class="data row450 col11" >(128,)</td>
      <td id="T_c6ca4_row450_col12" class="data row450 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row451" class="row_heading level0 row451" >451</th>
      <td id="T_c6ca4_row451_col0" class="data row451 col0" >feature_extractor_video.resnet.trunk.layer2.1.bn1.num_batches_tracked</td>
      <td id="T_c6ca4_row451_col1" class="data row451 col1" >feature_extractor_video.resnet.trunk.layer2.1.bn1.num_batches_tracked</td>
      <td id="T_c6ca4_row451_col2" class="data row451 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer2.1.bn1.num_batches_tracked</td>
      <td id="T_c6ca4_row451_col3" class="data row451 col3" >880066</td>
      <td id="T_c6ca4_row451_col4" class="data row451 col4" >601803</td>
      <td id="T_c6ca4_row451_col5" class="data row451 col5" >1.48187e+06</td>
      <td id="T_c6ca4_row451_col6" class="data row451 col6" >1.46238</td>
      <td id="T_c6ca4_row451_col7" class="data row451 col7" >1</td>
      <td id="T_c6ca4_row451_col8" class="data row451 col8" >880066</td>
      <td id="T_c6ca4_row451_col9" class="data row451 col9" >1.000000</td>
      <td id="T_c6ca4_row451_col10" class="data row451 col10" >torch.int64</td>
      <td id="T_c6ca4_row451_col11" class="data row451 col11" >()</td>
      <td id="T_c6ca4_row451_col12" class="data row451 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row452" class="row_heading level0 row452" >452</th>
      <td id="T_c6ca4_row452_col0" class="data row452 col0" >feature_extractor_video.resnet.trunk.layer2.1.bn1.running_mean</td>
      <td id="T_c6ca4_row452_col1" class="data row452 col1" >feature_extractor_video.resnet.trunk.layer2.1.bn1.running_mean</td>
      <td id="T_c6ca4_row452_col2" class="data row452 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer2.1.bn1.running_mean</td>
      <td id="T_c6ca4_row452_col3" class="data row452 col3" >1.2898</td>
      <td id="T_c6ca4_row452_col4" class="data row452 col4" >7.90524</td>
      <td id="T_c6ca4_row452_col5" class="data row452 col5" >7.79799</td>
      <td id="T_c6ca4_row452_col6" class="data row452 col6" >0.163157</td>
      <td id="T_c6ca4_row452_col7" class="data row452 col7" >0.9866</td>
      <td id="T_c6ca4_row452_col8" class="data row452 col8" >0.327637</td>
      <td id="T_c6ca4_row452_col9" class="data row452 col9" >128.000000</td>
      <td id="T_c6ca4_row452_col10" class="data row452 col10" >torch.float32</td>
      <td id="T_c6ca4_row452_col11" class="data row452 col11" >(128,)</td>
      <td id="T_c6ca4_row452_col12" class="data row452 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row453" class="row_heading level0 row453" >453</th>
      <td id="T_c6ca4_row453_col0" class="data row453 col0" >feature_extractor_video.resnet.trunk.layer2.1.bn1.running_var</td>
      <td id="T_c6ca4_row453_col1" class="data row453 col1" >feature_extractor_video.resnet.trunk.layer2.1.bn1.running_var</td>
      <td id="T_c6ca4_row453_col2" class="data row453 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer2.1.bn1.running_var</td>
      <td id="T_c6ca4_row453_col3" class="data row453 col3" >4.52375</td>
      <td id="T_c6ca4_row453_col4" class="data row453 col4" >22.4169</td>
      <td id="T_c6ca4_row453_col5" class="data row453 col5" >18.1175</td>
      <td id="T_c6ca4_row453_col6" class="data row453 col6" >0.2018</td>
      <td id="T_c6ca4_row453_col7" class="data row453 col7" >0.997563</td>
      <td id="T_c6ca4_row453_col8" class="data row453 col8" >1.20508</td>
      <td id="T_c6ca4_row453_col9" class="data row453 col9" >128.000000</td>
      <td id="T_c6ca4_row453_col10" class="data row453 col10" >torch.float32</td>
      <td id="T_c6ca4_row453_col11" class="data row453 col11" >(128,)</td>
      <td id="T_c6ca4_row453_col12" class="data row453 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row454" class="row_heading level0 row454" >454</th>
      <td id="T_c6ca4_row454_col0" class="data row454 col0" >feature_extractor_video.resnet.trunk.layer2.1.bn1.weight</td>
      <td id="T_c6ca4_row454_col1" class="data row454 col1" >feature_extractor_video.resnet.trunk.layer2.1.bn1.weight</td>
      <td id="T_c6ca4_row454_col2" class="data row454 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer2.1.bn1.weight</td>
      <td id="T_c6ca4_row454_col3" class="data row454 col3" >0.146706</td>
      <td id="T_c6ca4_row454_col4" class="data row454 col4" >2.40382</td>
      <td id="T_c6ca4_row454_col5" class="data row454 col5" >2.28957</td>
      <td id="T_c6ca4_row454_col6" class="data row454 col6" >0.0610304</td>
      <td id="T_c6ca4_row454_col7" class="data row454 col7" >0.99923</td>
      <td id="T_c6ca4_row454_col8" class="data row454 col8" >0.0340576</td>
      <td id="T_c6ca4_row454_col9" class="data row454 col9" >128.000000</td>
      <td id="T_c6ca4_row454_col10" class="data row454 col10" >torch.float32</td>
      <td id="T_c6ca4_row454_col11" class="data row454 col11" >(128,)</td>
      <td id="T_c6ca4_row454_col12" class="data row454 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row455" class="row_heading level0 row455" >455</th>
      <td id="T_c6ca4_row455_col0" class="data row455 col0" >feature_extractor_video.resnet.trunk.layer2.1.bn2.bias</td>
      <td id="T_c6ca4_row455_col1" class="data row455 col1" >feature_extractor_video.resnet.trunk.layer2.1.bn2.bias</td>
      <td id="T_c6ca4_row455_col2" class="data row455 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer2.1.bn2.bias</td>
      <td id="T_c6ca4_row455_col3" class="data row455 col3" >0.135476</td>
      <td id="T_c6ca4_row455_col4" class="data row455 col4" >1.83737</td>
      <td id="T_c6ca4_row455_col5" class="data row455 col5" >1.77489</td>
      <td id="T_c6ca4_row455_col6" class="data row455 col6" >0.0737336</td>
      <td id="T_c6ca4_row455_col7" class="data row455 col7" >0.997784</td>
      <td id="T_c6ca4_row455_col8" class="data row455 col8" >0.0422363</td>
      <td id="T_c6ca4_row455_col9" class="data row455 col9" >128.000000</td>
      <td id="T_c6ca4_row455_col10" class="data row455 col10" >torch.float32</td>
      <td id="T_c6ca4_row455_col11" class="data row455 col11" >(128,)</td>
      <td id="T_c6ca4_row455_col12" class="data row455 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row456" class="row_heading level0 row456" >456</th>
      <td id="T_c6ca4_row456_col0" class="data row456 col0" >feature_extractor_video.resnet.trunk.layer2.1.bn2.num_batches_tracked</td>
      <td id="T_c6ca4_row456_col1" class="data row456 col1" >feature_extractor_video.resnet.trunk.layer2.1.bn2.num_batches_tracked</td>
      <td id="T_c6ca4_row456_col2" class="data row456 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer2.1.bn2.num_batches_tracked</td>
      <td id="T_c6ca4_row456_col3" class="data row456 col3" >880066</td>
      <td id="T_c6ca4_row456_col4" class="data row456 col4" >601803</td>
      <td id="T_c6ca4_row456_col5" class="data row456 col5" >1.48187e+06</td>
      <td id="T_c6ca4_row456_col6" class="data row456 col6" >1.46238</td>
      <td id="T_c6ca4_row456_col7" class="data row456 col7" >1</td>
      <td id="T_c6ca4_row456_col8" class="data row456 col8" >880066</td>
      <td id="T_c6ca4_row456_col9" class="data row456 col9" >1.000000</td>
      <td id="T_c6ca4_row456_col10" class="data row456 col10" >torch.int64</td>
      <td id="T_c6ca4_row456_col11" class="data row456 col11" >()</td>
      <td id="T_c6ca4_row456_col12" class="data row456 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row457" class="row_heading level0 row457" >457</th>
      <td id="T_c6ca4_row457_col0" class="data row457 col0" >feature_extractor_video.resnet.trunk.layer2.1.bn2.running_mean</td>
      <td id="T_c6ca4_row457_col1" class="data row457 col1" >feature_extractor_video.resnet.trunk.layer2.1.bn2.running_mean</td>
      <td id="T_c6ca4_row457_col2" class="data row457 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer2.1.bn2.running_mean</td>
      <td id="T_c6ca4_row457_col3" class="data row457 col3" >1.67737</td>
      <td id="T_c6ca4_row457_col4" class="data row457 col4" >7.63996</td>
      <td id="T_c6ca4_row457_col5" class="data row457 col5" >6.90231</td>
      <td id="T_c6ca4_row457_col6" class="data row457 col6" >0.219552</td>
      <td id="T_c6ca4_row457_col7" class="data row457 col7" >0.978482</td>
      <td id="T_c6ca4_row457_col8" class="data row457 col8" >0.430664</td>
      <td id="T_c6ca4_row457_col9" class="data row457 col9" >128.000000</td>
      <td id="T_c6ca4_row457_col10" class="data row457 col10" >torch.float32</td>
      <td id="T_c6ca4_row457_col11" class="data row457 col11" >(128,)</td>
      <td id="T_c6ca4_row457_col12" class="data row457 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row458" class="row_heading level0 row458" >458</th>
      <td id="T_c6ca4_row458_col0" class="data row458 col0" >feature_extractor_video.resnet.trunk.layer2.1.bn2.running_var</td>
      <td id="T_c6ca4_row458_col1" class="data row458 col1" >feature_extractor_video.resnet.trunk.layer2.1.bn2.running_var</td>
      <td id="T_c6ca4_row458_col2" class="data row458 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer2.1.bn2.running_var</td>
      <td id="T_c6ca4_row458_col3" class="data row458 col3" >1.34927</td>
      <td id="T_c6ca4_row458_col4" class="data row458 col4" >5.51734</td>
      <td id="T_c6ca4_row458_col5" class="data row458 col5" >4.22868</td>
      <td id="T_c6ca4_row458_col6" class="data row458 col6" >0.24455</td>
      <td id="T_c6ca4_row458_col7" class="data row458 col7" >0.996574</td>
      <td id="T_c6ca4_row458_col8" class="data row458 col8" >0.507324</td>
      <td id="T_c6ca4_row458_col9" class="data row458 col9" >128.000000</td>
      <td id="T_c6ca4_row458_col10" class="data row458 col10" >torch.float32</td>
      <td id="T_c6ca4_row458_col11" class="data row458 col11" >(128,)</td>
      <td id="T_c6ca4_row458_col12" class="data row458 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row459" class="row_heading level0 row459" >459</th>
      <td id="T_c6ca4_row459_col0" class="data row459 col0" >feature_extractor_video.resnet.trunk.layer2.1.bn2.weight</td>
      <td id="T_c6ca4_row459_col1" class="data row459 col1" >feature_extractor_video.resnet.trunk.layer2.1.bn2.weight</td>
      <td id="T_c6ca4_row459_col2" class="data row459 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer2.1.bn2.weight</td>
      <td id="T_c6ca4_row459_col3" class="data row459 col3" >0.134743</td>
      <td id="T_c6ca4_row459_col4" class="data row459 col4" >2.59752</td>
      <td id="T_c6ca4_row459_col5" class="data row459 col5" >2.49877</td>
      <td id="T_c6ca4_row459_col6" class="data row459 col6" >0.0518735</td>
      <td id="T_c6ca4_row459_col7" class="data row459 col7" >0.999353</td>
      <td id="T_c6ca4_row459_col8" class="data row459 col8" >0.0351562</td>
      <td id="T_c6ca4_row459_col9" class="data row459 col9" >128.000000</td>
      <td id="T_c6ca4_row459_col10" class="data row459 col10" >torch.float32</td>
      <td id="T_c6ca4_row459_col11" class="data row459 col11" >(128,)</td>
      <td id="T_c6ca4_row459_col12" class="data row459 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row460" class="row_heading level0 row460" >460</th>
      <td id="T_c6ca4_row460_col0" class="data row460 col0" >feature_extractor_video.resnet.trunk.layer2.1.conv1.weight</td>
      <td id="T_c6ca4_row460_col1" class="data row460 col1" >feature_extractor_video.resnet.trunk.layer2.1.conv1.weight</td>
      <td id="T_c6ca4_row460_col2" class="data row460 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer2.1.conv1.weight</td>
      <td id="T_c6ca4_row460_col3" class="data row460 col3" >6.48246</td>
      <td id="T_c6ca4_row460_col4" class="data row460 col4" >53.5007</td>
      <td id="T_c6ca4_row460_col5" class="data row460 col5" >51.2672</td>
      <td id="T_c6ca4_row460_col6" class="data row460 col6" >0.121166</td>
      <td id="T_c6ca4_row460_col7" class="data row460 col7" >0.993249</td>
      <td id="T_c6ca4_row460_col8" class="data row460 col8" >0.0830078</td>
      <td id="T_c6ca4_row460_col9" class="data row460 col9" >147456.000000</td>
      <td id="T_c6ca4_row460_col10" class="data row460 col10" >torch.float32</td>
      <td id="T_c6ca4_row460_col11" class="data row460 col11" >(128, 128, 3, 3)</td>
      <td id="T_c6ca4_row460_col12" class="data row460 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row461" class="row_heading level0 row461" >461</th>
      <td id="T_c6ca4_row461_col0" class="data row461 col0" >feature_extractor_video.resnet.trunk.layer2.1.conv2.weight</td>
      <td id="T_c6ca4_row461_col1" class="data row461 col1" >feature_extractor_video.resnet.trunk.layer2.1.conv2.weight</td>
      <td id="T_c6ca4_row461_col2" class="data row461 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer2.1.conv2.weight</td>
      <td id="T_c6ca4_row461_col3" class="data row461 col3" >6.31287</td>
      <td id="T_c6ca4_row461_col4" class="data row461 col4" >51.4</td>
      <td id="T_c6ca4_row461_col5" class="data row461 col5" >49.2705</td>
      <td id="T_c6ca4_row461_col6" class="data row461 col6" >0.122819</td>
      <td id="T_c6ca4_row461_col7" class="data row461 col7" >0.993027</td>
      <td id="T_c6ca4_row461_col8" class="data row461 col8" >0.0905762</td>
      <td id="T_c6ca4_row461_col9" class="data row461 col9" >147456.000000</td>
      <td id="T_c6ca4_row461_col10" class="data row461 col10" >torch.float32</td>
      <td id="T_c6ca4_row461_col11" class="data row461 col11" >(128, 128, 3, 3)</td>
      <td id="T_c6ca4_row461_col12" class="data row461 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row462" class="row_heading level0 row462" >462</th>
      <td id="T_c6ca4_row462_col0" class="data row462 col0" >feature_extractor_video.resnet.trunk.layer2.1.relu1.weight</td>
      <td id="T_c6ca4_row462_col1" class="data row462 col1" >feature_extractor_video.resnet.trunk.layer2.1.relu1.weight</td>
      <td id="T_c6ca4_row462_col2" class="data row462 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer2.1.relu1.weight</td>
      <td id="T_c6ca4_row462_col3" class="data row462 col3" >0.166447</td>
      <td id="T_c6ca4_row462_col4" class="data row462 col4" >3.32665</td>
      <td id="T_c6ca4_row462_col5" class="data row462 col5" >3.20838</td>
      <td id="T_c6ca4_row462_col6" class="data row462 col6" >0.0500344</td>
      <td id="T_c6ca4_row462_col7" class="data row462 col7" >0.999357</td>
      <td id="T_c6ca4_row462_col8" class="data row462 col8" >0.0385742</td>
      <td id="T_c6ca4_row462_col9" class="data row462 col9" >128.000000</td>
      <td id="T_c6ca4_row462_col10" class="data row462 col10" >torch.float32</td>
      <td id="T_c6ca4_row462_col11" class="data row462 col11" >(128,)</td>
      <td id="T_c6ca4_row462_col12" class="data row462 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row463" class="row_heading level0 row463" >463</th>
      <td id="T_c6ca4_row463_col0" class="data row463 col0" >feature_extractor_video.resnet.trunk.layer2.1.relu2.weight</td>
      <td id="T_c6ca4_row463_col1" class="data row463 col1" >feature_extractor_video.resnet.trunk.layer2.1.relu2.weight</td>
      <td id="T_c6ca4_row463_col2" class="data row463 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer2.1.relu2.weight</td>
      <td id="T_c6ca4_row463_col3" class="data row463 col3" >0.259532</td>
      <td id="T_c6ca4_row463_col4" class="data row463 col4" >5.79229</td>
      <td id="T_c6ca4_row463_col5" class="data row463 col5" >5.59955</td>
      <td id="T_c6ca4_row463_col6" class="data row463 col6" >0.0448064</td>
      <td id="T_c6ca4_row463_col7" class="data row463 col7" >0.999534</td>
      <td id="T_c6ca4_row463_col8" class="data row463 col8" >0.052002</td>
      <td id="T_c6ca4_row463_col9" class="data row463 col9" >128.000000</td>
      <td id="T_c6ca4_row463_col10" class="data row463 col10" >torch.float32</td>
      <td id="T_c6ca4_row463_col11" class="data row463 col11" >(128,)</td>
      <td id="T_c6ca4_row463_col12" class="data row463 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row464" class="row_heading level0 row464" >464</th>
      <td id="T_c6ca4_row464_col0" class="data row464 col0" >feature_extractor_video.resnet.trunk.layer3.0.bn1.bias</td>
      <td id="T_c6ca4_row464_col1" class="data row464 col1" >feature_extractor_video.resnet.trunk.layer3.0.bn1.bias</td>
      <td id="T_c6ca4_row464_col2" class="data row464 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer3.0.bn1.bias</td>
      <td id="T_c6ca4_row464_col3" class="data row464 col3" >0.204435</td>
      <td id="T_c6ca4_row464_col4" class="data row464 col4" >2.14153</td>
      <td id="T_c6ca4_row464_col5" class="data row464 col5" >2.07909</td>
      <td id="T_c6ca4_row464_col6" class="data row464 col6" >0.0954621</td>
      <td id="T_c6ca4_row464_col7" class="data row464 col7" >0.995744</td>
      <td id="T_c6ca4_row464_col8" class="data row464 col8" >0.0761719</td>
      <td id="T_c6ca4_row464_col9" class="data row464 col9" >256.000000</td>
      <td id="T_c6ca4_row464_col10" class="data row464 col10" >torch.float32</td>
      <td id="T_c6ca4_row464_col11" class="data row464 col11" >(256,)</td>
      <td id="T_c6ca4_row464_col12" class="data row464 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row465" class="row_heading level0 row465" >465</th>
      <td id="T_c6ca4_row465_col0" class="data row465 col0" >feature_extractor_video.resnet.trunk.layer3.0.bn1.num_batches_tracked</td>
      <td id="T_c6ca4_row465_col1" class="data row465 col1" >feature_extractor_video.resnet.trunk.layer3.0.bn1.num_batches_tracked</td>
      <td id="T_c6ca4_row465_col2" class="data row465 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer3.0.bn1.num_batches_tracked</td>
      <td id="T_c6ca4_row465_col3" class="data row465 col3" >880066</td>
      <td id="T_c6ca4_row465_col4" class="data row465 col4" >601803</td>
      <td id="T_c6ca4_row465_col5" class="data row465 col5" >1.48187e+06</td>
      <td id="T_c6ca4_row465_col6" class="data row465 col6" >1.46238</td>
      <td id="T_c6ca4_row465_col7" class="data row465 col7" >1</td>
      <td id="T_c6ca4_row465_col8" class="data row465 col8" >880066</td>
      <td id="T_c6ca4_row465_col9" class="data row465 col9" >1.000000</td>
      <td id="T_c6ca4_row465_col10" class="data row465 col10" >torch.int64</td>
      <td id="T_c6ca4_row465_col11" class="data row465 col11" >()</td>
      <td id="T_c6ca4_row465_col12" class="data row465 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row466" class="row_heading level0 row466" >466</th>
      <td id="T_c6ca4_row466_col0" class="data row466 col0" >feature_extractor_video.resnet.trunk.layer3.0.bn1.running_mean</td>
      <td id="T_c6ca4_row466_col1" class="data row466 col1" >feature_extractor_video.resnet.trunk.layer3.0.bn1.running_mean</td>
      <td id="T_c6ca4_row466_col2" class="data row466 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer3.0.bn1.running_mean</td>
      <td id="T_c6ca4_row466_col3" class="data row466 col3" >3.14203</td>
      <td id="T_c6ca4_row466_col4" class="data row466 col4" >17.3553</td>
      <td id="T_c6ca4_row466_col5" class="data row466 col5" >15.9006</td>
      <td id="T_c6ca4_row466_col6" class="data row466 col6" >0.181042</td>
      <td id="T_c6ca4_row466_col7" class="data row466 col7" >0.985947</td>
      <td id="T_c6ca4_row466_col8" class="data row466 col8" >0.698242</td>
      <td id="T_c6ca4_row466_col9" class="data row466 col9" >256.000000</td>
      <td id="T_c6ca4_row466_col10" class="data row466 col10" >torch.float32</td>
      <td id="T_c6ca4_row466_col11" class="data row466 col11" >(256,)</td>
      <td id="T_c6ca4_row466_col12" class="data row466 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row467" class="row_heading level0 row467" >467</th>
      <td id="T_c6ca4_row467_col0" class="data row467 col0" >feature_extractor_video.resnet.trunk.layer3.0.bn1.running_var</td>
      <td id="T_c6ca4_row467_col1" class="data row467 col1" >feature_extractor_video.resnet.trunk.layer3.0.bn1.running_var</td>
      <td id="T_c6ca4_row467_col2" class="data row467 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer3.0.bn1.running_var</td>
      <td id="T_c6ca4_row467_col3" class="data row467 col3" >9.16982</td>
      <td id="T_c6ca4_row467_col4" class="data row467 col4" >39.5381</td>
      <td id="T_c6ca4_row467_col5" class="data row467 col5" >30.7609</td>
      <td id="T_c6ca4_row467_col6" class="data row467 col6" >0.231924</td>
      <td id="T_c6ca4_row467_col7" class="data row467 col7" >0.997103</td>
      <td id="T_c6ca4_row467_col8" class="data row467 col8" >1.59375</td>
      <td id="T_c6ca4_row467_col9" class="data row467 col9" >256.000000</td>
      <td id="T_c6ca4_row467_col10" class="data row467 col10" >torch.float32</td>
      <td id="T_c6ca4_row467_col11" class="data row467 col11" >(256,)</td>
      <td id="T_c6ca4_row467_col12" class="data row467 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row468" class="row_heading level0 row468" >468</th>
      <td id="T_c6ca4_row468_col0" class="data row468 col0" >feature_extractor_video.resnet.trunk.layer3.0.bn1.weight</td>
      <td id="T_c6ca4_row468_col1" class="data row468 col1" >feature_extractor_video.resnet.trunk.layer3.0.bn1.weight</td>
      <td id="T_c6ca4_row468_col2" class="data row468 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer3.0.bn1.weight</td>
      <td id="T_c6ca4_row468_col3" class="data row468 col3" >0.229402</td>
      <td id="T_c6ca4_row468_col4" class="data row468 col4" >3.378</td>
      <td id="T_c6ca4_row468_col5" class="data row468 col5" >3.21825</td>
      <td id="T_c6ca4_row468_col6" class="data row468 col6" >0.0679105</td>
      <td id="T_c6ca4_row468_col7" class="data row468 col7" >0.998753</td>
      <td id="T_c6ca4_row468_col8" class="data row468 col8" >0.0534668</td>
      <td id="T_c6ca4_row468_col9" class="data row468 col9" >256.000000</td>
      <td id="T_c6ca4_row468_col10" class="data row468 col10" >torch.float32</td>
      <td id="T_c6ca4_row468_col11" class="data row468 col11" >(256,)</td>
      <td id="T_c6ca4_row468_col12" class="data row468 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row469" class="row_heading level0 row469" >469</th>
      <td id="T_c6ca4_row469_col0" class="data row469 col0" >feature_extractor_video.resnet.trunk.layer3.0.bn2.bias</td>
      <td id="T_c6ca4_row469_col1" class="data row469 col1" >feature_extractor_video.resnet.trunk.layer3.0.bn2.bias</td>
      <td id="T_c6ca4_row469_col2" class="data row469 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer3.0.bn2.bias</td>
      <td id="T_c6ca4_row469_col3" class="data row469 col3" >0.160413</td>
      <td id="T_c6ca4_row469_col4" class="data row469 col4" >2.06753</td>
      <td id="T_c6ca4_row469_col5" class="data row469 col5" >1.99735</td>
      <td id="T_c6ca4_row469_col6" class="data row469 col6" >0.0775867</td>
      <td id="T_c6ca4_row469_col7" class="data row469 col7" >0.997481</td>
      <td id="T_c6ca4_row469_col8" class="data row469 col8" >0.0859375</td>
      <td id="T_c6ca4_row469_col9" class="data row469 col9" >256.000000</td>
      <td id="T_c6ca4_row469_col10" class="data row469 col10" >torch.float32</td>
      <td id="T_c6ca4_row469_col11" class="data row469 col11" >(256,)</td>
      <td id="T_c6ca4_row469_col12" class="data row469 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row470" class="row_heading level0 row470" >470</th>
      <td id="T_c6ca4_row470_col0" class="data row470 col0" >feature_extractor_video.resnet.trunk.layer3.0.bn2.num_batches_tracked</td>
      <td id="T_c6ca4_row470_col1" class="data row470 col1" >feature_extractor_video.resnet.trunk.layer3.0.bn2.num_batches_tracked</td>
      <td id="T_c6ca4_row470_col2" class="data row470 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer3.0.bn2.num_batches_tracked</td>
      <td id="T_c6ca4_row470_col3" class="data row470 col3" >880066</td>
      <td id="T_c6ca4_row470_col4" class="data row470 col4" >601803</td>
      <td id="T_c6ca4_row470_col5" class="data row470 col5" >1.48187e+06</td>
      <td id="T_c6ca4_row470_col6" class="data row470 col6" >1.46238</td>
      <td id="T_c6ca4_row470_col7" class="data row470 col7" >1</td>
      <td id="T_c6ca4_row470_col8" class="data row470 col8" >880066</td>
      <td id="T_c6ca4_row470_col9" class="data row470 col9" >1.000000</td>
      <td id="T_c6ca4_row470_col10" class="data row470 col10" >torch.int64</td>
      <td id="T_c6ca4_row470_col11" class="data row470 col11" >()</td>
      <td id="T_c6ca4_row470_col12" class="data row470 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row471" class="row_heading level0 row471" >471</th>
      <td id="T_c6ca4_row471_col0" class="data row471 col0" >feature_extractor_video.resnet.trunk.layer3.0.bn2.running_mean</td>
      <td id="T_c6ca4_row471_col1" class="data row471 col1" >feature_extractor_video.resnet.trunk.layer3.0.bn2.running_mean</td>
      <td id="T_c6ca4_row471_col2" class="data row471 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer3.0.bn2.running_mean</td>
      <td id="T_c6ca4_row471_col3" class="data row471 col3" >3.08385</td>
      <td id="T_c6ca4_row471_col4" class="data row471 col4" >14.1577</td>
      <td id="T_c6ca4_row471_col5" class="data row471 col5" >12.2686</td>
      <td id="T_c6ca4_row471_col6" class="data row471 col6" >0.217821</td>
      <td id="T_c6ca4_row471_col7" class="data row471 col7" >0.982898</td>
      <td id="T_c6ca4_row471_col8" class="data row471 col8" >0.797852</td>
      <td id="T_c6ca4_row471_col9" class="data row471 col9" >256.000000</td>
      <td id="T_c6ca4_row471_col10" class="data row471 col10" >torch.float32</td>
      <td id="T_c6ca4_row471_col11" class="data row471 col11" >(256,)</td>
      <td id="T_c6ca4_row471_col12" class="data row471 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row472" class="row_heading level0 row472" >472</th>
      <td id="T_c6ca4_row472_col0" class="data row472 col0" >feature_extractor_video.resnet.trunk.layer3.0.bn2.running_var</td>
      <td id="T_c6ca4_row472_col1" class="data row472 col1" >feature_extractor_video.resnet.trunk.layer3.0.bn2.running_var</td>
      <td id="T_c6ca4_row472_col2" class="data row472 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer3.0.bn2.running_var</td>
      <td id="T_c6ca4_row472_col3" class="data row472 col3" >6.35708</td>
      <td id="T_c6ca4_row472_col4" class="data row472 col4" >23.9722</td>
      <td id="T_c6ca4_row472_col5" class="data row472 col5" >17.9284</td>
      <td id="T_c6ca4_row472_col6" class="data row472 col6" >0.265185</td>
      <td id="T_c6ca4_row472_col7" class="data row472 col7" >0.99548</td>
      <td id="T_c6ca4_row472_col8" class="data row472 col8" >1.70117</td>
      <td id="T_c6ca4_row472_col9" class="data row472 col9" >256.000000</td>
      <td id="T_c6ca4_row472_col10" class="data row472 col10" >torch.float32</td>
      <td id="T_c6ca4_row472_col11" class="data row472 col11" >(256,)</td>
      <td id="T_c6ca4_row472_col12" class="data row472 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row473" class="row_heading level0 row473" >473</th>
      <td id="T_c6ca4_row473_col0" class="data row473 col0" >feature_extractor_video.resnet.trunk.layer3.0.bn2.weight</td>
      <td id="T_c6ca4_row473_col1" class="data row473 col1" >feature_extractor_video.resnet.trunk.layer3.0.bn2.weight</td>
      <td id="T_c6ca4_row473_col2" class="data row473 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer3.0.bn2.weight</td>
      <td id="T_c6ca4_row473_col3" class="data row473 col3" >0.193179</td>
      <td id="T_c6ca4_row473_col4" class="data row473 col4" >3.95748</td>
      <td id="T_c6ca4_row473_col5" class="data row473 col5" >3.84345</td>
      <td id="T_c6ca4_row473_col6" class="data row473 col6" >0.0488138</td>
      <td id="T_c6ca4_row473_col7" class="data row473 col7" >0.999201</td>
      <td id="T_c6ca4_row473_col8" class="data row473 col8" >0.0490723</td>
      <td id="T_c6ca4_row473_col9" class="data row473 col9" >256.000000</td>
      <td id="T_c6ca4_row473_col10" class="data row473 col10" >torch.float32</td>
      <td id="T_c6ca4_row473_col11" class="data row473 col11" >(256,)</td>
      <td id="T_c6ca4_row473_col12" class="data row473 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row474" class="row_heading level0 row474" >474</th>
      <td id="T_c6ca4_row474_col0" class="data row474 col0" >feature_extractor_video.resnet.trunk.layer3.0.conv1.weight</td>
      <td id="T_c6ca4_row474_col1" class="data row474 col1" >feature_extractor_video.resnet.trunk.layer3.0.conv1.weight</td>
      <td id="T_c6ca4_row474_col2" class="data row474 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer3.0.conv1.weight</td>
      <td id="T_c6ca4_row474_col3" class="data row474 col3" >9.02219</td>
      <td id="T_c6ca4_row474_col4" class="data row474 col4" >70.3312</td>
      <td id="T_c6ca4_row474_col5" class="data row474 col5" >67.4551</td>
      <td id="T_c6ca4_row474_col6" class="data row474 col6" >0.128281</td>
      <td id="T_c6ca4_row474_col7" class="data row474 col7" >0.992293</td>
      <td id="T_c6ca4_row474_col8" class="data row474 col8" >0.104492</td>
      <td id="T_c6ca4_row474_col9" class="data row474 col9" >294912.000000</td>
      <td id="T_c6ca4_row474_col10" class="data row474 col10" >torch.float32</td>
      <td id="T_c6ca4_row474_col11" class="data row474 col11" >(256, 128, 3, 3)</td>
      <td id="T_c6ca4_row474_col12" class="data row474 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row475" class="row_heading level0 row475" >475</th>
      <td id="T_c6ca4_row475_col0" class="data row475 col0" >feature_extractor_video.resnet.trunk.layer3.0.conv2.weight</td>
      <td id="T_c6ca4_row475_col1" class="data row475 col1" >feature_extractor_video.resnet.trunk.layer3.0.conv2.weight</td>
      <td id="T_c6ca4_row475_col2" class="data row475 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer3.0.conv2.weight</td>
      <td id="T_c6ca4_row475_col3" class="data row475 col3" >12.6106</td>
      <td id="T_c6ca4_row475_col4" class="data row475 col4" >93.7009</td>
      <td id="T_c6ca4_row475_col5" class="data row475 col5" >89.9587</td>
      <td id="T_c6ca4_row475_col6" class="data row475 col6" >0.134584</td>
      <td id="T_c6ca4_row475_col7" class="data row475 col7" >0.991398</td>
      <td id="T_c6ca4_row475_col8" class="data row475 col8" >0.12207</td>
      <td id="T_c6ca4_row475_col9" class="data row475 col9" >589824.000000</td>
      <td id="T_c6ca4_row475_col10" class="data row475 col10" >torch.float32</td>
      <td id="T_c6ca4_row475_col11" class="data row475 col11" >(256, 256, 3, 3)</td>
      <td id="T_c6ca4_row475_col12" class="data row475 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row476" class="row_heading level0 row476" >476</th>
      <td id="T_c6ca4_row476_col0" class="data row476 col0" >feature_extractor_video.resnet.trunk.layer3.0.downsample.0.weight</td>
      <td id="T_c6ca4_row476_col1" class="data row476 col1" >feature_extractor_video.resnet.trunk.layer3.0.downsample.0.weight</td>
      <td id="T_c6ca4_row476_col2" class="data row476 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer3.0.downsample.0.weight</td>
      <td id="T_c6ca4_row476_col3" class="data row476 col3" >3.04472</td>
      <td id="T_c6ca4_row476_col4" class="data row476 col4" >24.888</td>
      <td id="T_c6ca4_row476_col5" class="data row476 col5" >23.8393</td>
      <td id="T_c6ca4_row476_col6" class="data row476 col6" >0.122337</td>
      <td id="T_c6ca4_row476_col7" class="data row476 col7" >0.993114</td>
      <td id="T_c6ca4_row476_col8" class="data row476 col8" >0.0975342</td>
      <td id="T_c6ca4_row476_col9" class="data row476 col9" >32768.000000</td>
      <td id="T_c6ca4_row476_col10" class="data row476 col10" >torch.float32</td>
      <td id="T_c6ca4_row476_col11" class="data row476 col11" >(256, 128, 1, 1)</td>
      <td id="T_c6ca4_row476_col12" class="data row476 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row477" class="row_heading level0 row477" >477</th>
      <td id="T_c6ca4_row477_col0" class="data row477 col0" >feature_extractor_video.resnet.trunk.layer3.0.downsample.1.bias</td>
      <td id="T_c6ca4_row477_col1" class="data row477 col1" >feature_extractor_video.resnet.trunk.layer3.0.downsample.1.bias</td>
      <td id="T_c6ca4_row477_col2" class="data row477 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer3.0.downsample.1.bias</td>
      <td id="T_c6ca4_row477_col3" class="data row477 col3" >0.160413</td>
      <td id="T_c6ca4_row477_col4" class="data row477 col4" >2.06753</td>
      <td id="T_c6ca4_row477_col5" class="data row477 col5" >1.99735</td>
      <td id="T_c6ca4_row477_col6" class="data row477 col6" >0.0775867</td>
      <td id="T_c6ca4_row477_col7" class="data row477 col7" >0.997481</td>
      <td id="T_c6ca4_row477_col8" class="data row477 col8" >0.0859375</td>
      <td id="T_c6ca4_row477_col9" class="data row477 col9" >256.000000</td>
      <td id="T_c6ca4_row477_col10" class="data row477 col10" >torch.float32</td>
      <td id="T_c6ca4_row477_col11" class="data row477 col11" >(256,)</td>
      <td id="T_c6ca4_row477_col12" class="data row477 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row478" class="row_heading level0 row478" >478</th>
      <td id="T_c6ca4_row478_col0" class="data row478 col0" >feature_extractor_video.resnet.trunk.layer3.0.downsample.1.num_batches_tracked</td>
      <td id="T_c6ca4_row478_col1" class="data row478 col1" >feature_extractor_video.resnet.trunk.layer3.0.downsample.1.num_batches_tracked</td>
      <td id="T_c6ca4_row478_col2" class="data row478 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer3.0.downsample.1.num_batches_tracked</td>
      <td id="T_c6ca4_row478_col3" class="data row478 col3" >880066</td>
      <td id="T_c6ca4_row478_col4" class="data row478 col4" >601803</td>
      <td id="T_c6ca4_row478_col5" class="data row478 col5" >1.48187e+06</td>
      <td id="T_c6ca4_row478_col6" class="data row478 col6" >1.46238</td>
      <td id="T_c6ca4_row478_col7" class="data row478 col7" >1</td>
      <td id="T_c6ca4_row478_col8" class="data row478 col8" >880066</td>
      <td id="T_c6ca4_row478_col9" class="data row478 col9" >1.000000</td>
      <td id="T_c6ca4_row478_col10" class="data row478 col10" >torch.int64</td>
      <td id="T_c6ca4_row478_col11" class="data row478 col11" >()</td>
      <td id="T_c6ca4_row478_col12" class="data row478 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row479" class="row_heading level0 row479" >479</th>
      <td id="T_c6ca4_row479_col0" class="data row479 col0" >feature_extractor_video.resnet.trunk.layer3.0.downsample.1.running_mean</td>
      <td id="T_c6ca4_row479_col1" class="data row479 col1" >feature_extractor_video.resnet.trunk.layer3.0.downsample.1.running_mean</td>
      <td id="T_c6ca4_row479_col2" class="data row479 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer3.0.downsample.1.running_mean</td>
      <td id="T_c6ca4_row479_col3" class="data row479 col3" >0.756236</td>
      <td id="T_c6ca4_row479_col4" class="data row479 col4" >4.39583</td>
      <td id="T_c6ca4_row479_col5" class="data row479 col5" >4.08468</td>
      <td id="T_c6ca4_row479_col6" class="data row479 col6" >0.172035</td>
      <td id="T_c6ca4_row479_col7" class="data row479 col7" >0.986771</td>
      <td id="T_c6ca4_row479_col8" class="data row479 col8" >0.140961</td>
      <td id="T_c6ca4_row479_col9" class="data row479 col9" >256.000000</td>
      <td id="T_c6ca4_row479_col10" class="data row479 col10" >torch.float32</td>
      <td id="T_c6ca4_row479_col11" class="data row479 col11" >(256,)</td>
      <td id="T_c6ca4_row479_col12" class="data row479 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row480" class="row_heading level0 row480" >480</th>
      <td id="T_c6ca4_row480_col0" class="data row480 col0" >feature_extractor_video.resnet.trunk.layer3.0.downsample.1.running_var</td>
      <td id="T_c6ca4_row480_col1" class="data row480 col1" >feature_extractor_video.resnet.trunk.layer3.0.downsample.1.running_var</td>
      <td id="T_c6ca4_row480_col2" class="data row480 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer3.0.downsample.1.running_var</td>
      <td id="T_c6ca4_row480_col3" class="data row480 col3" >0.765435</td>
      <td id="T_c6ca4_row480_col4" class="data row480 col4" >3.06955</td>
      <td id="T_c6ca4_row480_col5" class="data row480 col5" >2.33585</td>
      <td id="T_c6ca4_row480_col6" class="data row480 col6" >0.249364</td>
      <td id="T_c6ca4_row480_col7" class="data row480 col7" >0.996682</td>
      <td id="T_c6ca4_row480_col8" class="data row480 col8" >0.161621</td>
      <td id="T_c6ca4_row480_col9" class="data row480 col9" >256.000000</td>
      <td id="T_c6ca4_row480_col10" class="data row480 col10" >torch.float32</td>
      <td id="T_c6ca4_row480_col11" class="data row480 col11" >(256,)</td>
      <td id="T_c6ca4_row480_col12" class="data row480 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row481" class="row_heading level0 row481" >481</th>
      <td id="T_c6ca4_row481_col0" class="data row481 col0" >feature_extractor_video.resnet.trunk.layer3.0.downsample.1.weight</td>
      <td id="T_c6ca4_row481_col1" class="data row481 col1" >feature_extractor_video.resnet.trunk.layer3.0.downsample.1.weight</td>
      <td id="T_c6ca4_row481_col2" class="data row481 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer3.0.downsample.1.weight</td>
      <td id="T_c6ca4_row481_col3" class="data row481 col3" >0.159604</td>
      <td id="T_c6ca4_row481_col4" class="data row481 col4" >1.81502</td>
      <td id="T_c6ca4_row481_col5" class="data row481 col5" >1.77406</td>
      <td id="T_c6ca4_row481_col6" class="data row481 col6" >0.087935</td>
      <td id="T_c6ca4_row481_col7" class="data row481 col7" >0.996305</td>
      <td id="T_c6ca4_row481_col8" class="data row481 col8" >0.0400391</td>
      <td id="T_c6ca4_row481_col9" class="data row481 col9" >256.000000</td>
      <td id="T_c6ca4_row481_col10" class="data row481 col10" >torch.float32</td>
      <td id="T_c6ca4_row481_col11" class="data row481 col11" >(256,)</td>
      <td id="T_c6ca4_row481_col12" class="data row481 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row482" class="row_heading level0 row482" >482</th>
      <td id="T_c6ca4_row482_col0" class="data row482 col0" >feature_extractor_video.resnet.trunk.layer3.0.relu1.weight</td>
      <td id="T_c6ca4_row482_col1" class="data row482 col1" >feature_extractor_video.resnet.trunk.layer3.0.relu1.weight</td>
      <td id="T_c6ca4_row482_col2" class="data row482 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer3.0.relu1.weight</td>
      <td id="T_c6ca4_row482_col3" class="data row482 col3" >0.351862</td>
      <td id="T_c6ca4_row482_col4" class="data row482 col4" >6.29446</td>
      <td id="T_c6ca4_row482_col5" class="data row482 col5" >6.043</td>
      <td id="T_c6ca4_row482_col6" class="data row482 col6" >0.0559003</td>
      <td id="T_c6ca4_row482_col7" class="data row482 col7" >0.999204</td>
      <td id="T_c6ca4_row482_col8" class="data row482 col8" >0.0722656</td>
      <td id="T_c6ca4_row482_col9" class="data row482 col9" >256.000000</td>
      <td id="T_c6ca4_row482_col10" class="data row482 col10" >torch.float32</td>
      <td id="T_c6ca4_row482_col11" class="data row482 col11" >(256,)</td>
      <td id="T_c6ca4_row482_col12" class="data row482 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row483" class="row_heading level0 row483" >483</th>
      <td id="T_c6ca4_row483_col0" class="data row483 col0" >feature_extractor_video.resnet.trunk.layer3.0.relu2.weight</td>
      <td id="T_c6ca4_row483_col1" class="data row483 col1" >feature_extractor_video.resnet.trunk.layer3.0.relu2.weight</td>
      <td id="T_c6ca4_row483_col2" class="data row483 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer3.0.relu2.weight</td>
      <td id="T_c6ca4_row483_col3" class="data row483 col3" >0.312948</td>
      <td id="T_c6ca4_row483_col4" class="data row483 col4" >7.56985</td>
      <td id="T_c6ca4_row483_col5" class="data row483 col5" >7.34094</td>
      <td id="T_c6ca4_row483_col6" class="data row483 col6" >0.0413414</td>
      <td id="T_c6ca4_row483_col7" class="data row483 col7" >0.99959</td>
      <td id="T_c6ca4_row483_col8" class="data row483 col8" >0.0449219</td>
      <td id="T_c6ca4_row483_col9" class="data row483 col9" >256.000000</td>
      <td id="T_c6ca4_row483_col10" class="data row483 col10" >torch.float32</td>
      <td id="T_c6ca4_row483_col11" class="data row483 col11" >(256,)</td>
      <td id="T_c6ca4_row483_col12" class="data row483 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row484" class="row_heading level0 row484" >484</th>
      <td id="T_c6ca4_row484_col0" class="data row484 col0" >feature_extractor_video.resnet.trunk.layer3.1.bn1.bias</td>
      <td id="T_c6ca4_row484_col1" class="data row484 col1" >feature_extractor_video.resnet.trunk.layer3.1.bn1.bias</td>
      <td id="T_c6ca4_row484_col2" class="data row484 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer3.1.bn1.bias</td>
      <td id="T_c6ca4_row484_col3" class="data row484 col3" >0.145499</td>
      <td id="T_c6ca4_row484_col4" class="data row484 col4" >1.69763</td>
      <td id="T_c6ca4_row484_col5" class="data row484 col5" >1.68925</td>
      <td id="T_c6ca4_row484_col6" class="data row484 col6" >0.0857072</td>
      <td id="T_c6ca4_row484_col7" class="data row484 col7" >0.996321</td>
      <td id="T_c6ca4_row484_col8" class="data row484 col8" >0.0360718</td>
      <td id="T_c6ca4_row484_col9" class="data row484 col9" >256.000000</td>
      <td id="T_c6ca4_row484_col10" class="data row484 col10" >torch.float32</td>
      <td id="T_c6ca4_row484_col11" class="data row484 col11" >(256,)</td>
      <td id="T_c6ca4_row484_col12" class="data row484 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row485" class="row_heading level0 row485" >485</th>
      <td id="T_c6ca4_row485_col0" class="data row485 col0" >feature_extractor_video.resnet.trunk.layer3.1.bn1.num_batches_tracked</td>
      <td id="T_c6ca4_row485_col1" class="data row485 col1" >feature_extractor_video.resnet.trunk.layer3.1.bn1.num_batches_tracked</td>
      <td id="T_c6ca4_row485_col2" class="data row485 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer3.1.bn1.num_batches_tracked</td>
      <td id="T_c6ca4_row485_col3" class="data row485 col3" >880066</td>
      <td id="T_c6ca4_row485_col4" class="data row485 col4" >601803</td>
      <td id="T_c6ca4_row485_col5" class="data row485 col5" >1.48187e+06</td>
      <td id="T_c6ca4_row485_col6" class="data row485 col6" >1.46238</td>
      <td id="T_c6ca4_row485_col7" class="data row485 col7" >1</td>
      <td id="T_c6ca4_row485_col8" class="data row485 col8" >880066</td>
      <td id="T_c6ca4_row485_col9" class="data row485 col9" >1.000000</td>
      <td id="T_c6ca4_row485_col10" class="data row485 col10" >torch.int64</td>
      <td id="T_c6ca4_row485_col11" class="data row485 col11" >()</td>
      <td id="T_c6ca4_row485_col12" class="data row485 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row486" class="row_heading level0 row486" >486</th>
      <td id="T_c6ca4_row486_col0" class="data row486 col0" >feature_extractor_video.resnet.trunk.layer3.1.bn1.running_mean</td>
      <td id="T_c6ca4_row486_col1" class="data row486 col1" >feature_extractor_video.resnet.trunk.layer3.1.bn1.running_mean</td>
      <td id="T_c6ca4_row486_col2" class="data row486 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer3.1.bn1.running_mean</td>
      <td id="T_c6ca4_row486_col3" class="data row486 col3" >3.34662</td>
      <td id="T_c6ca4_row486_col4" class="data row486 col4" >21.6186</td>
      <td id="T_c6ca4_row486_col5" class="data row486 col5" >20.4006</td>
      <td id="T_c6ca4_row486_col6" class="data row486 col6" >0.154803</td>
      <td id="T_c6ca4_row486_col7" class="data row486 col7" >0.988985</td>
      <td id="T_c6ca4_row486_col8" class="data row486 col8" >0.640625</td>
      <td id="T_c6ca4_row486_col9" class="data row486 col9" >256.000000</td>
      <td id="T_c6ca4_row486_col10" class="data row486 col10" >torch.float32</td>
      <td id="T_c6ca4_row486_col11" class="data row486 col11" >(256,)</td>
      <td id="T_c6ca4_row486_col12" class="data row486 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row487" class="row_heading level0 row487" >487</th>
      <td id="T_c6ca4_row487_col0" class="data row487 col0" >feature_extractor_video.resnet.trunk.layer3.1.bn1.running_var</td>
      <td id="T_c6ca4_row487_col1" class="data row487 col1" >feature_extractor_video.resnet.trunk.layer3.1.bn1.running_var</td>
      <td id="T_c6ca4_row487_col2" class="data row487 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer3.1.bn1.running_var</td>
      <td id="T_c6ca4_row487_col3" class="data row487 col3" >9.32093</td>
      <td id="T_c6ca4_row487_col4" class="data row487 col4" >52.3667</td>
      <td id="T_c6ca4_row487_col5" class="data row487 col5" >44.0222</td>
      <td id="T_c6ca4_row487_col6" class="data row487 col6" >0.177993</td>
      <td id="T_c6ca4_row487_col7" class="data row487 col7" >0.996259</td>
      <td id="T_c6ca4_row487_col8" class="data row487 col8" >1.32812</td>
      <td id="T_c6ca4_row487_col9" class="data row487 col9" >256.000000</td>
      <td id="T_c6ca4_row487_col10" class="data row487 col10" >torch.float32</td>
      <td id="T_c6ca4_row487_col11" class="data row487 col11" >(256,)</td>
      <td id="T_c6ca4_row487_col12" class="data row487 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row488" class="row_heading level0 row488" >488</th>
      <td id="T_c6ca4_row488_col0" class="data row488 col0" >feature_extractor_video.resnet.trunk.layer3.1.bn1.weight</td>
      <td id="T_c6ca4_row488_col1" class="data row488 col1" >feature_extractor_video.resnet.trunk.layer3.1.bn1.weight</td>
      <td id="T_c6ca4_row488_col2" class="data row488 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer3.1.bn1.weight</td>
      <td id="T_c6ca4_row488_col3" class="data row488 col3" >0.19482</td>
      <td id="T_c6ca4_row488_col4" class="data row488 col4" >3.37497</td>
      <td id="T_c6ca4_row488_col5" class="data row488 col5" >3.27161</td>
      <td id="T_c6ca4_row488_col6" class="data row488 col6" >0.057725</td>
      <td id="T_c6ca4_row488_col7" class="data row488 col7" >0.998765</td>
      <td id="T_c6ca4_row488_col8" class="data row488 col8" >0.0917969</td>
      <td id="T_c6ca4_row488_col9" class="data row488 col9" >256.000000</td>
      <td id="T_c6ca4_row488_col10" class="data row488 col10" >torch.float32</td>
      <td id="T_c6ca4_row488_col11" class="data row488 col11" >(256,)</td>
      <td id="T_c6ca4_row488_col12" class="data row488 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row489" class="row_heading level0 row489" >489</th>
      <td id="T_c6ca4_row489_col0" class="data row489 col0" >feature_extractor_video.resnet.trunk.layer3.1.bn2.bias</td>
      <td id="T_c6ca4_row489_col1" class="data row489 col1" >feature_extractor_video.resnet.trunk.layer3.1.bn2.bias</td>
      <td id="T_c6ca4_row489_col2" class="data row489 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer3.1.bn2.bias</td>
      <td id="T_c6ca4_row489_col3" class="data row489 col3" >0.213059</td>
      <td id="T_c6ca4_row489_col4" class="data row489 col4" >3.03295</td>
      <td id="T_c6ca4_row489_col5" class="data row489 col5" >2.89005</td>
      <td id="T_c6ca4_row489_col6" class="data row489 col6" >0.070248</td>
      <td id="T_c6ca4_row489_col7" class="data row489 col7" >0.998575</td>
      <td id="T_c6ca4_row489_col8" class="data row489 col8" >0.0388794</td>
      <td id="T_c6ca4_row489_col9" class="data row489 col9" >256.000000</td>
      <td id="T_c6ca4_row489_col10" class="data row489 col10" >torch.float32</td>
      <td id="T_c6ca4_row489_col11" class="data row489 col11" >(256,)</td>
      <td id="T_c6ca4_row489_col12" class="data row489 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row490" class="row_heading level0 row490" >490</th>
      <td id="T_c6ca4_row490_col0" class="data row490 col0" >feature_extractor_video.resnet.trunk.layer3.1.bn2.num_batches_tracked</td>
      <td id="T_c6ca4_row490_col1" class="data row490 col1" >feature_extractor_video.resnet.trunk.layer3.1.bn2.num_batches_tracked</td>
      <td id="T_c6ca4_row490_col2" class="data row490 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer3.1.bn2.num_batches_tracked</td>
      <td id="T_c6ca4_row490_col3" class="data row490 col3" >880066</td>
      <td id="T_c6ca4_row490_col4" class="data row490 col4" >601803</td>
      <td id="T_c6ca4_row490_col5" class="data row490 col5" >1.48187e+06</td>
      <td id="T_c6ca4_row490_col6" class="data row490 col6" >1.46238</td>
      <td id="T_c6ca4_row490_col7" class="data row490 col7" >1</td>
      <td id="T_c6ca4_row490_col8" class="data row490 col8" >880066</td>
      <td id="T_c6ca4_row490_col9" class="data row490 col9" >1.000000</td>
      <td id="T_c6ca4_row490_col10" class="data row490 col10" >torch.int64</td>
      <td id="T_c6ca4_row490_col11" class="data row490 col11" >()</td>
      <td id="T_c6ca4_row490_col12" class="data row490 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row491" class="row_heading level0 row491" >491</th>
      <td id="T_c6ca4_row491_col0" class="data row491 col0" >feature_extractor_video.resnet.trunk.layer3.1.bn2.running_mean</td>
      <td id="T_c6ca4_row491_col1" class="data row491 col1" >feature_extractor_video.resnet.trunk.layer3.1.bn2.running_mean</td>
      <td id="T_c6ca4_row491_col2" class="data row491 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer3.1.bn2.running_mean</td>
      <td id="T_c6ca4_row491_col3" class="data row491 col3" >2.51918</td>
      <td id="T_c6ca4_row491_col4" class="data row491 col4" >12.2671</td>
      <td id="T_c6ca4_row491_col5" class="data row491 col5" >11.943</td>
      <td id="T_c6ca4_row491_col6" class="data row491 col6" >0.205361</td>
      <td id="T_c6ca4_row491_col7" class="data row491 col7" >0.9787</td>
      <td id="T_c6ca4_row491_col8" class="data row491 col8" >0.509766</td>
      <td id="T_c6ca4_row491_col9" class="data row491 col9" >256.000000</td>
      <td id="T_c6ca4_row491_col10" class="data row491 col10" >torch.float32</td>
      <td id="T_c6ca4_row491_col11" class="data row491 col11" >(256,)</td>
      <td id="T_c6ca4_row491_col12" class="data row491 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row492" class="row_heading level0 row492" >492</th>
      <td id="T_c6ca4_row492_col0" class="data row492 col0" >feature_extractor_video.resnet.trunk.layer3.1.bn2.running_var</td>
      <td id="T_c6ca4_row492_col1" class="data row492 col1" >feature_extractor_video.resnet.trunk.layer3.1.bn2.running_var</td>
      <td id="T_c6ca4_row492_col2" class="data row492 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer3.1.bn2.running_var</td>
      <td id="T_c6ca4_row492_col3" class="data row492 col3" >5.4434</td>
      <td id="T_c6ca4_row492_col4" class="data row492 col4" >18.3694</td>
      <td id="T_c6ca4_row492_col5" class="data row492 col5" >13.2062</td>
      <td id="T_c6ca4_row492_col6" class="data row492 col6" >0.29633</td>
      <td id="T_c6ca4_row492_col7" class="data row492 col7" >0.993875</td>
      <td id="T_c6ca4_row492_col8" class="data row492 col8" >1.27539</td>
      <td id="T_c6ca4_row492_col9" class="data row492 col9" >256.000000</td>
      <td id="T_c6ca4_row492_col10" class="data row492 col10" >torch.float32</td>
      <td id="T_c6ca4_row492_col11" class="data row492 col11" >(256,)</td>
      <td id="T_c6ca4_row492_col12" class="data row492 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row493" class="row_heading level0 row493" >493</th>
      <td id="T_c6ca4_row493_col0" class="data row493 col0" >feature_extractor_video.resnet.trunk.layer3.1.bn2.weight</td>
      <td id="T_c6ca4_row493_col1" class="data row493 col1" >feature_extractor_video.resnet.trunk.layer3.1.bn2.weight</td>
      <td id="T_c6ca4_row493_col2" class="data row493 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer3.1.bn2.weight</td>
      <td id="T_c6ca4_row493_col3" class="data row493 col3" >0.457555</td>
      <td id="T_c6ca4_row493_col4" class="data row493 col4" >4.06725</td>
      <td id="T_c6ca4_row493_col5" class="data row493 col5" >3.69069</td>
      <td id="T_c6ca4_row493_col6" class="data row493 col6" >0.112497</td>
      <td id="T_c6ca4_row493_col7" class="data row493 col7" >0.99775</td>
      <td id="T_c6ca4_row493_col8" class="data row493 col8" >0.0966797</td>
      <td id="T_c6ca4_row493_col9" class="data row493 col9" >256.000000</td>
      <td id="T_c6ca4_row493_col10" class="data row493 col10" >torch.float32</td>
      <td id="T_c6ca4_row493_col11" class="data row493 col11" >(256,)</td>
      <td id="T_c6ca4_row493_col12" class="data row493 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row494" class="row_heading level0 row494" >494</th>
      <td id="T_c6ca4_row494_col0" class="data row494 col0" >feature_extractor_video.resnet.trunk.layer3.1.conv1.weight</td>
      <td id="T_c6ca4_row494_col1" class="data row494 col1" >feature_extractor_video.resnet.trunk.layer3.1.conv1.weight</td>
      <td id="T_c6ca4_row494_col2" class="data row494 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer3.1.conv1.weight</td>
      <td id="T_c6ca4_row494_col3" class="data row494 col3" >13.3696</td>
      <td id="T_c6ca4_row494_col4" class="data row494 col4" >88.7757</td>
      <td id="T_c6ca4_row494_col5" class="data row494 col5" >85.3228</td>
      <td id="T_c6ca4_row494_col6" class="data row494 col6" >0.1506</td>
      <td id="T_c6ca4_row494_col7" class="data row494 col7" >0.988988</td>
      <td id="T_c6ca4_row494_col8" class="data row494 col8" >0.116211</td>
      <td id="T_c6ca4_row494_col9" class="data row494 col9" >589824.000000</td>
      <td id="T_c6ca4_row494_col10" class="data row494 col10" >torch.float32</td>
      <td id="T_c6ca4_row494_col11" class="data row494 col11" >(256, 256, 3, 3)</td>
      <td id="T_c6ca4_row494_col12" class="data row494 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row495" class="row_heading level0 row495" >495</th>
      <td id="T_c6ca4_row495_col0" class="data row495 col0" >feature_extractor_video.resnet.trunk.layer3.1.conv2.weight</td>
      <td id="T_c6ca4_row495_col1" class="data row495 col1" >feature_extractor_video.resnet.trunk.layer3.1.conv2.weight</td>
      <td id="T_c6ca4_row495_col2" class="data row495 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer3.1.conv2.weight</td>
      <td id="T_c6ca4_row495_col3" class="data row495 col3" >13.0131</td>
      <td id="T_c6ca4_row495_col4" class="data row495 col4" >86.3</td>
      <td id="T_c6ca4_row495_col5" class="data row495 col5" >83.1325</td>
      <td id="T_c6ca4_row495_col6" class="data row495 col6" >0.150789</td>
      <td id="T_c6ca4_row495_col7" class="data row495 col7" >0.988897</td>
      <td id="T_c6ca4_row495_col8" class="data row495 col8" >0.0922852</td>
      <td id="T_c6ca4_row495_col9" class="data row495 col9" >589824.000000</td>
      <td id="T_c6ca4_row495_col10" class="data row495 col10" >torch.float32</td>
      <td id="T_c6ca4_row495_col11" class="data row495 col11" >(256, 256, 3, 3)</td>
      <td id="T_c6ca4_row495_col12" class="data row495 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row496" class="row_heading level0 row496" >496</th>
      <td id="T_c6ca4_row496_col0" class="data row496 col0" >feature_extractor_video.resnet.trunk.layer3.1.relu1.weight</td>
      <td id="T_c6ca4_row496_col1" class="data row496 col1" >feature_extractor_video.resnet.trunk.layer3.1.relu1.weight</td>
      <td id="T_c6ca4_row496_col2" class="data row496 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer3.1.relu1.weight</td>
      <td id="T_c6ca4_row496_col3" class="data row496 col3" >0.183403</td>
      <td id="T_c6ca4_row496_col4" class="data row496 col4" >4.41309</td>
      <td id="T_c6ca4_row496_col5" class="data row496 col5" >4.33463</td>
      <td id="T_c6ca4_row496_col6" class="data row496 col6" >0.0415588</td>
      <td id="T_c6ca4_row496_col7" class="data row496 col7" >0.999282</td>
      <td id="T_c6ca4_row496_col8" class="data row496 col8" >0.0415039</td>
      <td id="T_c6ca4_row496_col9" class="data row496 col9" >256.000000</td>
      <td id="T_c6ca4_row496_col10" class="data row496 col10" >torch.float32</td>
      <td id="T_c6ca4_row496_col11" class="data row496 col11" >(256,)</td>
      <td id="T_c6ca4_row496_col12" class="data row496 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row497" class="row_heading level0 row497" >497</th>
      <td id="T_c6ca4_row497_col0" class="data row497 col0" >feature_extractor_video.resnet.trunk.layer3.1.relu2.weight</td>
      <td id="T_c6ca4_row497_col1" class="data row497 col1" >feature_extractor_video.resnet.trunk.layer3.1.relu2.weight</td>
      <td id="T_c6ca4_row497_col2" class="data row497 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer3.1.relu2.weight</td>
      <td id="T_c6ca4_row497_col3" class="data row497 col3" >0.347105</td>
      <td id="T_c6ca4_row497_col4" class="data row497 col4" >8.72623</td>
      <td id="T_c6ca4_row497_col5" class="data row497 col5" >8.45733</td>
      <td id="T_c6ca4_row497_col6" class="data row497 col6" >0.0397773</td>
      <td id="T_c6ca4_row497_col7" class="data row497 col7" >0.999674</td>
      <td id="T_c6ca4_row497_col8" class="data row497 col8" >0.0610352</td>
      <td id="T_c6ca4_row497_col9" class="data row497 col9" >256.000000</td>
      <td id="T_c6ca4_row497_col10" class="data row497 col10" >torch.float32</td>
      <td id="T_c6ca4_row497_col11" class="data row497 col11" >(256,)</td>
      <td id="T_c6ca4_row497_col12" class="data row497 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row498" class="row_heading level0 row498" >498</th>
      <td id="T_c6ca4_row498_col0" class="data row498 col0" >feature_extractor_video.resnet.trunk.layer4.0.bn1.bias</td>
      <td id="T_c6ca4_row498_col1" class="data row498 col1" >feature_extractor_video.resnet.trunk.layer4.0.bn1.bias</td>
      <td id="T_c6ca4_row498_col2" class="data row498 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer4.0.bn1.bias</td>
      <td id="T_c6ca4_row498_col3" class="data row498 col3" >0.293831</td>
      <td id="T_c6ca4_row498_col4" class="data row498 col4" >1.53815</td>
      <td id="T_c6ca4_row498_col5" class="data row498 col5" >1.5852</td>
      <td id="T_c6ca4_row498_col6" class="data row498 col6" >0.191028</td>
      <td id="T_c6ca4_row498_col7" class="data row498 col7" >0.98275</td>
      <td id="T_c6ca4_row498_col8" class="data row498 col8" >0.0542603</td>
      <td id="T_c6ca4_row498_col9" class="data row498 col9" >512.000000</td>
      <td id="T_c6ca4_row498_col10" class="data row498 col10" >torch.float32</td>
      <td id="T_c6ca4_row498_col11" class="data row498 col11" >(512,)</td>
      <td id="T_c6ca4_row498_col12" class="data row498 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row499" class="row_heading level0 row499" >499</th>
      <td id="T_c6ca4_row499_col0" class="data row499 col0" >feature_extractor_video.resnet.trunk.layer4.0.bn1.num_batches_tracked</td>
      <td id="T_c6ca4_row499_col1" class="data row499 col1" >feature_extractor_video.resnet.trunk.layer4.0.bn1.num_batches_tracked</td>
      <td id="T_c6ca4_row499_col2" class="data row499 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer4.0.bn1.num_batches_tracked</td>
      <td id="T_c6ca4_row499_col3" class="data row499 col3" >880066</td>
      <td id="T_c6ca4_row499_col4" class="data row499 col4" >601803</td>
      <td id="T_c6ca4_row499_col5" class="data row499 col5" >1.48187e+06</td>
      <td id="T_c6ca4_row499_col6" class="data row499 col6" >1.46238</td>
      <td id="T_c6ca4_row499_col7" class="data row499 col7" >1</td>
      <td id="T_c6ca4_row499_col8" class="data row499 col8" >880066</td>
      <td id="T_c6ca4_row499_col9" class="data row499 col9" >1.000000</td>
      <td id="T_c6ca4_row499_col10" class="data row499 col10" >torch.int64</td>
      <td id="T_c6ca4_row499_col11" class="data row499 col11" >()</td>
      <td id="T_c6ca4_row499_col12" class="data row499 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row500" class="row_heading level0 row500" >500</th>
      <td id="T_c6ca4_row500_col0" class="data row500 col0" >feature_extractor_video.resnet.trunk.layer4.0.bn1.running_mean</td>
      <td id="T_c6ca4_row500_col1" class="data row500 col1" >feature_extractor_video.resnet.trunk.layer4.0.bn1.running_mean</td>
      <td id="T_c6ca4_row500_col2" class="data row500 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer4.0.bn1.running_mean</td>
      <td id="T_c6ca4_row500_col3" class="data row500 col3" >4.91587</td>
      <td id="T_c6ca4_row500_col4" class="data row500 col4" >19.6786</td>
      <td id="T_c6ca4_row500_col5" class="data row500 col5" >19.4316</td>
      <td id="T_c6ca4_row500_col6" class="data row500 col6" >0.249807</td>
      <td id="T_c6ca4_row500_col7" class="data row500 col7" >0.968481</td>
      <td id="T_c6ca4_row500_col8" class="data row500 col8" >1.07031</td>
      <td id="T_c6ca4_row500_col9" class="data row500 col9" >512.000000</td>
      <td id="T_c6ca4_row500_col10" class="data row500 col10" >torch.float32</td>
      <td id="T_c6ca4_row500_col11" class="data row500 col11" >(512,)</td>
      <td id="T_c6ca4_row500_col12" class="data row500 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row501" class="row_heading level0 row501" >501</th>
      <td id="T_c6ca4_row501_col0" class="data row501 col0" >feature_extractor_video.resnet.trunk.layer4.0.bn1.running_var</td>
      <td id="T_c6ca4_row501_col1" class="data row501 col1" >feature_extractor_video.resnet.trunk.layer4.0.bn1.running_var</td>
      <td id="T_c6ca4_row501_col2" class="data row501 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer4.0.bn1.running_var</td>
      <td id="T_c6ca4_row501_col3" class="data row501 col3" >33.5654</td>
      <td id="T_c6ca4_row501_col4" class="data row501 col4" >81.2589</td>
      <td id="T_c6ca4_row501_col5" class="data row501 col5" >51.9912</td>
      <td id="T_c6ca4_row501_col6" class="data row501 col6" >0.413068</td>
      <td id="T_c6ca4_row501_col7" class="data row501 col7" >0.968041</td>
      <td id="T_c6ca4_row501_col8" class="data row501 col8" >6.08594</td>
      <td id="T_c6ca4_row501_col9" class="data row501 col9" >512.000000</td>
      <td id="T_c6ca4_row501_col10" class="data row501 col10" >torch.float32</td>
      <td id="T_c6ca4_row501_col11" class="data row501 col11" >(512,)</td>
      <td id="T_c6ca4_row501_col12" class="data row501 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row502" class="row_heading level0 row502" >502</th>
      <td id="T_c6ca4_row502_col0" class="data row502 col0" >feature_extractor_video.resnet.trunk.layer4.0.bn1.weight</td>
      <td id="T_c6ca4_row502_col1" class="data row502 col1" >feature_extractor_video.resnet.trunk.layer4.0.bn1.weight</td>
      <td id="T_c6ca4_row502_col2" class="data row502 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer4.0.bn1.weight</td>
      <td id="T_c6ca4_row502_col3" class="data row502 col3" >0.547905</td>
      <td id="T_c6ca4_row502_col4" class="data row502 col4" >5.46607</td>
      <td id="T_c6ca4_row502_col5" class="data row502 col5" >5.13214</td>
      <td id="T_c6ca4_row502_col6" class="data row502 col6" >0.100237</td>
      <td id="T_c6ca4_row502_col7" class="data row502 col7" >0.996637</td>
      <td id="T_c6ca4_row502_col8" class="data row502 col8" >0.0664062</td>
      <td id="T_c6ca4_row502_col9" class="data row502 col9" >512.000000</td>
      <td id="T_c6ca4_row502_col10" class="data row502 col10" >torch.float32</td>
      <td id="T_c6ca4_row502_col11" class="data row502 col11" >(512,)</td>
      <td id="T_c6ca4_row502_col12" class="data row502 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row503" class="row_heading level0 row503" >503</th>
      <td id="T_c6ca4_row503_col0" class="data row503 col0" >feature_extractor_video.resnet.trunk.layer4.0.bn2.bias</td>
      <td id="T_c6ca4_row503_col1" class="data row503 col1" >feature_extractor_video.resnet.trunk.layer4.0.bn2.bias</td>
      <td id="T_c6ca4_row503_col2" class="data row503 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer4.0.bn2.bias</td>
      <td id="T_c6ca4_row503_col3" class="data row503 col3" >0.220717</td>
      <td id="T_c6ca4_row503_col4" class="data row503 col4" >0.523757</td>
      <td id="T_c6ca4_row503_col5" class="data row503 col5" >0.675258</td>
      <td id="T_c6ca4_row503_col6" class="data row503 col6" >0.421412</td>
      <td id="T_c6ca4_row503_col7" class="data row503 col7" >0.963577</td>
      <td id="T_c6ca4_row503_col8" class="data row503 col8" >0.0481262</td>
      <td id="T_c6ca4_row503_col9" class="data row503 col9" >512.000000</td>
      <td id="T_c6ca4_row503_col10" class="data row503 col10" >torch.float32</td>
      <td id="T_c6ca4_row503_col11" class="data row503 col11" >(512,)</td>
      <td id="T_c6ca4_row503_col12" class="data row503 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row504" class="row_heading level0 row504" >504</th>
      <td id="T_c6ca4_row504_col0" class="data row504 col0" >feature_extractor_video.resnet.trunk.layer4.0.bn2.num_batches_tracked</td>
      <td id="T_c6ca4_row504_col1" class="data row504 col1" >feature_extractor_video.resnet.trunk.layer4.0.bn2.num_batches_tracked</td>
      <td id="T_c6ca4_row504_col2" class="data row504 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer4.0.bn2.num_batches_tracked</td>
      <td id="T_c6ca4_row504_col3" class="data row504 col3" >880066</td>
      <td id="T_c6ca4_row504_col4" class="data row504 col4" >601803</td>
      <td id="T_c6ca4_row504_col5" class="data row504 col5" >1.48187e+06</td>
      <td id="T_c6ca4_row504_col6" class="data row504 col6" >1.46238</td>
      <td id="T_c6ca4_row504_col7" class="data row504 col7" >1</td>
      <td id="T_c6ca4_row504_col8" class="data row504 col8" >880066</td>
      <td id="T_c6ca4_row504_col9" class="data row504 col9" >1.000000</td>
      <td id="T_c6ca4_row504_col10" class="data row504 col10" >torch.int64</td>
      <td id="T_c6ca4_row504_col11" class="data row504 col11" >()</td>
      <td id="T_c6ca4_row504_col12" class="data row504 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row505" class="row_heading level0 row505" >505</th>
      <td id="T_c6ca4_row505_col0" class="data row505 col0" >feature_extractor_video.resnet.trunk.layer4.0.bn2.running_mean</td>
      <td id="T_c6ca4_row505_col1" class="data row505 col1" >feature_extractor_video.resnet.trunk.layer4.0.bn2.running_mean</td>
      <td id="T_c6ca4_row505_col2" class="data row505 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer4.0.bn2.running_mean</td>
      <td id="T_c6ca4_row505_col3" class="data row505 col3" >5.11558</td>
      <td id="T_c6ca4_row505_col4" class="data row505 col4" >23.7788</td>
      <td id="T_c6ca4_row505_col5" class="data row505 col5" >21.1902</td>
      <td id="T_c6ca4_row505_col6" class="data row505 col6" >0.215132</td>
      <td id="T_c6ca4_row505_col7" class="data row505 col7" >0.980682</td>
      <td id="T_c6ca4_row505_col8" class="data row505 col8" >0.937195</td>
      <td id="T_c6ca4_row505_col9" class="data row505 col9" >512.000000</td>
      <td id="T_c6ca4_row505_col10" class="data row505 col10" >torch.float32</td>
      <td id="T_c6ca4_row505_col11" class="data row505 col11" >(512,)</td>
      <td id="T_c6ca4_row505_col12" class="data row505 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row506" class="row_heading level0 row506" >506</th>
      <td id="T_c6ca4_row506_col0" class="data row506 col0" >feature_extractor_video.resnet.trunk.layer4.0.bn2.running_var</td>
      <td id="T_c6ca4_row506_col1" class="data row506 col1" >feature_extractor_video.resnet.trunk.layer4.0.bn2.running_var</td>
      <td id="T_c6ca4_row506_col2" class="data row506 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer4.0.bn2.running_var</td>
      <td id="T_c6ca4_row506_col3" class="data row506 col3" >435.866</td>
      <td id="T_c6ca4_row506_col4" class="data row506 col4" >711.993</td>
      <td id="T_c6ca4_row506_col5" class="data row506 col5" >286.356</td>
      <td id="T_c6ca4_row506_col6" class="data row506 col6" >0.612178</td>
      <td id="T_c6ca4_row506_col7" class="data row506 col7" >0.978389</td>
      <td id="T_c6ca4_row506_col8" class="data row506 col8" >71.4062</td>
      <td id="T_c6ca4_row506_col9" class="data row506 col9" >512.000000</td>
      <td id="T_c6ca4_row506_col10" class="data row506 col10" >torch.float32</td>
      <td id="T_c6ca4_row506_col11" class="data row506 col11" >(512,)</td>
      <td id="T_c6ca4_row506_col12" class="data row506 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row507" class="row_heading level0 row507" >507</th>
      <td id="T_c6ca4_row507_col0" class="data row507 col0" >feature_extractor_video.resnet.trunk.layer4.0.bn2.weight</td>
      <td id="T_c6ca4_row507_col1" class="data row507 col1" >feature_extractor_video.resnet.trunk.layer4.0.bn2.weight</td>
      <td id="T_c6ca4_row507_col2" class="data row507 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer4.0.bn2.weight</td>
      <td id="T_c6ca4_row507_col3" class="data row507 col3" >0.246896</td>
      <td id="T_c6ca4_row507_col4" class="data row507 col4" >5.42981</td>
      <td id="T_c6ca4_row507_col5" class="data row507 col5" >5.24919</td>
      <td id="T_c6ca4_row507_col6" class="data row507 col6" >0.0454706</td>
      <td id="T_c6ca4_row507_col7" class="data row507 col7" >0.999503</td>
      <td id="T_c6ca4_row507_col8" class="data row507 col8" >0.0500488</td>
      <td id="T_c6ca4_row507_col9" class="data row507 col9" >512.000000</td>
      <td id="T_c6ca4_row507_col10" class="data row507 col10" >torch.float32</td>
      <td id="T_c6ca4_row507_col11" class="data row507 col11" >(512,)</td>
      <td id="T_c6ca4_row507_col12" class="data row507 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row508" class="row_heading level0 row508" >508</th>
      <td id="T_c6ca4_row508_col0" class="data row508 col0" >feature_extractor_video.resnet.trunk.layer4.0.conv1.weight</td>
      <td id="T_c6ca4_row508_col1" class="data row508 col1" >feature_extractor_video.resnet.trunk.layer4.0.conv1.weight</td>
      <td id="T_c6ca4_row508_col2" class="data row508 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer4.0.conv1.weight</td>
      <td id="T_c6ca4_row508_col3" class="data row508 col3" >18.8914</td>
      <td id="T_c6ca4_row508_col4" class="data row508 col4" >120.655</td>
      <td id="T_c6ca4_row508_col5" class="data row508 col5" >116.114</td>
      <td id="T_c6ca4_row508_col6" class="data row508 col6" >0.156573</td>
      <td id="T_c6ca4_row508_col7" class="data row508 col7" >0.987999</td>
      <td id="T_c6ca4_row508_col8" class="data row508 col8" >0.145508</td>
      <td id="T_c6ca4_row508_col9" class="data row508 col9" >1179648.000000</td>
      <td id="T_c6ca4_row508_col10" class="data row508 col10" >torch.float32</td>
      <td id="T_c6ca4_row508_col11" class="data row508 col11" >(512, 256, 3, 3)</td>
      <td id="T_c6ca4_row508_col12" class="data row508 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row509" class="row_heading level0 row509" >509</th>
      <td id="T_c6ca4_row509_col0" class="data row509 col0" >feature_extractor_video.resnet.trunk.layer4.0.conv2.weight</td>
      <td id="T_c6ca4_row509_col1" class="data row509 col1" >feature_extractor_video.resnet.trunk.layer4.0.conv2.weight</td>
      <td id="T_c6ca4_row509_col2" class="data row509 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer4.0.conv2.weight</td>
      <td id="T_c6ca4_row509_col3" class="data row509 col3" >25.2671</td>
      <td id="T_c6ca4_row509_col4" class="data row509 col4" >148.766</td>
      <td id="T_c6ca4_row509_col5" class="data row509 col5" >143.532</td>
      <td id="T_c6ca4_row509_col6" class="data row509 col6" >0.169845</td>
      <td id="T_c6ca4_row509_col7" class="data row509 col7" >0.985692</td>
      <td id="T_c6ca4_row509_col8" class="data row509 col8" >0.151855</td>
      <td id="T_c6ca4_row509_col9" class="data row509 col9" >2359296.000000</td>
      <td id="T_c6ca4_row509_col10" class="data row509 col10" >torch.float32</td>
      <td id="T_c6ca4_row509_col11" class="data row509 col11" >(512, 512, 3, 3)</td>
      <td id="T_c6ca4_row509_col12" class="data row509 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row510" class="row_heading level0 row510" >510</th>
      <td id="T_c6ca4_row510_col0" class="data row510 col0" >feature_extractor_video.resnet.trunk.layer4.0.downsample.0.weight</td>
      <td id="T_c6ca4_row510_col1" class="data row510 col1" >feature_extractor_video.resnet.trunk.layer4.0.downsample.0.weight</td>
      <td id="T_c6ca4_row510_col2" class="data row510 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer4.0.downsample.0.weight</td>
      <td id="T_c6ca4_row510_col3" class="data row510 col3" >6.49144</td>
      <td id="T_c6ca4_row510_col4" class="data row510 col4" >45.0402</td>
      <td id="T_c6ca4_row510_col5" class="data row510 col5" >43.1654</td>
      <td id="T_c6ca4_row510_col6" class="data row510 col6" >0.144126</td>
      <td id="T_c6ca4_row510_col7" class="data row510 col7" >0.990067</td>
      <td id="T_c6ca4_row510_col8" class="data row510 col8" >0.116089</td>
      <td id="T_c6ca4_row510_col9" class="data row510 col9" >131072.000000</td>
      <td id="T_c6ca4_row510_col10" class="data row510 col10" >torch.float32</td>
      <td id="T_c6ca4_row510_col11" class="data row510 col11" >(512, 256, 1, 1)</td>
      <td id="T_c6ca4_row510_col12" class="data row510 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row511" class="row_heading level0 row511" >511</th>
      <td id="T_c6ca4_row511_col0" class="data row511 col0" >feature_extractor_video.resnet.trunk.layer4.0.downsample.1.bias</td>
      <td id="T_c6ca4_row511_col1" class="data row511 col1" >feature_extractor_video.resnet.trunk.layer4.0.downsample.1.bias</td>
      <td id="T_c6ca4_row511_col2" class="data row511 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer4.0.downsample.1.bias</td>
      <td id="T_c6ca4_row511_col3" class="data row511 col3" >0.220718</td>
      <td id="T_c6ca4_row511_col4" class="data row511 col4" >0.523757</td>
      <td id="T_c6ca4_row511_col5" class="data row511 col5" >0.675258</td>
      <td id="T_c6ca4_row511_col6" class="data row511 col6" >0.421412</td>
      <td id="T_c6ca4_row511_col7" class="data row511 col7" >0.963577</td>
      <td id="T_c6ca4_row511_col8" class="data row511 col8" >0.0481262</td>
      <td id="T_c6ca4_row511_col9" class="data row511 col9" >512.000000</td>
      <td id="T_c6ca4_row511_col10" class="data row511 col10" >torch.float32</td>
      <td id="T_c6ca4_row511_col11" class="data row511 col11" >(512,)</td>
      <td id="T_c6ca4_row511_col12" class="data row511 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row512" class="row_heading level0 row512" >512</th>
      <td id="T_c6ca4_row512_col0" class="data row512 col0" >feature_extractor_video.resnet.trunk.layer4.0.downsample.1.num_batches_tracked</td>
      <td id="T_c6ca4_row512_col1" class="data row512 col1" >feature_extractor_video.resnet.trunk.layer4.0.downsample.1.num_batches_tracked</td>
      <td id="T_c6ca4_row512_col2" class="data row512 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer4.0.downsample.1.num_batches_tracked</td>
      <td id="T_c6ca4_row512_col3" class="data row512 col3" >880066</td>
      <td id="T_c6ca4_row512_col4" class="data row512 col4" >601803</td>
      <td id="T_c6ca4_row512_col5" class="data row512 col5" >1.48187e+06</td>
      <td id="T_c6ca4_row512_col6" class="data row512 col6" >1.46238</td>
      <td id="T_c6ca4_row512_col7" class="data row512 col7" >1</td>
      <td id="T_c6ca4_row512_col8" class="data row512 col8" >880066</td>
      <td id="T_c6ca4_row512_col9" class="data row512 col9" >1.000000</td>
      <td id="T_c6ca4_row512_col10" class="data row512 col10" >torch.int64</td>
      <td id="T_c6ca4_row512_col11" class="data row512 col11" >()</td>
      <td id="T_c6ca4_row512_col12" class="data row512 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row513" class="row_heading level0 row513" >513</th>
      <td id="T_c6ca4_row513_col0" class="data row513 col0" >feature_extractor_video.resnet.trunk.layer4.0.downsample.1.running_mean</td>
      <td id="T_c6ca4_row513_col1" class="data row513 col1" >feature_extractor_video.resnet.trunk.layer4.0.downsample.1.running_mean</td>
      <td id="T_c6ca4_row513_col2" class="data row513 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer4.0.downsample.1.running_mean</td>
      <td id="T_c6ca4_row513_col3" class="data row513 col3" >2.06437</td>
      <td id="T_c6ca4_row513_col4" class="data row513 col4" >10.8656</td>
      <td id="T_c6ca4_row513_col5" class="data row513 col5" >9.70771</td>
      <td id="T_c6ca4_row513_col6" class="data row513 col6" >0.189991</td>
      <td id="T_c6ca4_row513_col7" class="data row513 col7" >0.986154</td>
      <td id="T_c6ca4_row513_col8" class="data row513 col8" >0.309937</td>
      <td id="T_c6ca4_row513_col9" class="data row513 col9" >512.000000</td>
      <td id="T_c6ca4_row513_col10" class="data row513 col10" >torch.float32</td>
      <td id="T_c6ca4_row513_col11" class="data row513 col11" >(512,)</td>
      <td id="T_c6ca4_row513_col12" class="data row513 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row514" class="row_heading level0 row514" >514</th>
      <td id="T_c6ca4_row514_col0" class="data row514 col0" >feature_extractor_video.resnet.trunk.layer4.0.downsample.1.running_var</td>
      <td id="T_c6ca4_row514_col1" class="data row514 col1" >feature_extractor_video.resnet.trunk.layer4.0.downsample.1.running_var</td>
      <td id="T_c6ca4_row514_col2" class="data row514 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer4.0.downsample.1.running_var</td>
      <td id="T_c6ca4_row514_col3" class="data row514 col3" >166.31</td>
      <td id="T_c6ca4_row514_col4" class="data row514 col4" >288.9</td>
      <td id="T_c6ca4_row514_col5" class="data row514 col5" >125.378</td>
      <td id="T_c6ca4_row514_col6" class="data row514 col6" >0.575667</td>
      <td id="T_c6ca4_row514_col7" class="data row514 col7" >0.987307</td>
      <td id="T_c6ca4_row514_col8" class="data row514 col8" >25.6406</td>
      <td id="T_c6ca4_row514_col9" class="data row514 col9" >512.000000</td>
      <td id="T_c6ca4_row514_col10" class="data row514 col10" >torch.float32</td>
      <td id="T_c6ca4_row514_col11" class="data row514 col11" >(512,)</td>
      <td id="T_c6ca4_row514_col12" class="data row514 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row515" class="row_heading level0 row515" >515</th>
      <td id="T_c6ca4_row515_col0" class="data row515 col0" >feature_extractor_video.resnet.trunk.layer4.0.downsample.1.weight</td>
      <td id="T_c6ca4_row515_col1" class="data row515 col1" >feature_extractor_video.resnet.trunk.layer4.0.downsample.1.weight</td>
      <td id="T_c6ca4_row515_col2" class="data row515 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer4.0.downsample.1.weight</td>
      <td id="T_c6ca4_row515_col3" class="data row515 col3" >0.219596</td>
      <td id="T_c6ca4_row515_col4" class="data row515 col4" >4.19114</td>
      <td id="T_c6ca4_row515_col5" class="data row515 col5" >4.09463</td>
      <td id="T_c6ca4_row515_col6" class="data row515 col6" >0.0523954</td>
      <td id="T_c6ca4_row515_col7" class="data row515 col7" >0.998866</td>
      <td id="T_c6ca4_row515_col8" class="data row515 col8" >0.0410156</td>
      <td id="T_c6ca4_row515_col9" class="data row515 col9" >512.000000</td>
      <td id="T_c6ca4_row515_col10" class="data row515 col10" >torch.float32</td>
      <td id="T_c6ca4_row515_col11" class="data row515 col11" >(512,)</td>
      <td id="T_c6ca4_row515_col12" class="data row515 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row516" class="row_heading level0 row516" >516</th>
      <td id="T_c6ca4_row516_col0" class="data row516 col0" >feature_extractor_video.resnet.trunk.layer4.0.relu1.weight</td>
      <td id="T_c6ca4_row516_col1" class="data row516 col1" >feature_extractor_video.resnet.trunk.layer4.0.relu1.weight</td>
      <td id="T_c6ca4_row516_col2" class="data row516 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer4.0.relu1.weight</td>
      <td id="T_c6ca4_row516_col3" class="data row516 col3" >0.41839</td>
      <td id="T_c6ca4_row516_col4" class="data row516 col4" >6.8501</td>
      <td id="T_c6ca4_row516_col5" class="data row516 col5" >6.65192</td>
      <td id="T_c6ca4_row516_col6" class="data row516 col6" >0.0610779</td>
      <td id="T_c6ca4_row516_col7" class="data row516 col7" >0.99851</td>
      <td id="T_c6ca4_row516_col8" class="data row516 col8" >0.0842896</td>
      <td id="T_c6ca4_row516_col9" class="data row516 col9" >512.000000</td>
      <td id="T_c6ca4_row516_col10" class="data row516 col10" >torch.float32</td>
      <td id="T_c6ca4_row516_col11" class="data row516 col11" >(512,)</td>
      <td id="T_c6ca4_row516_col12" class="data row516 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row517" class="row_heading level0 row517" >517</th>
      <td id="T_c6ca4_row517_col0" class="data row517 col0" >feature_extractor_video.resnet.trunk.layer4.0.relu2.weight</td>
      <td id="T_c6ca4_row517_col1" class="data row517 col1" >feature_extractor_video.resnet.trunk.layer4.0.relu2.weight</td>
      <td id="T_c6ca4_row517_col2" class="data row517 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer4.0.relu2.weight</td>
      <td id="T_c6ca4_row517_col3" class="data row517 col3" >0.462015</td>
      <td id="T_c6ca4_row517_col4" class="data row517 col4" >7.79377</td>
      <td id="T_c6ca4_row517_col5" class="data row517 col5" >7.48985</td>
      <td id="T_c6ca4_row517_col6" class="data row517 col6" >0.0592801</td>
      <td id="T_c6ca4_row517_col7" class="data row517 col7" >0.998963</td>
      <td id="T_c6ca4_row517_col8" class="data row517 col8" >0.0546875</td>
      <td id="T_c6ca4_row517_col9" class="data row517 col9" >512.000000</td>
      <td id="T_c6ca4_row517_col10" class="data row517 col10" >torch.float32</td>
      <td id="T_c6ca4_row517_col11" class="data row517 col11" >(512,)</td>
      <td id="T_c6ca4_row517_col12" class="data row517 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row518" class="row_heading level0 row518" >518</th>
      <td id="T_c6ca4_row518_col0" class="data row518 col0" >feature_extractor_video.resnet.trunk.layer4.1.bn1.bias</td>
      <td id="T_c6ca4_row518_col1" class="data row518 col1" >feature_extractor_video.resnet.trunk.layer4.1.bn1.bias</td>
      <td id="T_c6ca4_row518_col2" class="data row518 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer4.1.bn1.bias</td>
      <td id="T_c6ca4_row518_col3" class="data row518 col3" >0.255874</td>
      <td id="T_c6ca4_row518_col4" class="data row518 col4" >2.46023</td>
      <td id="T_c6ca4_row518_col5" class="data row518 col5" >2.49093</td>
      <td id="T_c6ca4_row518_col6" class="data row518 col6" >0.104004</td>
      <td id="T_c6ca4_row518_col7" class="data row518 col7" >0.994735</td>
      <td id="T_c6ca4_row518_col8" class="data row518 col8" >0.0375671</td>
      <td id="T_c6ca4_row518_col9" class="data row518 col9" >512.000000</td>
      <td id="T_c6ca4_row518_col10" class="data row518 col10" >torch.float32</td>
      <td id="T_c6ca4_row518_col11" class="data row518 col11" >(512,)</td>
      <td id="T_c6ca4_row518_col12" class="data row518 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row519" class="row_heading level0 row519" >519</th>
      <td id="T_c6ca4_row519_col0" class="data row519 col0" >feature_extractor_video.resnet.trunk.layer4.1.bn1.num_batches_tracked</td>
      <td id="T_c6ca4_row519_col1" class="data row519 col1" >feature_extractor_video.resnet.trunk.layer4.1.bn1.num_batches_tracked</td>
      <td id="T_c6ca4_row519_col2" class="data row519 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer4.1.bn1.num_batches_tracked</td>
      <td id="T_c6ca4_row519_col3" class="data row519 col3" >880066</td>
      <td id="T_c6ca4_row519_col4" class="data row519 col4" >601803</td>
      <td id="T_c6ca4_row519_col5" class="data row519 col5" >1.48187e+06</td>
      <td id="T_c6ca4_row519_col6" class="data row519 col6" >1.46238</td>
      <td id="T_c6ca4_row519_col7" class="data row519 col7" >1</td>
      <td id="T_c6ca4_row519_col8" class="data row519 col8" >880066</td>
      <td id="T_c6ca4_row519_col9" class="data row519 col9" >1.000000</td>
      <td id="T_c6ca4_row519_col10" class="data row519 col10" >torch.int64</td>
      <td id="T_c6ca4_row519_col11" class="data row519 col11" >()</td>
      <td id="T_c6ca4_row519_col12" class="data row519 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row520" class="row_heading level0 row520" >520</th>
      <td id="T_c6ca4_row520_col0" class="data row520 col0" >feature_extractor_video.resnet.trunk.layer4.1.bn1.running_mean</td>
      <td id="T_c6ca4_row520_col1" class="data row520 col1" >feature_extractor_video.resnet.trunk.layer4.1.bn1.running_mean</td>
      <td id="T_c6ca4_row520_col2" class="data row520 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer4.1.bn1.running_mean</td>
      <td id="T_c6ca4_row520_col3" class="data row520 col3" >2.57101</td>
      <td id="T_c6ca4_row520_col4" class="data row520 col4" >4.33627</td>
      <td id="T_c6ca4_row520_col5" class="data row520 col5" >6.4152</td>
      <td id="T_c6ca4_row520_col6" class="data row520 col6" >0.592909</td>
      <td id="T_c6ca4_row520_col7" class="data row520 col7" >0.958873</td>
      <td id="T_c6ca4_row520_col8" class="data row520 col8" >0.825195</td>
      <td id="T_c6ca4_row520_col9" class="data row520 col9" >512.000000</td>
      <td id="T_c6ca4_row520_col10" class="data row520 col10" >torch.float32</td>
      <td id="T_c6ca4_row520_col11" class="data row520 col11" >(512,)</td>
      <td id="T_c6ca4_row520_col12" class="data row520 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row521" class="row_heading level0 row521" >521</th>
      <td id="T_c6ca4_row521_col0" class="data row521 col0" >feature_extractor_video.resnet.trunk.layer4.1.bn1.running_var</td>
      <td id="T_c6ca4_row521_col1" class="data row521 col1" >feature_extractor_video.resnet.trunk.layer4.1.bn1.running_var</td>
      <td id="T_c6ca4_row521_col2" class="data row521 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer4.1.bn1.running_var</td>
      <td id="T_c6ca4_row521_col3" class="data row521 col3" >3.94329</td>
      <td id="T_c6ca4_row521_col4" class="data row521 col4" >4.78549</td>
      <td id="T_c6ca4_row521_col5" class="data row521 col5" >8.41765</td>
      <td id="T_c6ca4_row521_col6" class="data row521 col6" >0.824009</td>
      <td id="T_c6ca4_row521_col7" class="data row521 col7" >0.970745</td>
      <td id="T_c6ca4_row521_col8" class="data row521 col8" >0.614624</td>
      <td id="T_c6ca4_row521_col9" class="data row521 col9" >512.000000</td>
      <td id="T_c6ca4_row521_col10" class="data row521 col10" >torch.float32</td>
      <td id="T_c6ca4_row521_col11" class="data row521 col11" >(512,)</td>
      <td id="T_c6ca4_row521_col12" class="data row521 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row522" class="row_heading level0 row522" >522</th>
      <td id="T_c6ca4_row522_col0" class="data row522 col0" >feature_extractor_video.resnet.trunk.layer4.1.bn1.weight</td>
      <td id="T_c6ca4_row522_col1" class="data row522 col1" >feature_extractor_video.resnet.trunk.layer4.1.bn1.weight</td>
      <td id="T_c6ca4_row522_col2" class="data row522 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer4.1.bn1.weight</td>
      <td id="T_c6ca4_row522_col3" class="data row522 col3" >0.459647</td>
      <td id="T_c6ca4_row522_col4" class="data row522 col4" >5.11491</td>
      <td id="T_c6ca4_row522_col5" class="data row522 col5" >4.8082</td>
      <td id="T_c6ca4_row522_col6" class="data row522 col6" >0.0898642</td>
      <td id="T_c6ca4_row522_col7" class="data row522 col7" >0.997617</td>
      <td id="T_c6ca4_row522_col8" class="data row522 col8" >0.0839844</td>
      <td id="T_c6ca4_row522_col9" class="data row522 col9" >512.000000</td>
      <td id="T_c6ca4_row522_col10" class="data row522 col10" >torch.float32</td>
      <td id="T_c6ca4_row522_col11" class="data row522 col11" >(512,)</td>
      <td id="T_c6ca4_row522_col12" class="data row522 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row523" class="row_heading level0 row523" >523</th>
      <td id="T_c6ca4_row523_col0" class="data row523 col0" >feature_extractor_video.resnet.trunk.layer4.1.bn2.bias</td>
      <td id="T_c6ca4_row523_col1" class="data row523 col1" >feature_extractor_video.resnet.trunk.layer4.1.bn2.bias</td>
      <td id="T_c6ca4_row523_col2" class="data row523 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer4.1.bn2.bias</td>
      <td id="T_c6ca4_row523_col3" class="data row523 col3" >0.26721</td>
      <td id="T_c6ca4_row523_col4" class="data row523 col4" >2.05483</td>
      <td id="T_c6ca4_row523_col5" class="data row523 col5" >2.23307</td>
      <td id="T_c6ca4_row523_col6" class="data row523 col6" >0.13004</td>
      <td id="T_c6ca4_row523_col7" class="data row523 col7" >0.995682</td>
      <td id="T_c6ca4_row523_col8" class="data row523 col8" >0.0475464</td>
      <td id="T_c6ca4_row523_col9" class="data row523 col9" >512.000000</td>
      <td id="T_c6ca4_row523_col10" class="data row523 col10" >torch.float32</td>
      <td id="T_c6ca4_row523_col11" class="data row523 col11" >(512,)</td>
      <td id="T_c6ca4_row523_col12" class="data row523 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row524" class="row_heading level0 row524" >524</th>
      <td id="T_c6ca4_row524_col0" class="data row524 col0" >feature_extractor_video.resnet.trunk.layer4.1.bn2.num_batches_tracked</td>
      <td id="T_c6ca4_row524_col1" class="data row524 col1" >feature_extractor_video.resnet.trunk.layer4.1.bn2.num_batches_tracked</td>
      <td id="T_c6ca4_row524_col2" class="data row524 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer4.1.bn2.num_batches_tracked</td>
      <td id="T_c6ca4_row524_col3" class="data row524 col3" >880066</td>
      <td id="T_c6ca4_row524_col4" class="data row524 col4" >601803</td>
      <td id="T_c6ca4_row524_col5" class="data row524 col5" >1.48187e+06</td>
      <td id="T_c6ca4_row524_col6" class="data row524 col6" >1.46238</td>
      <td id="T_c6ca4_row524_col7" class="data row524 col7" >1</td>
      <td id="T_c6ca4_row524_col8" class="data row524 col8" >880066</td>
      <td id="T_c6ca4_row524_col9" class="data row524 col9" >1.000000</td>
      <td id="T_c6ca4_row524_col10" class="data row524 col10" >torch.int64</td>
      <td id="T_c6ca4_row524_col11" class="data row524 col11" >()</td>
      <td id="T_c6ca4_row524_col12" class="data row524 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row525" class="row_heading level0 row525" >525</th>
      <td id="T_c6ca4_row525_col0" class="data row525 col0" >feature_extractor_video.resnet.trunk.layer4.1.bn2.running_mean</td>
      <td id="T_c6ca4_row525_col1" class="data row525 col1" >feature_extractor_video.resnet.trunk.layer4.1.bn2.running_mean</td>
      <td id="T_c6ca4_row525_col2" class="data row525 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer4.1.bn2.running_mean</td>
      <td id="T_c6ca4_row525_col3" class="data row525 col3" >5.6306</td>
      <td id="T_c6ca4_row525_col4" class="data row525 col4" >24.4092</td>
      <td id="T_c6ca4_row525_col5" class="data row525 col5" >22.9619</td>
      <td id="T_c6ca4_row525_col6" class="data row525 col6" >0.230676</td>
      <td id="T_c6ca4_row525_col7" class="data row525 col7" >0.973586</td>
      <td id="T_c6ca4_row525_col8" class="data row525 col8" >1.03516</td>
      <td id="T_c6ca4_row525_col9" class="data row525 col9" >512.000000</td>
      <td id="T_c6ca4_row525_col10" class="data row525 col10" >torch.float32</td>
      <td id="T_c6ca4_row525_col11" class="data row525 col11" >(512,)</td>
      <td id="T_c6ca4_row525_col12" class="data row525 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row526" class="row_heading level0 row526" >526</th>
      <td id="T_c6ca4_row526_col0" class="data row526 col0" >feature_extractor_video.resnet.trunk.layer4.1.bn2.running_var</td>
      <td id="T_c6ca4_row526_col1" class="data row526 col1" >feature_extractor_video.resnet.trunk.layer4.1.bn2.running_var</td>
      <td id="T_c6ca4_row526_col2" class="data row526 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer4.1.bn2.running_var</td>
      <td id="T_c6ca4_row526_col3" class="data row526 col3" >172.561</td>
      <td id="T_c6ca4_row526_col4" class="data row526 col4" >250.992</td>
      <td id="T_c6ca4_row526_col5" class="data row526 col5" >81.3153</td>
      <td id="T_c6ca4_row526_col6" class="data row526 col6" >0.687514</td>
      <td id="T_c6ca4_row526_col7" class="data row526 col7" >0.975821</td>
      <td id="T_c6ca4_row526_col8" class="data row526 col8" >21.0781</td>
      <td id="T_c6ca4_row526_col9" class="data row526 col9" >512.000000</td>
      <td id="T_c6ca4_row526_col10" class="data row526 col10" >torch.float32</td>
      <td id="T_c6ca4_row526_col11" class="data row526 col11" >(512,)</td>
      <td id="T_c6ca4_row526_col12" class="data row526 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row527" class="row_heading level0 row527" >527</th>
      <td id="T_c6ca4_row527_col0" class="data row527 col0" >feature_extractor_video.resnet.trunk.layer4.1.bn2.weight</td>
      <td id="T_c6ca4_row527_col1" class="data row527 col1" >feature_extractor_video.resnet.trunk.layer4.1.bn2.weight</td>
      <td id="T_c6ca4_row527_col2" class="data row527 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer4.1.bn2.weight</td>
      <td id="T_c6ca4_row527_col3" class="data row527 col3" >0.3828</td>
      <td id="T_c6ca4_row527_col4" class="data row527 col4" >3.90517</td>
      <td id="T_c6ca4_row527_col5" class="data row527 col5" >3.61567</td>
      <td id="T_c6ca4_row527_col6" class="data row527 col6" >0.098024</td>
      <td id="T_c6ca4_row527_col7" class="data row527 col7" >0.997779</td>
      <td id="T_c6ca4_row527_col8" class="data row527 col8" >0.0539551</td>
      <td id="T_c6ca4_row527_col9" class="data row527 col9" >512.000000</td>
      <td id="T_c6ca4_row527_col10" class="data row527 col10" >torch.float32</td>
      <td id="T_c6ca4_row527_col11" class="data row527 col11" >(512,)</td>
      <td id="T_c6ca4_row527_col12" class="data row527 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row528" class="row_heading level0 row528" >528</th>
      <td id="T_c6ca4_row528_col0" class="data row528 col0" >feature_extractor_video.resnet.trunk.layer4.1.conv1.weight</td>
      <td id="T_c6ca4_row528_col1" class="data row528 col1" >feature_extractor_video.resnet.trunk.layer4.1.conv1.weight</td>
      <td id="T_c6ca4_row528_col2" class="data row528 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer4.1.conv1.weight</td>
      <td id="T_c6ca4_row528_col3" class="data row528 col3" >26.8936</td>
      <td id="T_c6ca4_row528_col4" class="data row528 col4" >137.248</td>
      <td id="T_c6ca4_row528_col5" class="data row528 col5" >133.008</td>
      <td id="T_c6ca4_row528_col6" class="data row528 col6" >0.195949</td>
      <td id="T_c6ca4_row528_col7" class="data row528 col7" >0.980682</td>
      <td id="T_c6ca4_row528_col8" class="data row528 col8" >0.150391</td>
      <td id="T_c6ca4_row528_col9" class="data row528 col9" >2359296.000000</td>
      <td id="T_c6ca4_row528_col10" class="data row528 col10" >torch.float32</td>
      <td id="T_c6ca4_row528_col11" class="data row528 col11" >(512, 512, 3, 3)</td>
      <td id="T_c6ca4_row528_col12" class="data row528 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row529" class="row_heading level0 row529" >529</th>
      <td id="T_c6ca4_row529_col0" class="data row529 col0" >feature_extractor_video.resnet.trunk.layer4.1.conv2.weight</td>
      <td id="T_c6ca4_row529_col1" class="data row529 col1" >feature_extractor_video.resnet.trunk.layer4.1.conv2.weight</td>
      <td id="T_c6ca4_row529_col2" class="data row529 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer4.1.conv2.weight</td>
      <td id="T_c6ca4_row529_col3" class="data row529 col3" >24.5546</td>
      <td id="T_c6ca4_row529_col4" class="data row529 col4" >120.777</td>
      <td id="T_c6ca4_row529_col5" class="data row529 col5" >117.369</td>
      <td id="T_c6ca4_row529_col6" class="data row529 col6" >0.203305</td>
      <td id="T_c6ca4_row529_col7" class="data row529 col7" >0.979143</td>
      <td id="T_c6ca4_row529_col8" class="data row529 col8" >0.112549</td>
      <td id="T_c6ca4_row529_col9" class="data row529 col9" >2359296.000000</td>
      <td id="T_c6ca4_row529_col10" class="data row529 col10" >torch.float32</td>
      <td id="T_c6ca4_row529_col11" class="data row529 col11" >(512, 512, 3, 3)</td>
      <td id="T_c6ca4_row529_col12" class="data row529 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row530" class="row_heading level0 row530" >530</th>
      <td id="T_c6ca4_row530_col0" class="data row530 col0" >feature_extractor_video.resnet.trunk.layer4.1.relu1.weight</td>
      <td id="T_c6ca4_row530_col1" class="data row530 col1" >feature_extractor_video.resnet.trunk.layer4.1.relu1.weight</td>
      <td id="T_c6ca4_row530_col2" class="data row530 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer4.1.relu1.weight</td>
      <td id="T_c6ca4_row530_col3" class="data row530 col3" >0.349259</td>
      <td id="T_c6ca4_row530_col4" class="data row530 col4" >5.07952</td>
      <td id="T_c6ca4_row530_col5" class="data row530 col5" >5.01189</td>
      <td id="T_c6ca4_row530_col6" class="data row530 col6" >0.0687584</td>
      <td id="T_c6ca4_row530_col7" class="data row530 col7" >0.997694</td>
      <td id="T_c6ca4_row530_col8" class="data row530 col8" >0.0683594</td>
      <td id="T_c6ca4_row530_col9" class="data row530 col9" >512.000000</td>
      <td id="T_c6ca4_row530_col10" class="data row530 col10" >torch.float32</td>
      <td id="T_c6ca4_row530_col11" class="data row530 col11" >(512,)</td>
      <td id="T_c6ca4_row530_col12" class="data row530 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row531" class="row_heading level0 row531" >531</th>
      <td id="T_c6ca4_row531_col0" class="data row531 col0" >feature_extractor_video.resnet.trunk.layer4.1.relu2.weight</td>
      <td id="T_c6ca4_row531_col1" class="data row531 col1" >feature_extractor_video.resnet.trunk.layer4.1.relu2.weight</td>
      <td id="T_c6ca4_row531_col2" class="data row531 col2" >av_romanizer.w2v_model.feature_extractor_video.resnet.trunk.layer4.1.relu2.weight</td>
      <td id="T_c6ca4_row531_col3" class="data row531 col3" >0.216943</td>
      <td id="T_c6ca4_row531_col4" class="data row531 col4" >1.84801</td>
      <td id="T_c6ca4_row531_col5" class="data row531 col5" >1.70884</td>
      <td id="T_c6ca4_row531_col6" class="data row531 col6" >0.117393</td>
      <td id="T_c6ca4_row531_col7" class="data row531 col7" >0.995615</td>
      <td id="T_c6ca4_row531_col8" class="data row531 col8" >0.0463867</td>
      <td id="T_c6ca4_row531_col9" class="data row531 col9" >512.000000</td>
      <td id="T_c6ca4_row531_col10" class="data row531 col10" >torch.float32</td>
      <td id="T_c6ca4_row531_col11" class="data row531 col11" >(512,)</td>
      <td id="T_c6ca4_row531_col12" class="data row531 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row532" class="row_heading level0 row532" >532</th>
      <td id="T_c6ca4_row532_col0" class="data row532 col0" >final_proj.bias</td>
      <td id="T_c6ca4_row532_col1" class="data row532 col1" >final_proj.bias</td>
      <td id="T_c6ca4_row532_col2" class="data row532 col2" >nan</td>
      <td id="T_c6ca4_row532_col3" class="data row532 col3" ></td>
      <td id="T_c6ca4_row532_col4" class="data row532 col4" ></td>
      <td id="T_c6ca4_row532_col5" class="data row532 col5" ></td>
      <td id="T_c6ca4_row532_col6" class="data row532 col6" ></td>
      <td id="T_c6ca4_row532_col7" class="data row532 col7" ></td>
      <td id="T_c6ca4_row532_col8" class="data row532 col8" ></td>
      <td id="T_c6ca4_row532_col9" class="data row532 col9" >nan</td>
      <td id="T_c6ca4_row532_col10" class="data row532 col10" >torch.float32</td>
      <td id="T_c6ca4_row532_col11" class="data row532 col11" >(256,)</td>
      <td id="T_c6ca4_row532_col12" class="data row532 col12" >missing from model 2</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row533" class="row_heading level0 row533" >533</th>
      <td id="T_c6ca4_row533_col0" class="data row533 col0" >final_proj.weight</td>
      <td id="T_c6ca4_row533_col1" class="data row533 col1" >final_proj.weight</td>
      <td id="T_c6ca4_row533_col2" class="data row533 col2" >nan</td>
      <td id="T_c6ca4_row533_col3" class="data row533 col3" ></td>
      <td id="T_c6ca4_row533_col4" class="data row533 col4" ></td>
      <td id="T_c6ca4_row533_col5" class="data row533 col5" ></td>
      <td id="T_c6ca4_row533_col6" class="data row533 col6" ></td>
      <td id="T_c6ca4_row533_col7" class="data row533 col7" ></td>
      <td id="T_c6ca4_row533_col8" class="data row533 col8" ></td>
      <td id="T_c6ca4_row533_col9" class="data row533 col9" >nan</td>
      <td id="T_c6ca4_row533_col10" class="data row533 col10" >torch.float32</td>
      <td id="T_c6ca4_row533_col11" class="data row533 col11" >(256, 1024)</td>
      <td id="T_c6ca4_row533_col12" class="data row533 col12" >missing from model 2</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row534" class="row_heading level0 row534" >534</th>
      <td id="T_c6ca4_row534_col0" class="data row534 col0" >label_embs_concat</td>
      <td id="T_c6ca4_row534_col1" class="data row534 col1" >label_embs_concat</td>
      <td id="T_c6ca4_row534_col2" class="data row534 col2" >av_romanizer.w2v_model.label_embs_concat</td>
      <td id="T_c6ca4_row534_col3" class="data row534 col3" >6.36277</td>
      <td id="T_c6ca4_row534_col4" class="data row534 col4" >117.703</td>
      <td id="T_c6ca4_row534_col5" class="data row534 col5" >111.341</td>
      <td id="T_c6ca4_row534_col6" class="data row534 col6" >0.0540576</td>
      <td id="T_c6ca4_row534_col7" class="data row534 col7" >1</td>
      <td id="T_c6ca4_row534_col8" class="data row534 col8" >0.046875</td>
      <td id="T_c6ca4_row534_col9" class="data row534 col9" >513024.000000</td>
      <td id="T_c6ca4_row534_col10" class="data row534 col10" >torch.float32</td>
      <td id="T_c6ca4_row534_col11" class="data row534 col11" >(2004, 256)</td>
      <td id="T_c6ca4_row534_col12" class="data row534 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row535" class="row_heading level0 row535" >535</th>
      <td id="T_c6ca4_row535_col0" class="data row535 col0" >layer_norm.bias</td>
      <td id="T_c6ca4_row535_col1" class="data row535 col1" >layer_norm.bias</td>
      <td id="T_c6ca4_row535_col2" class="data row535 col2" >av_romanizer.w2v_model.layer_norm.bias</td>
      <td id="T_c6ca4_row535_col3" class="data row535 col3" >0.357726</td>
      <td id="T_c6ca4_row535_col4" class="data row535 col4" >1.86376</td>
      <td id="T_c6ca4_row535_col5" class="data row535 col5" >1.76655</td>
      <td id="T_c6ca4_row535_col6" class="data row535 col6" >0.191938</td>
      <td id="T_c6ca4_row535_col7" class="data row535 col7" >0.982001</td>
      <td id="T_c6ca4_row535_col8" class="data row535 col8" >0.0900879</td>
      <td id="T_c6ca4_row535_col9" class="data row535 col9" >2048.000000</td>
      <td id="T_c6ca4_row535_col10" class="data row535 col10" >torch.float32</td>
      <td id="T_c6ca4_row535_col11" class="data row535 col11" >(2048,)</td>
      <td id="T_c6ca4_row535_col12" class="data row535 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row536" class="row_heading level0 row536" >536</th>
      <td id="T_c6ca4_row536_col0" class="data row536 col0" >layer_norm.weight</td>
      <td id="T_c6ca4_row536_col1" class="data row536 col1" >layer_norm.weight</td>
      <td id="T_c6ca4_row536_col2" class="data row536 col2" >av_romanizer.w2v_model.layer_norm.weight</td>
      <td id="T_c6ca4_row536_col3" class="data row536 col3" >0.362024</td>
      <td id="T_c6ca4_row536_col4" class="data row536 col4" >3.03036</td>
      <td id="T_c6ca4_row536_col5" class="data row536 col5" >2.96459</td>
      <td id="T_c6ca4_row536_col6" class="data row536 col6" >0.119466</td>
      <td id="T_c6ca4_row536_col7" class="data row536 col7" >0.992946</td>
      <td id="T_c6ca4_row536_col8" class="data row536 col8" >0.076416</td>
      <td id="T_c6ca4_row536_col9" class="data row536 col9" >2048.000000</td>
      <td id="T_c6ca4_row536_col10" class="data row536 col10" >torch.float32</td>
      <td id="T_c6ca4_row536_col11" class="data row536 col11" >(2048,)</td>
      <td id="T_c6ca4_row536_col12" class="data row536 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row537" class="row_heading level0 row537" >537</th>
      <td id="T_c6ca4_row537_col0" class="data row537 col0" >mask_emb</td>
      <td id="T_c6ca4_row537_col1" class="data row537 col1" >mask_emb</td>
      <td id="T_c6ca4_row537_col2" class="data row537 col2" >av_romanizer.w2v_model.mask_emb</td>
      <td id="T_c6ca4_row537_col3" class="data row537 col3" >5.51385</td>
      <td id="T_c6ca4_row537_col4" class="data row537 col4" >2.12599</td>
      <td id="T_c6ca4_row537_col5" class="data row537 col5" >5.36175</td>
      <td id="T_c6ca4_row537_col6" class="data row537 col6" >2.59354</td>
      <td id="T_c6ca4_row537_col7" class="data row537 col7" >0.125697</td>
      <td id="T_c6ca4_row537_col8" class="data row537 col8" >1.64551</td>
      <td id="T_c6ca4_row537_col9" class="data row537 col9" >104.000000</td>
      <td id="T_c6ca4_row537_col10" class="data row537 col10" >torch.float32</td>
      <td id="T_c6ca4_row537_col11" class="data row537 col11" >(104,)</td>
      <td id="T_c6ca4_row537_col12" class="data row537 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row538" class="row_heading level0 row538" >538</th>
      <td id="T_c6ca4_row538_col0" class="data row538 col0" >post_extract_proj.bias</td>
      <td id="T_c6ca4_row538_col1" class="data row538 col1" >post_extract_proj.bias</td>
      <td id="T_c6ca4_row538_col2" class="data row538 col2" >av_romanizer.w2v_model.post_extract_proj.bias</td>
      <td id="T_c6ca4_row538_col3" class="data row538 col3" >0.612798</td>
      <td id="T_c6ca4_row538_col4" class="data row538 col4" >5.63464</td>
      <td id="T_c6ca4_row538_col5" class="data row538 col5" >5.36558</td>
      <td id="T_c6ca4_row538_col6" class="data row538 col6" >0.108755</td>
      <td id="T_c6ca4_row538_col7" class="data row538 col7" >0.994987</td>
      <td id="T_c6ca4_row538_col8" class="data row538 col8" >0.139648</td>
      <td id="T_c6ca4_row538_col9" class="data row538 col9" >1024.000000</td>
      <td id="T_c6ca4_row538_col10" class="data row538 col10" >torch.float32</td>
      <td id="T_c6ca4_row538_col11" class="data row538 col11" >(1024,)</td>
      <td id="T_c6ca4_row538_col12" class="data row538 col12" >nan</td>
    </tr>
    <tr>
      <th id="T_c6ca4_level0_row539" class="row_heading level0 row539" >539</th>
      <td id="T_c6ca4_row539_col0" class="data row539 col0" >post_extract_proj.weight</td>
      <td id="T_c6ca4_row539_col1" class="data row539 col1" >post_extract_proj.weight</td>
      <td id="T_c6ca4_row539_col2" class="data row539 col2" >av_romanizer.w2v_model.post_extract_proj.weight</td>
      <td id="T_c6ca4_row539_col3" class="data row539 col3" >12.2788</td>
      <td id="T_c6ca4_row539_col4" class="data row539 col4" >80.5908</td>
      <td id="T_c6ca4_row539_col5" class="data row539 col5" >77.1734</td>
      <td id="T_c6ca4_row539_col6" class="data row539 col6" >0.15236</td>
      <td id="T_c6ca4_row539_col7" class="data row539 col7" >0.988818</td>
      <td id="T_c6ca4_row539_col8" class="data row539 col8" >0.145996</td>
      <td id="T_c6ca4_row539_col9" class="data row539 col9" >2097152.000000</td>
      <td id="T_c6ca4_row539_col10" class="data row539 col10" >torch.float32</td>
      <td id="T_c6ca4_row539_col11" class="data row539 col11" >(1024, 2048)</td>
      <td id="T_c6ca4_row539_col12" class="data row539 col12" >nan</td>
    </tr>
  </tbody>
</table>

  </body>
</html>
