
<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <title>LLaMA + LoRA Parameter Comparison</title>
    <style>
      body { font-family: system-ui, -apple-system, 'Segoe UI', sans-serif; margin: 2rem; }
      h1 { margin-bottom: 1rem; }
      .summary { margin-bottom: 1.5rem; }
      table { border-collapse: collapse; width: 100%; }
      th, td { border: 1px solid #ddd; padding: 0.4rem 0.6rem; }
      th { background: #f8f8f8; position: sticky; top: 0; }
    </style>
  </head>
  <body>
    <h1>LLaMA + LoRA Parameter Comparison</h1>
    <p class="summary">Baseline: llama (Llama-3.2-3B) | Adapter: lora (checkpoint_best.pt)</p>
    <style type="text/css">
#T_721ed_row0_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 78.3%, transparent 78.3%);
}
#T_721ed_row0_col7 {
  background-color: #feec9f;
  color: #000000;
}
#T_721ed_row1_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 58.3%, transparent 58.3%);
}
#T_721ed_row1_col7, #T_721ed_row71_col7 {
  background-color: #84ca66;
  color: #000000;
}
#T_721ed_row2_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 95.1%, transparent 95.1%);
}
#T_721ed_row2_col7 {
  background-color: #d93429;
  color: #f1f1f1;
}
#T_721ed_row3_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 100.0%, transparent 100.0%);
}
#T_721ed_row3_col7 {
  background-color: #a50026;
  color: #f1f1f1;
}
#T_721ed_row4_col6, #T_721ed_row105_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 66.4%, transparent 66.4%);
}
#T_721ed_row4_col7, #T_721ed_row105_col7, #T_721ed_row108_col7 {
  background-color: #c7e77f;
  color: #000000;
}
#T_721ed_row5_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 84.3%, transparent 84.3%);
}
#T_721ed_row5_col7 {
  background-color: #fdbd6d;
  color: #000000;
}
#T_721ed_row6_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 87.1%, transparent 87.1%);
}
#T_721ed_row6_col7 {
  background-color: #fa9b58;
  color: #000000;
}
#T_721ed_row7_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 76.7%, transparent 76.7%);
}
#T_721ed_row7_col7 {
  background-color: #fff5ae;
  color: #000000;
}
#T_721ed_row8_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 81.8%, transparent 81.8%);
}
#T_721ed_row8_col7 {
  background-color: #fed481;
  color: #000000;
}
#T_721ed_row9_col6, #T_721ed_row74_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 74.7%, transparent 74.7%);
}
#T_721ed_row9_col7, #T_721ed_row35_col7, #T_721ed_row74_col7 {
  background-color: #feffbe;
  color: #000000;
}
#T_721ed_row10_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 84.8%, transparent 84.8%);
}
#T_721ed_row10_col7 {
  background-color: #fdb768;
  color: #000000;
}
#T_721ed_row11_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 66.8%, transparent 66.8%);
}
#T_721ed_row11_col7 {
  background-color: #c9e881;
  color: #000000;
}
#T_721ed_row12_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 61.5%, transparent 61.5%);
}
#T_721ed_row12_col7, #T_721ed_row103_col7 {
  background-color: #a0d669;
  color: #000000;
}
#T_721ed_row13_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 66.0%, transparent 66.0%);
}
#T_721ed_row13_col7 {
  background-color: #c3e67d;
  color: #000000;
}
#T_721ed_row14_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 61.2%, transparent 61.2%);
}
#T_721ed_row14_col7, #T_721ed_row57_col7 {
  background-color: #9dd569;
  color: #000000;
}
#T_721ed_row15_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 62.7%, transparent 62.7%);
}
#T_721ed_row15_col7, #T_721ed_row16_col7, #T_721ed_row43_col7 {
  background-color: #abdb6d;
  color: #000000;
}
#T_721ed_row16_col6, #T_721ed_row43_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 62.9%, transparent 62.9%);
}
#T_721ed_row17_col6, #T_721ed_row22_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 56.0%, transparent 56.0%);
}
#T_721ed_row17_col7, #T_721ed_row22_col7, #T_721ed_row70_col7 {
  background-color: #70c164;
  color: #000000;
}
#T_721ed_row18_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 59.4%, transparent 59.4%);
}
#T_721ed_row18_col7 {
  background-color: #8ecf67;
  color: #000000;
}
#T_721ed_row19_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 87.9%, transparent 87.9%);
}
#T_721ed_row19_col7 {
  background-color: #f99355;
  color: #000000;
}
#T_721ed_row20_col6, #T_721ed_row78_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 53.2%, transparent 53.2%);
}
#T_721ed_row20_col7, #T_721ed_row75_col7, #T_721ed_row78_col7 {
  background-color: #57b65f;
  color: #f1f1f1;
}
#T_721ed_row21_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 55.3%, transparent 55.3%);
}
#T_721ed_row21_col7, #T_721ed_row25_col7 {
  background-color: #6bbf64;
  color: #000000;
}
#T_721ed_row23_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 54.3%, transparent 54.3%);
}
#T_721ed_row23_col7, #T_721ed_row36_col7, #T_721ed_row67_col7, #T_721ed_row77_col7 {
  background-color: #63bc62;
  color: #f1f1f1;
}
#T_721ed_row24_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 82.5%, transparent 82.5%);
}
#T_721ed_row24_col7 {
  background-color: #fecc7b;
  color: #000000;
}
#T_721ed_row25_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 55.4%, transparent 55.4%);
}
#T_721ed_row26_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 81.3%, transparent 81.3%);
}
#T_721ed_row26_col7 {
  background-color: #fed884;
  color: #000000;
}
#T_721ed_row27_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 50.9%, transparent 50.9%);
}
#T_721ed_row27_col7, #T_721ed_row28_col7 {
  background-color: #42ac5a;
  color: #f1f1f1;
}
#T_721ed_row28_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 50.8%, transparent 50.8%);
}
#T_721ed_row29_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 52.0%, transparent 52.0%);
}
#T_721ed_row29_col7 {
  background-color: #4eb15d;
  color: #f1f1f1;
}
#T_721ed_row30_col6, #T_721ed_row59_col6, #T_721ed_row68_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 47.1%, transparent 47.1%);
}
#T_721ed_row30_col7, #T_721ed_row38_col7, #T_721ed_row59_col7, #T_721ed_row68_col7 {
  background-color: #219c52;
  color: #f1f1f1;
}
#T_721ed_row31_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 46.2%, transparent 46.2%);
}
#T_721ed_row31_col7 {
  background-color: #1b9950;
  color: #f1f1f1;
}
#T_721ed_row32_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 46.8%, transparent 46.8%);
}
#T_721ed_row32_col7 {
  background-color: #1e9a51;
  color: #f1f1f1;
}
#T_721ed_row33_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 46.0%, transparent 46.0%);
}
#T_721ed_row33_col7 {
  background-color: #199750;
  color: #f1f1f1;
}
#T_721ed_row34_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 47.5%, transparent 47.5%);
}
#T_721ed_row34_col7 {
  background-color: #249d53;
  color: #f1f1f1;
}
#T_721ed_row35_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 74.8%, transparent 74.8%);
}
#T_721ed_row36_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 54.5%, transparent 54.5%);
}
#T_721ed_row37_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 47.6%, transparent 47.6%);
}
#T_721ed_row37_col7 {
  background-color: #279f53;
  color: #f1f1f1;
}
#T_721ed_row38_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 46.9%, transparent 46.9%);
}
#T_721ed_row39_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 45.5%, transparent 45.5%);
}
#T_721ed_row39_col7 {
  background-color: #18954f;
  color: #f1f1f1;
}
#T_721ed_row40_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 41.9%, transparent 41.9%);
}
#T_721ed_row40_col7 {
  background-color: #0f8446;
  color: #f1f1f1;
}
#T_721ed_row41_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 64.5%, transparent 64.5%);
}
#T_721ed_row41_col7 {
  background-color: #b7e075;
  color: #000000;
}
#T_721ed_row42_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 40.9%, transparent 40.9%);
}
#T_721ed_row42_col7 {
  background-color: #0c7f43;
  color: #f1f1f1;
}
#T_721ed_row44_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 68.3%, transparent 68.3%);
}
#T_721ed_row44_col7 {
  background-color: #d5ed88;
  color: #000000;
}
#T_721ed_row45_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 61.8%, transparent 61.8%);
}
#T_721ed_row45_col7 {
  background-color: #a2d76a;
  color: #000000;
}
#T_721ed_row46_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 60.3%, transparent 60.3%);
}
#T_721ed_row46_col7 {
  background-color: #96d268;
  color: #000000;
}
#T_721ed_row47_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 57.2%, transparent 57.2%);
}
#T_721ed_row47_col7 {
  background-color: #7ac665;
  color: #000000;
}
#T_721ed_row48_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 63.7%, transparent 63.7%);
}
#T_721ed_row48_col7 {
  background-color: #b1de71;
  color: #000000;
}
#T_721ed_row49_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 57.7%, transparent 57.7%);
}
#T_721ed_row49_col7, #T_721ed_row109_col7 {
  background-color: #7fc866;
  color: #000000;
}
#T_721ed_row50_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 71.5%, transparent 71.5%);
}
#T_721ed_row50_col7 {
  background-color: #e9f6a1;
  color: #000000;
}
#T_721ed_row51_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 90.3%, transparent 90.3%);
}
#T_721ed_row51_col7, #T_721ed_row81_col7 {
  background-color: #f57245;
  color: #f1f1f1;
}
#T_721ed_row52_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 81.6%, transparent 81.6%);
}
#T_721ed_row52_col7 {
  background-color: #fed683;
  color: #000000;
}
#T_721ed_row53_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 59.9%, transparent 59.9%);
}
#T_721ed_row53_col7 {
  background-color: #93d168;
  color: #000000;
}
#T_721ed_row54_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 85.0%, transparent 85.0%);
}
#T_721ed_row54_col7 {
  background-color: #fdb567;
  color: #000000;
}
#T_721ed_row55_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 55.6%, transparent 55.6%);
}
#T_721ed_row55_col7, #T_721ed_row73_col7 {
  background-color: #6ec064;
  color: #000000;
}
#T_721ed_row56_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 93.7%, transparent 93.7%);
}
#T_721ed_row56_col7 {
  background-color: #e24731;
  color: #f1f1f1;
}
#T_721ed_row57_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 61.3%, transparent 61.3%);
}
#T_721ed_row58_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 63.0%, transparent 63.0%);
}
#T_721ed_row58_col7 {
  background-color: #addc6f;
  color: #000000;
}
#T_721ed_row60_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 59.8%, transparent 59.8%);
}
#T_721ed_row60_col7 {
  background-color: #91d068;
  color: #000000;
}
#T_721ed_row61_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 50.0%, transparent 50.0%);
}
#T_721ed_row61_col7, #T_721ed_row79_col7 {
  background-color: #39a758;
  color: #f1f1f1;
}
#T_721ed_row62_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 73.7%, transparent 73.7%);
}
#T_721ed_row62_col7, #T_721ed_row100_col7 {
  background-color: #f8fcb6;
  color: #000000;
}
#T_721ed_row63_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 48.6%, transparent 48.6%);
}
#T_721ed_row63_col7 {
  background-color: #30a356;
  color: #f1f1f1;
}
#T_721ed_row64_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 64.2%, transparent 64.2%);
}
#T_721ed_row64_col7 {
  background-color: #b5df74;
  color: #000000;
}
#T_721ed_row65_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 53.8%, transparent 53.8%);
}
#T_721ed_row65_col7 {
  background-color: #5db961;
  color: #f1f1f1;
}
#T_721ed_row66_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 49.3%, transparent 49.3%);
}
#T_721ed_row66_col7 {
  background-color: #36a657;
  color: #f1f1f1;
}
#T_721ed_row67_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 54.2%, transparent 54.2%);
}
#T_721ed_row69_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 71.8%, transparent 71.8%);
}
#T_721ed_row69_col7 {
  background-color: #ecf7a6;
  color: #000000;
}
#T_721ed_row70_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 55.9%, transparent 55.9%);
}
#T_721ed_row71_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 58.4%, transparent 58.4%);
}
#T_721ed_row72_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 67.2%, transparent 67.2%);
}
#T_721ed_row72_col7 {
  background-color: #cdea83;
  color: #000000;
}
#T_721ed_row73_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 55.7%, transparent 55.7%);
}
#T_721ed_row75_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 53.1%, transparent 53.1%);
}
#T_721ed_row76_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 42.9%, transparent 42.9%);
}
#T_721ed_row76_col7 {
  background-color: #118848;
  color: #f1f1f1;
}
#T_721ed_row77_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 54.4%, transparent 54.4%);
}
#T_721ed_row79_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 49.7%, transparent 49.7%);
}
#T_721ed_row80_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 43.8%, transparent 43.8%);
}
#T_721ed_row80_col7 {
  background-color: #138c4a;
  color: #f1f1f1;
}
#T_721ed_row81_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 90.2%, transparent 90.2%);
}
#T_721ed_row82_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 40.2%, transparent 40.2%);
}
#T_721ed_row82_col7 {
  background-color: #0a7b41;
  color: #f1f1f1;
}
#T_721ed_row83_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 38.1%, transparent 38.1%);
}
#T_721ed_row83_col7, #T_721ed_row86_col7 {
  background-color: #05713c;
  color: #f1f1f1;
}
#T_721ed_row84_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 39.5%, transparent 39.5%);
}
#T_721ed_row84_col7 {
  background-color: #097940;
  color: #f1f1f1;
}
#T_721ed_row85_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 38.7%, transparent 38.7%);
}
#T_721ed_row85_col7 {
  background-color: #07753e;
  color: #f1f1f1;
}
#T_721ed_row86_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 37.9%, transparent 37.9%);
}
#T_721ed_row87_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 37.2%, transparent 37.2%);
}
#T_721ed_row87_col7, #T_721ed_row90_col7 {
  background-color: #04703b;
  color: #f1f1f1;
}
#T_721ed_row88_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 35.5%, transparent 35.5%);
}
#T_721ed_row88_col7, #T_721ed_row92_col7 {
  background-color: #006837;
  color: #f1f1f1;
}
#T_721ed_row89_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 36.3%, transparent 36.3%);
}
#T_721ed_row89_col7 {
  background-color: #026c39;
  color: #f1f1f1;
}
#T_721ed_row90_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 37.5%, transparent 37.5%);
}
#T_721ed_row91_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 35.9%, transparent 35.9%);
}
#T_721ed_row91_col7 {
  background-color: #016a38;
  color: #f1f1f1;
}
#T_721ed_row92_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 35.3%, transparent 35.3%);
}
#T_721ed_row93_col6, #T_721ed_row95_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 76.5%, transparent 76.5%);
}
#T_721ed_row93_col7, #T_721ed_row95_col7 {
  background-color: #fff6b0;
  color: #000000;
}
#T_721ed_row94_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 80.9%, transparent 80.9%);
}
#T_721ed_row94_col7 {
  background-color: #fedc88;
  color: #000000;
}
#T_721ed_row96_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 69.2%, transparent 69.2%);
}
#T_721ed_row96_col7 {
  background-color: #dcf08f;
  color: #000000;
}
#T_721ed_row97_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 66.3%, transparent 66.3%);
}
#T_721ed_row97_col7 {
  background-color: #c5e67e;
  color: #000000;
}
#T_721ed_row98_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 67.7%, transparent 67.7%);
}
#T_721ed_row98_col7 {
  background-color: #d1ec86;
  color: #000000;
}
#T_721ed_row99_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 68.5%, transparent 68.5%);
}
#T_721ed_row99_col7 {
  background-color: #d7ee8a;
  color: #000000;
}
#T_721ed_row100_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 73.8%, transparent 73.8%);
}
#T_721ed_row101_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 71.3%, transparent 71.3%);
}
#T_721ed_row101_col7 {
  background-color: #e8f59f;
  color: #000000;
}
#T_721ed_row102_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 67.0%, transparent 67.0%);
}
#T_721ed_row102_col7 {
  background-color: #cbe982;
  color: #000000;
}
#T_721ed_row103_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 61.4%, transparent 61.4%);
}
#T_721ed_row104_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 63.8%, transparent 63.8%);
}
#T_721ed_row104_col7 {
  background-color: #b3df72;
  color: #000000;
}
#T_721ed_row106_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 64.9%, transparent 64.9%);
}
#T_721ed_row106_col7 {
  background-color: #bbe278;
  color: #000000;
}
#T_721ed_row107_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 56.9%, transparent 56.9%);
}
#T_721ed_row107_col7 {
  background-color: #78c565;
  color: #000000;
}
#T_721ed_row108_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 66.6%, transparent 66.6%);
}
#T_721ed_row109_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 57.8%, transparent 57.8%);
}
#T_721ed_row110_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 60.6%, transparent 60.6%);
}
#T_721ed_row110_col7 {
  background-color: #98d368;
  color: #000000;
}
#T_721ed_row111_col6 {
  width: 10em;
  background: linear-gradient(90deg, #ffb347 65.1%, transparent 65.1%);
}
#T_721ed_row111_col7 {
  background-color: #bde379;
  color: #000000;
}
</style>
<table id="T_721ed">
  <thead>
    <tr>
      <th class="blank level0" >&nbsp;</th>
      <th id="T_721ed_level0_col0" class="col_heading level0 col0" >name</th>
      <th id="T_721ed_level0_col1" class="col_heading level0 col1" >llama (Llama-3.2-3B)</th>
      <th id="T_721ed_level0_col2" class="col_heading level0 col2" >lora (checkpoint_best.pt)</th>
      <th id="T_721ed_level0_col3" class="col_heading level0 col3" >delta_l2</th>
      <th id="T_721ed_level0_col4" class="col_heading level0 col4" >baseline_l2</th>
      <th id="T_721ed_level0_col5" class="col_heading level0 col5" >updated_l2</th>
      <th id="T_721ed_level0_col6" class="col_heading level0 col6" >relative_delta</th>
      <th id="T_721ed_level0_col7" class="col_heading level0 col7" >cosine_similarity</th>
      <th id="T_721ed_level0_col8" class="col_heading level0 col8" >max_abs_delta</th>
      <th id="T_721ed_level0_col9" class="col_heading level0 col9" >delta_l1</th>
      <th id="T_721ed_level0_col10" class="col_heading level0 col10" >delta_density</th>
      <th id="T_721ed_level0_col11" class="col_heading level0 col11" >numel</th>
      <th id="T_721ed_level0_col12" class="col_heading level0 col12" >dtype</th>
      <th id="T_721ed_level0_col13" class="col_heading level0 col13" >shape</th>
      <th id="T_721ed_level0_col14" class="col_heading level0 col14" >lora_rank</th>
      <th id="T_721ed_level0_col15" class="col_heading level0 col15" >lora_alpha</th>
      <th id="T_721ed_level0_col16" class="col_heading level0 col16" >lora_scale</th>
      <th id="T_721ed_level0_col17" class="col_heading level0 col17" >lora_up</th>
      <th id="T_721ed_level0_col18" class="col_heading level0 col18" >lora_down</th>
      <th id="T_721ed_level0_col19" class="col_heading level0 col19" >message</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th id="T_721ed_level0_row0" class="row_heading level0 row0" >0</th>
      <td id="T_721ed_row0_col0" class="data row0 col0" >model.layers.1.self_attn.q_proj.weight</td>
      <td id="T_721ed_row0_col1" class="data row0 col1" >model.layers.1.self_attn.q_proj.weight</td>
      <td id="T_721ed_row0_col2" class="data row0 col2" >llama.model.layers.1.self_attn.q_proj</td>
      <td id="T_721ed_row0_col3" class="data row0 col3" >8.88873</td>
      <td id="T_721ed_row0_col4" class="data row0 col4" >86.9201</td>
      <td id="T_721ed_row0_col5" class="data row0 col5" >87.3887</td>
      <td id="T_721ed_row0_col6" class="data row0 col6" >0.102263</td>
      <td id="T_721ed_row0_col7" class="data row0 col7" >0.994814</td>
      <td id="T_721ed_row0_col8" class="data row0 col8" >0.068485</td>
      <td id="T_721ed_row0_col9" class="data row0 col9" >16955.1</td>
      <td id="T_721ed_row0_col10" class="data row0 col10" >1</td>
      <td id="T_721ed_row0_col11" class="data row0 col11" >9437184</td>
      <td id="T_721ed_row0_col12" class="data row0 col12" >torch.float32</td>
      <td id="T_721ed_row0_col13" class="data row0 col13" >(3072, 3072)</td>
      <td id="T_721ed_row0_col14" class="data row0 col14" >16</td>
      <td id="T_721ed_row0_col15" class="data row0 col15" >16.000000</td>
      <td id="T_721ed_row0_col16" class="data row0 col16" >1.000000</td>
      <td id="T_721ed_row0_col17" class="data row0 col17" >llama.model.layers.1.self_attn.q_proj.lora_B.default.weight</td>
      <td id="T_721ed_row0_col18" class="data row0 col18" >llama.model.layers.1.self_attn.q_proj.lora_A.default.weight</td>
      <td id="T_721ed_row0_col19" class="data row0 col19" >nan</td>
    </tr>
    <tr>
      <th id="T_721ed_level0_row1" class="row_heading level0 row1" >1</th>
      <td id="T_721ed_row1_col0" class="data row1 col0" >model.layers.0.self_attn.q_proj.weight</td>
      <td id="T_721ed_row1_col1" class="data row1 col1" >model.layers.0.self_attn.q_proj.weight</td>
      <td id="T_721ed_row1_col2" class="data row1 col2" >llama.model.layers.0.self_attn.q_proj</td>
      <td id="T_721ed_row1_col3" class="data row1 col3" >8.1497</td>
      <td id="T_721ed_row1_col4" class="data row1 col4" >106.996</td>
      <td id="T_721ed_row1_col5" class="data row1 col5" >107.174</td>
      <td id="T_721ed_row1_col6" class="data row1 col6" >0.0761684</td>
      <td id="T_721ed_row1_col7" class="data row1 col7" >0.997105</td>
      <td id="T_721ed_row1_col8" class="data row1 col8" >0.0596914</td>
      <td id="T_721ed_row1_col9" class="data row1 col9" >17070.8</td>
      <td id="T_721ed_row1_col10" class="data row1 col10" >1</td>
      <td id="T_721ed_row1_col11" class="data row1 col11" >9437184</td>
      <td id="T_721ed_row1_col12" class="data row1 col12" >torch.float32</td>
      <td id="T_721ed_row1_col13" class="data row1 col13" >(3072, 3072)</td>
      <td id="T_721ed_row1_col14" class="data row1 col14" >16</td>
      <td id="T_721ed_row1_col15" class="data row1 col15" >16.000000</td>
      <td id="T_721ed_row1_col16" class="data row1 col16" >1.000000</td>
      <td id="T_721ed_row1_col17" class="data row1 col17" >llama.model.layers.0.self_attn.q_proj.lora_B.default.weight</td>
      <td id="T_721ed_row1_col18" class="data row1 col18" >llama.model.layers.0.self_attn.q_proj.lora_A.default.weight</td>
      <td id="T_721ed_row1_col19" class="data row1 col19" >nan</td>
    </tr>
    <tr>
      <th id="T_721ed_level0_row2" class="row_heading level0 row2" >2</th>
      <td id="T_721ed_row2_col0" class="data row2 col0" >model.layers.22.self_attn.o_proj.weight</td>
      <td id="T_721ed_row2_col1" class="data row2 col1" >model.layers.22.self_attn.o_proj.weight</td>
      <td id="T_721ed_row2_col2" class="data row2 col2" >llama.model.layers.22.self_attn.o_proj</td>
      <td id="T_721ed_row2_col3" class="data row2 col3" >6.79239</td>
      <td id="T_721ed_row2_col4" class="data row2 col4" >54.7315</td>
      <td id="T_721ed_row2_col5" class="data row2 col5" >55.1404</td>
      <td id="T_721ed_row2_col6" class="data row2 col6" >0.124104</td>
      <td id="T_721ed_row2_col7" class="data row2 col7" >0.992384</td>
      <td id="T_721ed_row2_col8" class="data row2 col8" >0.0246528</td>
      <td id="T_721ed_row2_col9" class="data row2 col9" >15760.5</td>
      <td id="T_721ed_row2_col10" class="data row2 col10" >1</td>
      <td id="T_721ed_row2_col11" class="data row2 col11" >9437184</td>
      <td id="T_721ed_row2_col12" class="data row2 col12" >torch.float32</td>
      <td id="T_721ed_row2_col13" class="data row2 col13" >(3072, 3072)</td>
      <td id="T_721ed_row2_col14" class="data row2 col14" >16</td>
      <td id="T_721ed_row2_col15" class="data row2 col15" >16.000000</td>
      <td id="T_721ed_row2_col16" class="data row2 col16" >1.000000</td>
      <td id="T_721ed_row2_col17" class="data row2 col17" >llama.model.layers.22.self_attn.o_proj.lora_B.default.weight</td>
      <td id="T_721ed_row2_col18" class="data row2 col18" >llama.model.layers.22.self_attn.o_proj.lora_A.default.weight</td>
      <td id="T_721ed_row2_col19" class="data row2 col19" >nan</td>
    </tr>
    <tr>
      <th id="T_721ed_level0_row3" class="row_heading level0 row3" >3</th>
      <td id="T_721ed_row3_col0" class="data row3 col0" >model.layers.20.self_attn.o_proj.weight</td>
      <td id="T_721ed_row3_col1" class="data row3 col1" >model.layers.20.self_attn.o_proj.weight</td>
      <td id="T_721ed_row3_col2" class="data row3 col2" >llama.model.layers.20.self_attn.o_proj</td>
      <td id="T_721ed_row3_col3" class="data row3 col3" >6.71089</td>
      <td id="T_721ed_row3_col4" class="data row3 col4" >51.4089</td>
      <td id="T_721ed_row3_col5" class="data row3 col5" >51.8302</td>
      <td id="T_721ed_row3_col6" class="data row3 col6" >0.130539</td>
      <td id="T_721ed_row3_col7" class="data row3 col7" >0.991582</td>
      <td id="T_721ed_row3_col8" class="data row3 col8" >0.0256012</td>
      <td id="T_721ed_row3_col9" class="data row3 col9" >15594.1</td>
      <td id="T_721ed_row3_col10" class="data row3 col10" >1</td>
      <td id="T_721ed_row3_col11" class="data row3 col11" >9437184</td>
      <td id="T_721ed_row3_col12" class="data row3 col12" >torch.float32</td>
      <td id="T_721ed_row3_col13" class="data row3 col13" >(3072, 3072)</td>
      <td id="T_721ed_row3_col14" class="data row3 col14" >16</td>
      <td id="T_721ed_row3_col15" class="data row3 col15" >16.000000</td>
      <td id="T_721ed_row3_col16" class="data row3 col16" >1.000000</td>
      <td id="T_721ed_row3_col17" class="data row3 col17" >llama.model.layers.20.self_attn.o_proj.lora_B.default.weight</td>
      <td id="T_721ed_row3_col18" class="data row3 col18" >llama.model.layers.20.self_attn.o_proj.lora_A.default.weight</td>
      <td id="T_721ed_row3_col19" class="data row3 col19" >nan</td>
    </tr>
    <tr>
      <th id="T_721ed_level0_row4" class="row_heading level0 row4" >4</th>
      <td id="T_721ed_row4_col0" class="data row4 col0" >model.layers.0.self_attn.k_proj.weight</td>
      <td id="T_721ed_row4_col1" class="data row4 col1" >model.layers.0.self_attn.k_proj.weight</td>
      <td id="T_721ed_row4_col2" class="data row4 col2" >llama.model.layers.0.self_attn.k_proj</td>
      <td id="T_721ed_row4_col3" class="data row4 col3" >6.50205</td>
      <td id="T_721ed_row4_col4" class="data row4 col4" >75.0282</td>
      <td id="T_721ed_row4_col5" class="data row4 col5" >75.2428</td>
      <td id="T_721ed_row4_col6" class="data row4 col6" >0.0866614</td>
      <td id="T_721ed_row4_col7" class="data row4 col7" >0.99626</td>
      <td id="T_721ed_row4_col8" class="data row4 col8" >0.0660154</td>
      <td id="T_721ed_row4_col9" class="data row4 col9" >7422.04</td>
      <td id="T_721ed_row4_col10" class="data row4 col10" >1</td>
      <td id="T_721ed_row4_col11" class="data row4 col11" >3145728</td>
      <td id="T_721ed_row4_col12" class="data row4 col12" >torch.float32</td>
      <td id="T_721ed_row4_col13" class="data row4 col13" >(1024, 3072)</td>
      <td id="T_721ed_row4_col14" class="data row4 col14" >16</td>
      <td id="T_721ed_row4_col15" class="data row4 col15" >16.000000</td>
      <td id="T_721ed_row4_col16" class="data row4 col16" >1.000000</td>
      <td id="T_721ed_row4_col17" class="data row4 col17" >llama.model.layers.0.self_attn.k_proj.lora_B.default.weight</td>
      <td id="T_721ed_row4_col18" class="data row4 col18" >llama.model.layers.0.self_attn.k_proj.lora_A.default.weight</td>
      <td id="T_721ed_row4_col19" class="data row4 col19" >nan</td>
    </tr>
    <tr>
      <th id="T_721ed_level0_row5" class="row_heading level0 row5" >5</th>
      <td id="T_721ed_row5_col0" class="data row5 col0" >model.layers.25.self_attn.o_proj.weight</td>
      <td id="T_721ed_row5_col1" class="data row5 col1" >model.layers.25.self_attn.o_proj.weight</td>
      <td id="T_721ed_row5_col2" class="data row5 col2" >llama.model.layers.25.self_attn.o_proj</td>
      <td id="T_721ed_row5_col3" class="data row5 col3" >6.35246</td>
      <td id="T_721ed_row5_col4" class="data row5 col4" >57.7257</td>
      <td id="T_721ed_row5_col5" class="data row5 col5" >58.0742</td>
      <td id="T_721ed_row5_col6" class="data row5 col6" >0.110046</td>
      <td id="T_721ed_row5_col7" class="data row5 col7" >0.993999</td>
      <td id="T_721ed_row5_col8" class="data row5 col8" >0.02924</td>
      <td id="T_721ed_row5_col9" class="data row5 col9" >15139</td>
      <td id="T_721ed_row5_col10" class="data row5 col10" >1</td>
      <td id="T_721ed_row5_col11" class="data row5 col11" >9437184</td>
      <td id="T_721ed_row5_col12" class="data row5 col12" >torch.float32</td>
      <td id="T_721ed_row5_col13" class="data row5 col13" >(3072, 3072)</td>
      <td id="T_721ed_row5_col14" class="data row5 col14" >16</td>
      <td id="T_721ed_row5_col15" class="data row5 col15" >16.000000</td>
      <td id="T_721ed_row5_col16" class="data row5 col16" >1.000000</td>
      <td id="T_721ed_row5_col17" class="data row5 col17" >llama.model.layers.25.self_attn.o_proj.lora_B.default.weight</td>
      <td id="T_721ed_row5_col18" class="data row5 col18" >llama.model.layers.25.self_attn.o_proj.lora_A.default.weight</td>
      <td id="T_721ed_row5_col19" class="data row5 col19" >nan</td>
    </tr>
    <tr>
      <th id="T_721ed_level0_row6" class="row_heading level0 row6" >6</th>
      <td id="T_721ed_row6_col0" class="data row6 col0" >model.layers.23.self_attn.o_proj.weight</td>
      <td id="T_721ed_row6_col1" class="data row6 col1" >model.layers.23.self_attn.o_proj.weight</td>
      <td id="T_721ed_row6_col2" class="data row6 col2" >llama.model.layers.23.self_attn.o_proj</td>
      <td id="T_721ed_row6_col3" class="data row6 col3" >6.24509</td>
      <td id="T_721ed_row6_col4" class="data row6 col4" >54.9002</td>
      <td id="T_721ed_row6_col5" class="data row6 col5" >55.2435</td>
      <td id="T_721ed_row6_col6" class="data row6 col6" >0.113754</td>
      <td id="T_721ed_row6_col7" class="data row6 col7" >0.99359</td>
      <td id="T_721ed_row6_col8" class="data row6 col8" >0.0200224</td>
      <td id="T_721ed_row6_col9" class="data row6 col9" >14747.1</td>
      <td id="T_721ed_row6_col10" class="data row6 col10" >1</td>
      <td id="T_721ed_row6_col11" class="data row6 col11" >9437184</td>
      <td id="T_721ed_row6_col12" class="data row6 col12" >torch.float32</td>
      <td id="T_721ed_row6_col13" class="data row6 col13" >(3072, 3072)</td>
      <td id="T_721ed_row6_col14" class="data row6 col14" >16</td>
      <td id="T_721ed_row6_col15" class="data row6 col15" >16.000000</td>
      <td id="T_721ed_row6_col16" class="data row6 col16" >1.000000</td>
      <td id="T_721ed_row6_col17" class="data row6 col17" >llama.model.layers.23.self_attn.o_proj.lora_B.default.weight</td>
      <td id="T_721ed_row6_col18" class="data row6 col18" >llama.model.layers.23.self_attn.o_proj.lora_A.default.weight</td>
      <td id="T_721ed_row6_col19" class="data row6 col19" >nan</td>
    </tr>
    <tr>
      <th id="T_721ed_level0_row7" class="row_heading level0 row7" >7</th>
      <td id="T_721ed_row7_col0" class="data row7 col0" >model.layers.27.self_attn.o_proj.weight</td>
      <td id="T_721ed_row7_col1" class="data row7 col1" >model.layers.27.self_attn.o_proj.weight</td>
      <td id="T_721ed_row7_col2" class="data row7 col2" >llama.model.layers.27.self_attn.o_proj</td>
      <td id="T_721ed_row7_col3" class="data row7 col3" >6.07126</td>
      <td id="T_721ed_row7_col4" class="data row7 col4" >60.6561</td>
      <td id="T_721ed_row7_col5" class="data row7 col5" >60.9597</td>
      <td id="T_721ed_row7_col6" class="data row7 col6" >0.100093</td>
      <td id="T_721ed_row7_col7" class="data row7 col7" >0.995028</td>
      <td id="T_721ed_row7_col8" class="data row7 col8" >0.0412287</td>
      <td id="T_721ed_row7_col9" class="data row7 col9" >14307</td>
      <td id="T_721ed_row7_col10" class="data row7 col10" >1</td>
      <td id="T_721ed_row7_col11" class="data row7 col11" >9437184</td>
      <td id="T_721ed_row7_col12" class="data row7 col12" >torch.float32</td>
      <td id="T_721ed_row7_col13" class="data row7 col13" >(3072, 3072)</td>
      <td id="T_721ed_row7_col14" class="data row7 col14" >16</td>
      <td id="T_721ed_row7_col15" class="data row7 col15" >16.000000</td>
      <td id="T_721ed_row7_col16" class="data row7 col16" >1.000000</td>
      <td id="T_721ed_row7_col17" class="data row7 col17" >llama.model.layers.27.self_attn.o_proj.lora_B.default.weight</td>
      <td id="T_721ed_row7_col18" class="data row7 col18" >llama.model.layers.27.self_attn.o_proj.lora_A.default.weight</td>
      <td id="T_721ed_row7_col19" class="data row7 col19" >nan</td>
    </tr>
    <tr>
      <th id="T_721ed_level0_row8" class="row_heading level0 row8" >8</th>
      <td id="T_721ed_row8_col0" class="data row8 col0" >model.layers.24.self_attn.o_proj.weight</td>
      <td id="T_721ed_row8_col1" class="data row8 col1" >model.layers.24.self_attn.o_proj.weight</td>
      <td id="T_721ed_row8_col2" class="data row8 col2" >llama.model.layers.24.self_attn.o_proj</td>
      <td id="T_721ed_row8_col3" class="data row8 col3" >6.04458</td>
      <td id="T_721ed_row8_col4" class="data row8 col4" >56.6355</td>
      <td id="T_721ed_row8_col5" class="data row8 col5" >56.9478</td>
      <td id="T_721ed_row8_col6" class="data row8 col6" >0.106728</td>
      <td id="T_721ed_row8_col7" class="data row8 col7" >0.994351</td>
      <td id="T_721ed_row8_col8" class="data row8 col8" >0.0177774</td>
      <td id="T_721ed_row8_col9" class="data row8 col9" >13998.3</td>
      <td id="T_721ed_row8_col10" class="data row8 col10" >1</td>
      <td id="T_721ed_row8_col11" class="data row8 col11" >9437184</td>
      <td id="T_721ed_row8_col12" class="data row8 col12" >torch.float32</td>
      <td id="T_721ed_row8_col13" class="data row8 col13" >(3072, 3072)</td>
      <td id="T_721ed_row8_col14" class="data row8 col14" >16</td>
      <td id="T_721ed_row8_col15" class="data row8 col15" >16.000000</td>
      <td id="T_721ed_row8_col16" class="data row8 col16" >1.000000</td>
      <td id="T_721ed_row8_col17" class="data row8 col17" >llama.model.layers.24.self_attn.o_proj.lora_B.default.weight</td>
      <td id="T_721ed_row8_col18" class="data row8 col18" >llama.model.layers.24.self_attn.o_proj.lora_A.default.weight</td>
      <td id="T_721ed_row8_col19" class="data row8 col19" >nan</td>
    </tr>
    <tr>
      <th id="T_721ed_level0_row9" class="row_heading level0 row9" >9</th>
      <td id="T_721ed_row9_col0" class="data row9 col0" >model.layers.26.self_attn.o_proj.weight</td>
      <td id="T_721ed_row9_col1" class="data row9 col1" >model.layers.26.self_attn.o_proj.weight</td>
      <td id="T_721ed_row9_col2" class="data row9 col2" >llama.model.layers.26.self_attn.o_proj</td>
      <td id="T_721ed_row9_col3" class="data row9 col3" >6.04437</td>
      <td id="T_721ed_row9_col4" class="data row9 col4" >62.0134</td>
      <td id="T_721ed_row9_col5" class="data row9 col5" >62.3028</td>
      <td id="T_721ed_row9_col6" class="data row9 col6" >0.0974688</td>
      <td id="T_721ed_row9_col7" class="data row9 col7" >0.995283</td>
      <td id="T_721ed_row9_col8" class="data row9 col8" >0.0207815</td>
      <td id="T_721ed_row9_col9" class="data row9 col9" >14304.3</td>
      <td id="T_721ed_row9_col10" class="data row9 col10" >1</td>
      <td id="T_721ed_row9_col11" class="data row9 col11" >9437184</td>
      <td id="T_721ed_row9_col12" class="data row9 col12" >torch.float32</td>
      <td id="T_721ed_row9_col13" class="data row9 col13" >(3072, 3072)</td>
      <td id="T_721ed_row9_col14" class="data row9 col14" >16</td>
      <td id="T_721ed_row9_col15" class="data row9 col15" >16.000000</td>
      <td id="T_721ed_row9_col16" class="data row9 col16" >1.000000</td>
      <td id="T_721ed_row9_col17" class="data row9 col17" >llama.model.layers.26.self_attn.o_proj.lora_B.default.weight</td>
      <td id="T_721ed_row9_col18" class="data row9 col18" >llama.model.layers.26.self_attn.o_proj.lora_A.default.weight</td>
      <td id="T_721ed_row9_col19" class="data row9 col19" >nan</td>
    </tr>
    <tr>
      <th id="T_721ed_level0_row10" class="row_heading level0 row10" >10</th>
      <td id="T_721ed_row10_col0" class="data row10 col0" >model.layers.21.self_attn.o_proj.weight</td>
      <td id="T_721ed_row10_col1" class="data row10 col1" >model.layers.21.self_attn.o_proj.weight</td>
      <td id="T_721ed_row10_col2" class="data row10 col2" >llama.model.layers.21.self_attn.o_proj</td>
      <td id="T_721ed_row10_col3" class="data row10 col3" >6.00952</td>
      <td id="T_721ed_row10_col4" class="data row10 col4" >54.2694</td>
      <td id="T_721ed_row10_col5" class="data row10 col5" >54.5942</td>
      <td id="T_721ed_row10_col6" class="data row10 col6" >0.110735</td>
      <td id="T_721ed_row10_col7" class="data row10 col7" >0.993923</td>
      <td id="T_721ed_row10_col8" class="data row10 col8" >0.0228605</td>
      <td id="T_721ed_row10_col9" class="data row10 col9" >14074.7</td>
      <td id="T_721ed_row10_col10" class="data row10 col10" >1</td>
      <td id="T_721ed_row10_col11" class="data row10 col11" >9437184</td>
      <td id="T_721ed_row10_col12" class="data row10 col12" >torch.float32</td>
      <td id="T_721ed_row10_col13" class="data row10 col13" >(3072, 3072)</td>
      <td id="T_721ed_row10_col14" class="data row10 col14" >16</td>
      <td id="T_721ed_row10_col15" class="data row10 col15" >16.000000</td>
      <td id="T_721ed_row10_col16" class="data row10 col16" >1.000000</td>
      <td id="T_721ed_row10_col17" class="data row10 col17" >llama.model.layers.21.self_attn.o_proj.lora_B.default.weight</td>
      <td id="T_721ed_row10_col18" class="data row10 col18" >llama.model.layers.21.self_attn.o_proj.lora_A.default.weight</td>
      <td id="T_721ed_row10_col19" class="data row10 col19" >nan</td>
    </tr>
    <tr>
      <th id="T_721ed_level0_row11" class="row_heading level0 row11" >11</th>
      <td id="T_721ed_row11_col0" class="data row11 col0" >model.layers.27.self_attn.q_proj.weight</td>
      <td id="T_721ed_row11_col1" class="data row11 col1" >model.layers.27.self_attn.q_proj.weight</td>
      <td id="T_721ed_row11_col2" class="data row11 col2" >llama.model.layers.27.self_attn.q_proj</td>
      <td id="T_721ed_row11_col3" class="data row11 col3" >5.82301</td>
      <td id="T_721ed_row11_col4" class="data row11 col4" >66.7859</td>
      <td id="T_721ed_row11_col5" class="data row11 col5" >67.0331</td>
      <td id="T_721ed_row11_col6" class="data row11 col6" >0.0871892</td>
      <td id="T_721ed_row11_col7" class="data row11 col7" >0.99622</td>
      <td id="T_721ed_row11_col8" class="data row11 col8" >0.0214047</td>
      <td id="T_721ed_row11_col9" class="data row11 col9" >13208.2</td>
      <td id="T_721ed_row11_col10" class="data row11 col10" >1</td>
      <td id="T_721ed_row11_col11" class="data row11 col11" >9437184</td>
      <td id="T_721ed_row11_col12" class="data row11 col12" >torch.float32</td>
      <td id="T_721ed_row11_col13" class="data row11 col13" >(3072, 3072)</td>
      <td id="T_721ed_row11_col14" class="data row11 col14" >16</td>
      <td id="T_721ed_row11_col15" class="data row11 col15" >16.000000</td>
      <td id="T_721ed_row11_col16" class="data row11 col16" >1.000000</td>
      <td id="T_721ed_row11_col17" class="data row11 col17" >llama.model.layers.27.self_attn.q_proj.lora_B.default.weight</td>
      <td id="T_721ed_row11_col18" class="data row11 col18" >llama.model.layers.27.self_attn.q_proj.lora_A.default.weight</td>
      <td id="T_721ed_row11_col19" class="data row11 col19" >nan</td>
    </tr>
    <tr>
      <th id="T_721ed_level0_row12" class="row_heading level0 row12" >12</th>
      <td id="T_721ed_row12_col0" class="data row12 col0" >model.layers.21.self_attn.q_proj.weight</td>
      <td id="T_721ed_row12_col1" class="data row12 col1" >model.layers.21.self_attn.q_proj.weight</td>
      <td id="T_721ed_row12_col2" class="data row12 col2" >llama.model.layers.21.self_attn.q_proj</td>
      <td id="T_721ed_row12_col3" class="data row12 col3" >5.75917</td>
      <td id="T_721ed_row12_col4" class="data row12 col4" >71.7273</td>
      <td id="T_721ed_row12_col5" class="data row12 col5" >71.9565</td>
      <td id="T_721ed_row12_col6" class="data row12 col6" >0.0802926</td>
      <td id="T_721ed_row12_col7" class="data row12 col7" >0.996792</td>
      <td id="T_721ed_row12_col8" class="data row12 col8" >0.0224375</td>
      <td id="T_721ed_row12_col9" class="data row12 col9" >13098.5</td>
      <td id="T_721ed_row12_col10" class="data row12 col10" >1</td>
      <td id="T_721ed_row12_col11" class="data row12 col11" >9437184</td>
      <td id="T_721ed_row12_col12" class="data row12 col12" >torch.float32</td>
      <td id="T_721ed_row12_col13" class="data row12 col13" >(3072, 3072)</td>
      <td id="T_721ed_row12_col14" class="data row12 col14" >16</td>
      <td id="T_721ed_row12_col15" class="data row12 col15" >16.000000</td>
      <td id="T_721ed_row12_col16" class="data row12 col16" >1.000000</td>
      <td id="T_721ed_row12_col17" class="data row12 col17" >llama.model.layers.21.self_attn.q_proj.lora_B.default.weight</td>
      <td id="T_721ed_row12_col18" class="data row12 col18" >llama.model.layers.21.self_attn.q_proj.lora_A.default.weight</td>
      <td id="T_721ed_row12_col19" class="data row12 col19" >nan</td>
    </tr>
    <tr>
      <th id="T_721ed_level0_row13" class="row_heading level0 row13" >13</th>
      <td id="T_721ed_row13_col0" class="data row13 col0" >model.layers.26.self_attn.q_proj.weight</td>
      <td id="T_721ed_row13_col1" class="data row13 col1" >model.layers.26.self_attn.q_proj.weight</td>
      <td id="T_721ed_row13_col2" class="data row13 col2" >llama.model.layers.26.self_attn.q_proj</td>
      <td id="T_721ed_row13_col3" class="data row13 col3" >5.71907</td>
      <td id="T_721ed_row13_col4" class="data row13 col4" >66.346</td>
      <td id="T_721ed_row13_col5" class="data row13 col5" >66.5762</td>
      <td id="T_721ed_row13_col6" class="data row13 col6" >0.0862008</td>
      <td id="T_721ed_row13_col7" class="data row13 col7" >0.996304</td>
      <td id="T_721ed_row13_col8" class="data row13 col8" >0.0198059</td>
      <td id="T_721ed_row13_col9" class="data row13 col9" >13129.9</td>
      <td id="T_721ed_row13_col10" class="data row13 col10" >1</td>
      <td id="T_721ed_row13_col11" class="data row13 col11" >9437184</td>
      <td id="T_721ed_row13_col12" class="data row13 col12" >torch.float32</td>
      <td id="T_721ed_row13_col13" class="data row13 col13" >(3072, 3072)</td>
      <td id="T_721ed_row13_col14" class="data row13 col14" >16</td>
      <td id="T_721ed_row13_col15" class="data row13 col15" >16.000000</td>
      <td id="T_721ed_row13_col16" class="data row13 col16" >1.000000</td>
      <td id="T_721ed_row13_col17" class="data row13 col17" >llama.model.layers.26.self_attn.q_proj.lora_B.default.weight</td>
      <td id="T_721ed_row13_col18" class="data row13 col18" >llama.model.layers.26.self_attn.q_proj.lora_A.default.weight</td>
      <td id="T_721ed_row13_col19" class="data row13 col19" >nan</td>
    </tr>
    <tr>
      <th id="T_721ed_level0_row14" class="row_heading level0 row14" >14</th>
      <td id="T_721ed_row14_col0" class="data row14 col0" >model.layers.23.self_attn.q_proj.weight</td>
      <td id="T_721ed_row14_col1" class="data row14 col1" >model.layers.23.self_attn.q_proj.weight</td>
      <td id="T_721ed_row14_col2" class="data row14 col2" >llama.model.layers.23.self_attn.q_proj</td>
      <td id="T_721ed_row14_col3" class="data row14 col3" >5.68104</td>
      <td id="T_721ed_row14_col4" class="data row14 col4" >71.0603</td>
      <td id="T_721ed_row14_col5" class="data row14 col5" >71.2843</td>
      <td id="T_721ed_row14_col6" class="data row14 col6" >0.0799467</td>
      <td id="T_721ed_row14_col7" class="data row14 col7" >0.996819</td>
      <td id="T_721ed_row14_col8" class="data row14 col8" >0.0238056</td>
      <td id="T_721ed_row14_col9" class="data row14 col9" >12889.5</td>
      <td id="T_721ed_row14_col10" class="data row14 col10" >1</td>
      <td id="T_721ed_row14_col11" class="data row14 col11" >9437184</td>
      <td id="T_721ed_row14_col12" class="data row14 col12" >torch.float32</td>
      <td id="T_721ed_row14_col13" class="data row14 col13" >(3072, 3072)</td>
      <td id="T_721ed_row14_col14" class="data row14 col14" >16</td>
      <td id="T_721ed_row14_col15" class="data row14 col15" >16.000000</td>
      <td id="T_721ed_row14_col16" class="data row14 col16" >1.000000</td>
      <td id="T_721ed_row14_col17" class="data row14 col17" >llama.model.layers.23.self_attn.q_proj.lora_B.default.weight</td>
      <td id="T_721ed_row14_col18" class="data row14 col18" >llama.model.layers.23.self_attn.q_proj.lora_A.default.weight</td>
      <td id="T_721ed_row14_col19" class="data row14 col19" >nan</td>
    </tr>
    <tr>
      <th id="T_721ed_level0_row15" class="row_heading level0 row15" >15</th>
      <td id="T_721ed_row15_col0" class="data row15 col0" >model.layers.24.self_attn.q_proj.weight</td>
      <td id="T_721ed_row15_col1" class="data row15 col1" >model.layers.24.self_attn.q_proj.weight</td>
      <td id="T_721ed_row15_col2" class="data row15 col2" >llama.model.layers.24.self_attn.q_proj</td>
      <td id="T_721ed_row15_col3" class="data row15 col3" >5.67503</td>
      <td id="T_721ed_row15_col4" class="data row15 col4" >69.3472</td>
      <td id="T_721ed_row15_col5" class="data row15 col5" >69.5782</td>
      <td id="T_721ed_row15_col6" class="data row15 col6" >0.0818351</td>
      <td id="T_721ed_row15_col7" class="data row15 col7" >0.996668</td>
      <td id="T_721ed_row15_col8" class="data row15 col8" >0.0211074</td>
      <td id="T_721ed_row15_col9" class="data row15 col9" >13075.7</td>
      <td id="T_721ed_row15_col10" class="data row15 col10" >1</td>
      <td id="T_721ed_row15_col11" class="data row15 col11" >9437184</td>
      <td id="T_721ed_row15_col12" class="data row15 col12" >torch.float32</td>
      <td id="T_721ed_row15_col13" class="data row15 col13" >(3072, 3072)</td>
      <td id="T_721ed_row15_col14" class="data row15 col14" >16</td>
      <td id="T_721ed_row15_col15" class="data row15 col15" >16.000000</td>
      <td id="T_721ed_row15_col16" class="data row15 col16" >1.000000</td>
      <td id="T_721ed_row15_col17" class="data row15 col17" >llama.model.layers.24.self_attn.q_proj.lora_B.default.weight</td>
      <td id="T_721ed_row15_col18" class="data row15 col18" >llama.model.layers.24.self_attn.q_proj.lora_A.default.weight</td>
      <td id="T_721ed_row15_col19" class="data row15 col19" >nan</td>
    </tr>
    <tr>
      <th id="T_721ed_level0_row16" class="row_heading level0 row16" >16</th>
      <td id="T_721ed_row16_col0" class="data row16 col0" >model.layers.25.self_attn.q_proj.weight</td>
      <td id="T_721ed_row16_col1" class="data row16 col1" >model.layers.25.self_attn.q_proj.weight</td>
      <td id="T_721ed_row16_col2" class="data row16 col2" >llama.model.layers.25.self_attn.q_proj</td>
      <td id="T_721ed_row16_col3" class="data row16 col3" >5.65979</td>
      <td id="T_721ed_row16_col4" class="data row16 col4" >68.9477</td>
      <td id="T_721ed_row16_col5" class="data row16 col5" >69.177</td>
      <td id="T_721ed_row16_col6" class="data row16 col6" >0.0820882</td>
      <td id="T_721ed_row16_col7" class="data row16 col7" >0.996647</td>
      <td id="T_721ed_row16_col8" class="data row16 col8" >0.0188447</td>
      <td id="T_721ed_row16_col9" class="data row16 col9" >13060.6</td>
      <td id="T_721ed_row16_col10" class="data row16 col10" >1</td>
      <td id="T_721ed_row16_col11" class="data row16 col11" >9437184</td>
      <td id="T_721ed_row16_col12" class="data row16 col12" >torch.float32</td>
      <td id="T_721ed_row16_col13" class="data row16 col13" >(3072, 3072)</td>
      <td id="T_721ed_row16_col14" class="data row16 col14" >16</td>
      <td id="T_721ed_row16_col15" class="data row16 col15" >16.000000</td>
      <td id="T_721ed_row16_col16" class="data row16 col16" >1.000000</td>
      <td id="T_721ed_row16_col17" class="data row16 col17" >llama.model.layers.25.self_attn.q_proj.lora_B.default.weight</td>
      <td id="T_721ed_row16_col18" class="data row16 col18" >llama.model.layers.25.self_attn.q_proj.lora_A.default.weight</td>
      <td id="T_721ed_row16_col19" class="data row16 col19" >nan</td>
    </tr>
    <tr>
      <th id="T_721ed_level0_row17" class="row_heading level0 row17" >17</th>
      <td id="T_721ed_row17_col0" class="data row17 col0" >model.layers.15.self_attn.q_proj.weight</td>
      <td id="T_721ed_row17_col1" class="data row17 col1" >model.layers.15.self_attn.q_proj.weight</td>
      <td id="T_721ed_row17_col2" class="data row17 col2" >llama.model.layers.15.self_attn.q_proj</td>
      <td id="T_721ed_row17_col3" class="data row17 col3" >5.64312</td>
      <td id="T_721ed_row17_col4" class="data row17 col4" >77.2229</td>
      <td id="T_721ed_row17_col5" class="data row17 col5" >77.4309</td>
      <td id="T_721ed_row17_col6" class="data row17 col6" >0.0730757</td>
      <td id="T_721ed_row17_col7" class="data row17 col7" >0.997341</td>
      <td id="T_721ed_row17_col8" class="data row17 col8" >0.0246535</td>
      <td id="T_721ed_row17_col9" class="data row17 col9" >12761.9</td>
      <td id="T_721ed_row17_col10" class="data row17 col10" >1</td>
      <td id="T_721ed_row17_col11" class="data row17 col11" >9437184</td>
      <td id="T_721ed_row17_col12" class="data row17 col12" >torch.float32</td>
      <td id="T_721ed_row17_col13" class="data row17 col13" >(3072, 3072)</td>
      <td id="T_721ed_row17_col14" class="data row17 col14" >16</td>
      <td id="T_721ed_row17_col15" class="data row17 col15" >16.000000</td>
      <td id="T_721ed_row17_col16" class="data row17 col16" >1.000000</td>
      <td id="T_721ed_row17_col17" class="data row17 col17" >llama.model.layers.15.self_attn.q_proj.lora_B.default.weight</td>
      <td id="T_721ed_row17_col18" class="data row17 col18" >llama.model.layers.15.self_attn.q_proj.lora_A.default.weight</td>
      <td id="T_721ed_row17_col19" class="data row17 col19" >nan</td>
    </tr>
    <tr>
      <th id="T_721ed_level0_row18" class="row_heading level0 row18" >18</th>
      <td id="T_721ed_row18_col0" class="data row18 col0" >model.layers.22.self_attn.q_proj.weight</td>
      <td id="T_721ed_row18_col1" class="data row18 col1" >model.layers.22.self_attn.q_proj.weight</td>
      <td id="T_721ed_row18_col2" class="data row18 col2" >llama.model.layers.22.self_attn.q_proj</td>
      <td id="T_721ed_row18_col3" class="data row18 col3" >5.51497</td>
      <td id="T_721ed_row18_col4" class="data row18 col4" >71.0742</td>
      <td id="T_721ed_row18_col5" class="data row18 col5" >71.2837</td>
      <td id="T_721ed_row18_col6" class="data row18 col6" >0.0775945</td>
      <td id="T_721ed_row18_col7" class="data row18 col7" >0.997003</td>
      <td id="T_721ed_row18_col8" class="data row18 col8" >0.0239242</td>
      <td id="T_721ed_row18_col9" class="data row18 col9" >12483.3</td>
      <td id="T_721ed_row18_col10" class="data row18 col10" >1</td>
      <td id="T_721ed_row18_col11" class="data row18 col11" >9437184</td>
      <td id="T_721ed_row18_col12" class="data row18 col12" >torch.float32</td>
      <td id="T_721ed_row18_col13" class="data row18 col13" >(3072, 3072)</td>
      <td id="T_721ed_row18_col14" class="data row18 col14" >16</td>
      <td id="T_721ed_row18_col15" class="data row18 col15" >16.000000</td>
      <td id="T_721ed_row18_col16" class="data row18 col16" >1.000000</td>
      <td id="T_721ed_row18_col17" class="data row18 col17" >llama.model.layers.22.self_attn.q_proj.lora_B.default.weight</td>
      <td id="T_721ed_row18_col18" class="data row18 col18" >llama.model.layers.22.self_attn.q_proj.lora_A.default.weight</td>
      <td id="T_721ed_row18_col19" class="data row18 col19" >nan</td>
    </tr>
    <tr>
      <th id="T_721ed_level0_row19" class="row_heading level0 row19" >19</th>
      <td id="T_721ed_row19_col0" class="data row19 col0" >model.layers.18.self_attn.o_proj.weight</td>
      <td id="T_721ed_row19_col1" class="data row19 col1" >model.layers.18.self_attn.o_proj.weight</td>
      <td id="T_721ed_row19_col2" class="data row19 col2" >llama.model.layers.18.self_attn.o_proj</td>
      <td id="T_721ed_row19_col3" class="data row19 col3" >5.50066</td>
      <td id="T_721ed_row19_col4" class="data row19 col4" >47.9476</td>
      <td id="T_721ed_row19_col5" class="data row19 col5" >48.2548</td>
      <td id="T_721ed_row19_col6" class="data row19 col6" >0.114722</td>
      <td id="T_721ed_row19_col7" class="data row19 col7" >0.993482</td>
      <td id="T_721ed_row19_col8" class="data row19 col8" >0.0157195</td>
      <td id="T_721ed_row19_col9" class="data row19 col9" >12933.1</td>
      <td id="T_721ed_row19_col10" class="data row19 col10" >1</td>
      <td id="T_721ed_row19_col11" class="data row19 col11" >9437184</td>
      <td id="T_721ed_row19_col12" class="data row19 col12" >torch.float32</td>
      <td id="T_721ed_row19_col13" class="data row19 col13" >(3072, 3072)</td>
      <td id="T_721ed_row19_col14" class="data row19 col14" >16</td>
      <td id="T_721ed_row19_col15" class="data row19 col15" >16.000000</td>
      <td id="T_721ed_row19_col16" class="data row19 col16" >1.000000</td>
      <td id="T_721ed_row19_col17" class="data row19 col17" >llama.model.layers.18.self_attn.o_proj.lora_B.default.weight</td>
      <td id="T_721ed_row19_col18" class="data row19 col18" >llama.model.layers.18.self_attn.o_proj.lora_A.default.weight</td>
      <td id="T_721ed_row19_col19" class="data row19 col19" >nan</td>
    </tr>
    <tr>
      <th id="T_721ed_level0_row20" class="row_heading level0 row20" >20</th>
      <td id="T_721ed_row20_col0" class="data row20 col0" >model.layers.14.self_attn.q_proj.weight</td>
      <td id="T_721ed_row20_col1" class="data row20 col1" >model.layers.14.self_attn.q_proj.weight</td>
      <td id="T_721ed_row20_col2" class="data row20 col2" >llama.model.layers.14.self_attn.q_proj</td>
      <td id="T_721ed_row20_col3" class="data row20 col3" >5.4862</td>
      <td id="T_721ed_row20_col4" class="data row20 col4" >78.9882</td>
      <td id="T_721ed_row20_col5" class="data row20 col5" >79.1802</td>
      <td id="T_721ed_row20_col6" class="data row20 col6" >0.069456</td>
      <td id="T_721ed_row20_col7" class="data row20 col7" >0.997597</td>
      <td id="T_721ed_row20_col8" class="data row20 col8" >0.0227773</td>
      <td id="T_721ed_row20_col9" class="data row20 col9" >12245.4</td>
      <td id="T_721ed_row20_col10" class="data row20 col10" >1</td>
      <td id="T_721ed_row20_col11" class="data row20 col11" >9437184</td>
      <td id="T_721ed_row20_col12" class="data row20 col12" >torch.float32</td>
      <td id="T_721ed_row20_col13" class="data row20 col13" >(3072, 3072)</td>
      <td id="T_721ed_row20_col14" class="data row20 col14" >16</td>
      <td id="T_721ed_row20_col15" class="data row20 col15" >16.000000</td>
      <td id="T_721ed_row20_col16" class="data row20 col16" >1.000000</td>
      <td id="T_721ed_row20_col17" class="data row20 col17" >llama.model.layers.14.self_attn.q_proj.lora_B.default.weight</td>
      <td id="T_721ed_row20_col18" class="data row20 col18" >llama.model.layers.14.self_attn.q_proj.lora_A.default.weight</td>
      <td id="T_721ed_row20_col19" class="data row20 col19" >nan</td>
    </tr>
    <tr>
      <th id="T_721ed_level0_row21" class="row_heading level0 row21" >21</th>
      <td id="T_721ed_row21_col0" class="data row21 col0" >model.layers.18.self_attn.q_proj.weight</td>
      <td id="T_721ed_row21_col1" class="data row21 col1" >model.layers.18.self_attn.q_proj.weight</td>
      <td id="T_721ed_row21_col2" class="data row21 col2" >llama.model.layers.18.self_attn.q_proj</td>
      <td id="T_721ed_row21_col3" class="data row21 col3" >5.44918</td>
      <td id="T_721ed_row21_col4" class="data row21 col4" >75.4274</td>
      <td id="T_721ed_row21_col5" class="data row21 col5" >75.6251</td>
      <td id="T_721ed_row21_col6" class="data row21 col6" >0.072244</td>
      <td id="T_721ed_row21_col7" class="data row21 col7" >0.997401</td>
      <td id="T_721ed_row21_col8" class="data row21 col8" >0.0257619</td>
      <td id="T_721ed_row21_col9" class="data row21 col9" >12327.7</td>
      <td id="T_721ed_row21_col10" class="data row21 col10" >1</td>
      <td id="T_721ed_row21_col11" class="data row21 col11" >9437184</td>
      <td id="T_721ed_row21_col12" class="data row21 col12" >torch.float32</td>
      <td id="T_721ed_row21_col13" class="data row21 col13" >(3072, 3072)</td>
      <td id="T_721ed_row21_col14" class="data row21 col14" >16</td>
      <td id="T_721ed_row21_col15" class="data row21 col15" >16.000000</td>
      <td id="T_721ed_row21_col16" class="data row21 col16" >1.000000</td>
      <td id="T_721ed_row21_col17" class="data row21 col17" >llama.model.layers.18.self_attn.q_proj.lora_B.default.weight</td>
      <td id="T_721ed_row21_col18" class="data row21 col18" >llama.model.layers.18.self_attn.q_proj.lora_A.default.weight</td>
      <td id="T_721ed_row21_col19" class="data row21 col19" >nan</td>
    </tr>
    <tr>
      <th id="T_721ed_level0_row22" class="row_heading level0 row22" >22</th>
      <td id="T_721ed_row22_col0" class="data row22 col0" >model.layers.19.self_attn.q_proj.weight</td>
      <td id="T_721ed_row22_col1" class="data row22 col1" >model.layers.19.self_attn.q_proj.weight</td>
      <td id="T_721ed_row22_col2" class="data row22 col2" >llama.model.layers.19.self_attn.q_proj</td>
      <td id="T_721ed_row22_col3" class="data row22 col3" >5.39254</td>
      <td id="T_721ed_row22_col4" class="data row22 col4" >73.7667</td>
      <td id="T_721ed_row22_col5" class="data row22 col5" >73.9655</td>
      <td id="T_721ed_row22_col6" class="data row22 col6" >0.0731026</td>
      <td id="T_721ed_row22_col7" class="data row22 col7" >0.997339</td>
      <td id="T_721ed_row22_col8" class="data row22 col8" >0.0290255</td>
      <td id="T_721ed_row22_col9" class="data row22 col9" >12209.6</td>
      <td id="T_721ed_row22_col10" class="data row22 col10" >1</td>
      <td id="T_721ed_row22_col11" class="data row22 col11" >9437184</td>
      <td id="T_721ed_row22_col12" class="data row22 col12" >torch.float32</td>
      <td id="T_721ed_row22_col13" class="data row22 col13" >(3072, 3072)</td>
      <td id="T_721ed_row22_col14" class="data row22 col14" >16</td>
      <td id="T_721ed_row22_col15" class="data row22 col15" >16.000000</td>
      <td id="T_721ed_row22_col16" class="data row22 col16" >1.000000</td>
      <td id="T_721ed_row22_col17" class="data row22 col17" >llama.model.layers.19.self_attn.q_proj.lora_B.default.weight</td>
      <td id="T_721ed_row22_col18" class="data row22 col18" >llama.model.layers.19.self_attn.q_proj.lora_A.default.weight</td>
      <td id="T_721ed_row22_col19" class="data row22 col19" >nan</td>
    </tr>
    <tr>
      <th id="T_721ed_level0_row23" class="row_heading level0 row23" >23</th>
      <td id="T_721ed_row23_col0" class="data row23 col0" >model.layers.17.self_attn.q_proj.weight</td>
      <td id="T_721ed_row23_col1" class="data row23 col1" >model.layers.17.self_attn.q_proj.weight</td>
      <td id="T_721ed_row23_col2" class="data row23 col2" >llama.model.layers.17.self_attn.q_proj</td>
      <td id="T_721ed_row23_col3" class="data row23 col3" >5.38507</td>
      <td id="T_721ed_row23_col4" class="data row23 col4" >75.9828</td>
      <td id="T_721ed_row23_col5" class="data row23 col5" >76.1744</td>
      <td id="T_721ed_row23_col6" class="data row23 col6" >0.0708721</td>
      <td id="T_721ed_row23_col7" class="data row23 col7" >0.997498</td>
      <td id="T_721ed_row23_col8" class="data row23 col8" >0.0201994</td>
      <td id="T_721ed_row23_col9" class="data row23 col9" >12162.2</td>
      <td id="T_721ed_row23_col10" class="data row23 col10" >1</td>
      <td id="T_721ed_row23_col11" class="data row23 col11" >9437184</td>
      <td id="T_721ed_row23_col12" class="data row23 col12" >torch.float32</td>
      <td id="T_721ed_row23_col13" class="data row23 col13" >(3072, 3072)</td>
      <td id="T_721ed_row23_col14" class="data row23 col14" >16</td>
      <td id="T_721ed_row23_col15" class="data row23 col15" >16.000000</td>
      <td id="T_721ed_row23_col16" class="data row23 col16" >1.000000</td>
      <td id="T_721ed_row23_col17" class="data row23 col17" >llama.model.layers.17.self_attn.q_proj.lora_B.default.weight</td>
      <td id="T_721ed_row23_col18" class="data row23 col18" >llama.model.layers.17.self_attn.q_proj.lora_A.default.weight</td>
      <td id="T_721ed_row23_col19" class="data row23 col19" >nan</td>
    </tr>
    <tr>
      <th id="T_721ed_level0_row24" class="row_heading level0 row24" >24</th>
      <td id="T_721ed_row24_col0" class="data row24 col0" >model.layers.19.self_attn.o_proj.weight</td>
      <td id="T_721ed_row24_col1" class="data row24 col1" >model.layers.19.self_attn.o_proj.weight</td>
      <td id="T_721ed_row24_col2" class="data row24 col2" >llama.model.layers.19.self_attn.o_proj</td>
      <td id="T_721ed_row24_col3" class="data row24 col3" >5.34858</td>
      <td id="T_721ed_row24_col4" class="data row24 col4" >49.6809</td>
      <td id="T_721ed_row24_col5" class="data row24 col5" >49.9605</td>
      <td id="T_721ed_row24_col6" class="data row24 col6" >0.107659</td>
      <td id="T_721ed_row24_col7" class="data row24 col7" >0.994253</td>
      <td id="T_721ed_row24_col8" class="data row24 col8" >0.0411174</td>
      <td id="T_721ed_row24_col9" class="data row24 col9" >12659.2</td>
      <td id="T_721ed_row24_col10" class="data row24 col10" >1</td>
      <td id="T_721ed_row24_col11" class="data row24 col11" >9437184</td>
      <td id="T_721ed_row24_col12" class="data row24 col12" >torch.float32</td>
      <td id="T_721ed_row24_col13" class="data row24 col13" >(3072, 3072)</td>
      <td id="T_721ed_row24_col14" class="data row24 col14" >16</td>
      <td id="T_721ed_row24_col15" class="data row24 col15" >16.000000</td>
      <td id="T_721ed_row24_col16" class="data row24 col16" >1.000000</td>
      <td id="T_721ed_row24_col17" class="data row24 col17" >llama.model.layers.19.self_attn.o_proj.lora_B.default.weight</td>
      <td id="T_721ed_row24_col18" class="data row24 col18" >llama.model.layers.19.self_attn.o_proj.lora_A.default.weight</td>
      <td id="T_721ed_row24_col19" class="data row24 col19" >nan</td>
    </tr>
    <tr>
      <th id="T_721ed_level0_row25" class="row_heading level0 row25" >25</th>
      <td id="T_721ed_row25_col0" class="data row25 col0" >model.layers.20.self_attn.q_proj.weight</td>
      <td id="T_721ed_row25_col1" class="data row25 col1" >model.layers.20.self_attn.q_proj.weight</td>
      <td id="T_721ed_row25_col2" class="data row25 col2" >llama.model.layers.20.self_attn.q_proj</td>
      <td id="T_721ed_row25_col3" class="data row25 col3" >5.32228</td>
      <td id="T_721ed_row25_col4" class="data row25 col4" >73.6294</td>
      <td id="T_721ed_row25_col5" class="data row25 col5" >73.8244</td>
      <td id="T_721ed_row25_col6" class="data row25 col6" >0.0722847</td>
      <td id="T_721ed_row25_col7" class="data row25 col7" >0.997398</td>
      <td id="T_721ed_row25_col8" class="data row25 col8" >0.0198861</td>
      <td id="T_721ed_row25_col9" class="data row25 col9" >12242.4</td>
      <td id="T_721ed_row25_col10" class="data row25 col10" >1</td>
      <td id="T_721ed_row25_col11" class="data row25 col11" >9437184</td>
      <td id="T_721ed_row25_col12" class="data row25 col12" >torch.float32</td>
      <td id="T_721ed_row25_col13" class="data row25 col13" >(3072, 3072)</td>
      <td id="T_721ed_row25_col14" class="data row25 col14" >16</td>
      <td id="T_721ed_row25_col15" class="data row25 col15" >16.000000</td>
      <td id="T_721ed_row25_col16" class="data row25 col16" >1.000000</td>
      <td id="T_721ed_row25_col17" class="data row25 col17" >llama.model.layers.20.self_attn.q_proj.lora_B.default.weight</td>
      <td id="T_721ed_row25_col18" class="data row25 col18" >llama.model.layers.20.self_attn.q_proj.lora_A.default.weight</td>
      <td id="T_721ed_row25_col19" class="data row25 col19" >nan</td>
    </tr>
    <tr>
      <th id="T_721ed_level0_row26" class="row_heading level0 row26" >26</th>
      <td id="T_721ed_row26_col0" class="data row26 col0" >model.layers.17.self_attn.o_proj.weight</td>
      <td id="T_721ed_row26_col1" class="data row26 col1" >model.layers.17.self_attn.o_proj.weight</td>
      <td id="T_721ed_row26_col2" class="data row26 col2" >llama.model.layers.17.self_attn.o_proj</td>
      <td id="T_721ed_row26_col3" class="data row26 col3" >5.1722</td>
      <td id="T_721ed_row26_col4" class="data row26 col4" >48.765</td>
      <td id="T_721ed_row26_col5" class="data row26 col5" >49.032</td>
      <td id="T_721ed_row26_col6" class="data row26 col6" >0.106064</td>
      <td id="T_721ed_row26_col7" class="data row26 col7" >0.994421</td>
      <td id="T_721ed_row26_col8" class="data row26 col8" >0.0138814</td>
      <td id="T_721ed_row26_col9" class="data row26 col9" >12261.9</td>
      <td id="T_721ed_row26_col10" class="data row26 col10" >1</td>
      <td id="T_721ed_row26_col11" class="data row26 col11" >9437184</td>
      <td id="T_721ed_row26_col12" class="data row26 col12" >torch.float32</td>
      <td id="T_721ed_row26_col13" class="data row26 col13" >(3072, 3072)</td>
      <td id="T_721ed_row26_col14" class="data row26 col14" >16</td>
      <td id="T_721ed_row26_col15" class="data row26 col15" >16.000000</td>
      <td id="T_721ed_row26_col16" class="data row26 col16" >1.000000</td>
      <td id="T_721ed_row26_col17" class="data row26 col17" >llama.model.layers.17.self_attn.o_proj.lora_B.default.weight</td>
      <td id="T_721ed_row26_col18" class="data row26 col18" >llama.model.layers.17.self_attn.o_proj.lora_A.default.weight</td>
      <td id="T_721ed_row26_col19" class="data row26 col19" >nan</td>
    </tr>
    <tr>
      <th id="T_721ed_level0_row27" class="row_heading level0 row27" >27</th>
      <td id="T_721ed_row27_col0" class="data row27 col0" >model.layers.3.self_attn.q_proj.weight</td>
      <td id="T_721ed_row27_col1" class="data row27 col1" >model.layers.3.self_attn.q_proj.weight</td>
      <td id="T_721ed_row27_col2" class="data row27 col2" >llama.model.layers.3.self_attn.q_proj</td>
      <td id="T_721ed_row27_col3" class="data row27 col3" >5.16772</td>
      <td id="T_721ed_row27_col4" class="data row27 col4" >77.7332</td>
      <td id="T_721ed_row27_col5" class="data row27 col5" >77.9139</td>
      <td id="T_721ed_row27_col6" class="data row27 col6" >0.0664801</td>
      <td id="T_721ed_row27_col7" class="data row27 col7" >0.997798</td>
      <td id="T_721ed_row27_col8" class="data row27 col8" >0.0191333</td>
      <td id="T_721ed_row27_col9" class="data row27 col9" >11599.3</td>
      <td id="T_721ed_row27_col10" class="data row27 col10" >1</td>
      <td id="T_721ed_row27_col11" class="data row27 col11" >9437184</td>
      <td id="T_721ed_row27_col12" class="data row27 col12" >torch.float32</td>
      <td id="T_721ed_row27_col13" class="data row27 col13" >(3072, 3072)</td>
      <td id="T_721ed_row27_col14" class="data row27 col14" >16</td>
      <td id="T_721ed_row27_col15" class="data row27 col15" >16.000000</td>
      <td id="T_721ed_row27_col16" class="data row27 col16" >1.000000</td>
      <td id="T_721ed_row27_col17" class="data row27 col17" >llama.model.layers.3.self_attn.q_proj.lora_B.default.weight</td>
      <td id="T_721ed_row27_col18" class="data row27 col18" >llama.model.layers.3.self_attn.q_proj.lora_A.default.weight</td>
      <td id="T_721ed_row27_col19" class="data row27 col19" >nan</td>
    </tr>
    <tr>
      <th id="T_721ed_level0_row28" class="row_heading level0 row28" >28</th>
      <td id="T_721ed_row28_col0" class="data row28 col0" >model.layers.16.self_attn.q_proj.weight</td>
      <td id="T_721ed_row28_col1" class="data row28 col1" >model.layers.16.self_attn.q_proj.weight</td>
      <td id="T_721ed_row28_col2" class="data row28 col2" >llama.model.layers.16.self_attn.q_proj</td>
      <td id="T_721ed_row28_col3" class="data row28 col3" >5.13175</td>
      <td id="T_721ed_row28_col4" class="data row28 col4" >77.4615</td>
      <td id="T_721ed_row28_col5" class="data row28 col5" >77.6328</td>
      <td id="T_721ed_row28_col6" class="data row28 col6" >0.066249</td>
      <td id="T_721ed_row28_col7" class="data row28 col7" >0.997813</td>
      <td id="T_721ed_row28_col8" class="data row28 col8" >0.0195027</td>
      <td id="T_721ed_row28_col9" class="data row28 col9" >11634.9</td>
      <td id="T_721ed_row28_col10" class="data row28 col10" >1</td>
      <td id="T_721ed_row28_col11" class="data row28 col11" >9437184</td>
      <td id="T_721ed_row28_col12" class="data row28 col12" >torch.float32</td>
      <td id="T_721ed_row28_col13" class="data row28 col13" >(3072, 3072)</td>
      <td id="T_721ed_row28_col14" class="data row28 col14" >16</td>
      <td id="T_721ed_row28_col15" class="data row28 col15" >16.000000</td>
      <td id="T_721ed_row28_col16" class="data row28 col16" >1.000000</td>
      <td id="T_721ed_row28_col17" class="data row28 col17" >llama.model.layers.16.self_attn.q_proj.lora_B.default.weight</td>
      <td id="T_721ed_row28_col18" class="data row28 col18" >llama.model.layers.16.self_attn.q_proj.lora_A.default.weight</td>
      <td id="T_721ed_row28_col19" class="data row28 col19" >nan</td>
    </tr>
    <tr>
      <th id="T_721ed_level0_row29" class="row_heading level0 row29" >29</th>
      <td id="T_721ed_row29_col0" class="data row29 col0" >model.layers.13.self_attn.q_proj.weight</td>
      <td id="T_721ed_row29_col1" class="data row29 col1" >model.layers.13.self_attn.q_proj.weight</td>
      <td id="T_721ed_row29_col2" class="data row29 col2" >llama.model.layers.13.self_attn.q_proj</td>
      <td id="T_721ed_row29_col3" class="data row29 col3" >4.95237</td>
      <td id="T_721ed_row29_col4" class="data row29 col4" >72.9186</td>
      <td id="T_721ed_row29_col5" class="data row29 col5" >73.0921</td>
      <td id="T_721ed_row29_col6" class="data row29 col6" >0.0679165</td>
      <td id="T_721ed_row29_col7" class="data row29 col7" >0.997702</td>
      <td id="T_721ed_row29_col8" class="data row29 col8" >0.0225682</td>
      <td id="T_721ed_row29_col9" class="data row29 col9" >11071.7</td>
      <td id="T_721ed_row29_col10" class="data row29 col10" >1</td>
      <td id="T_721ed_row29_col11" class="data row29 col11" >9437184</td>
      <td id="T_721ed_row29_col12" class="data row29 col12" >torch.float32</td>
      <td id="T_721ed_row29_col13" class="data row29 col13" >(3072, 3072)</td>
      <td id="T_721ed_row29_col14" class="data row29 col14" >16</td>
      <td id="T_721ed_row29_col15" class="data row29 col15" >16.000000</td>
      <td id="T_721ed_row29_col16" class="data row29 col16" >1.000000</td>
      <td id="T_721ed_row29_col17" class="data row29 col17" >llama.model.layers.13.self_attn.q_proj.lora_B.default.weight</td>
      <td id="T_721ed_row29_col18" class="data row29 col18" >llama.model.layers.13.self_attn.q_proj.lora_A.default.weight</td>
      <td id="T_721ed_row29_col19" class="data row29 col19" >nan</td>
    </tr>
    <tr>
      <th id="T_721ed_level0_row30" class="row_heading level0 row30" >30</th>
      <td id="T_721ed_row30_col0" class="data row30 col0" >model.layers.2.self_attn.q_proj.weight</td>
      <td id="T_721ed_row30_col1" class="data row30 col1" >model.layers.2.self_attn.q_proj.weight</td>
      <td id="T_721ed_row30_col2" class="data row30 col2" >llama.model.layers.2.self_attn.q_proj</td>
      <td id="T_721ed_row30_col3" class="data row30 col3" >4.84011</td>
      <td id="T_721ed_row30_col4" class="data row30 col4" >78.679</td>
      <td id="T_721ed_row30_col5" class="data row30 col5" >78.8307</td>
      <td id="T_721ed_row30_col6" class="data row30 col6" >0.0615173</td>
      <td id="T_721ed_row30_col7" class="data row30 col7" >0.998113</td>
      <td id="T_721ed_row30_col8" class="data row30 col8" >0.0219993</td>
      <td id="T_721ed_row30_col9" class="data row30 col9" >10870.5</td>
      <td id="T_721ed_row30_col10" class="data row30 col10" >1</td>
      <td id="T_721ed_row30_col11" class="data row30 col11" >9437184</td>
      <td id="T_721ed_row30_col12" class="data row30 col12" >torch.float32</td>
      <td id="T_721ed_row30_col13" class="data row30 col13" >(3072, 3072)</td>
      <td id="T_721ed_row30_col14" class="data row30 col14" >16</td>
      <td id="T_721ed_row30_col15" class="data row30 col15" >16.000000</td>
      <td id="T_721ed_row30_col16" class="data row30 col16" >1.000000</td>
      <td id="T_721ed_row30_col17" class="data row30 col17" >llama.model.layers.2.self_attn.q_proj.lora_B.default.weight</td>
      <td id="T_721ed_row30_col18" class="data row30 col18" >llama.model.layers.2.self_attn.q_proj.lora_A.default.weight</td>
      <td id="T_721ed_row30_col19" class="data row30 col19" >nan</td>
    </tr>
    <tr>
      <th id="T_721ed_level0_row31" class="row_heading level0 row31" >31</th>
      <td id="T_721ed_row31_col0" class="data row31 col0" >model.layers.4.self_attn.q_proj.weight</td>
      <td id="T_721ed_row31_col1" class="data row31 col1" >model.layers.4.self_attn.q_proj.weight</td>
      <td id="T_721ed_row31_col2" class="data row31 col2" >llama.model.layers.4.self_attn.q_proj</td>
      <td id="T_721ed_row31_col3" class="data row31 col3" >4.7559</td>
      <td id="T_721ed_row31_col4" class="data row31 col4" >78.8572</td>
      <td id="T_721ed_row31_col5" class="data row31 col5" >79.0034</td>
      <td id="T_721ed_row31_col6" class="data row31 col6" >0.0603103</td>
      <td id="T_721ed_row31_col7" class="data row31 col7" >0.998186</td>
      <td id="T_721ed_row31_col8" class="data row31 col8" >0.019249</td>
      <td id="T_721ed_row31_col9" class="data row31 col9" >10699.2</td>
      <td id="T_721ed_row31_col10" class="data row31 col10" >1</td>
      <td id="T_721ed_row31_col11" class="data row31 col11" >9437184</td>
      <td id="T_721ed_row31_col12" class="data row31 col12" >torch.float32</td>
      <td id="T_721ed_row31_col13" class="data row31 col13" >(3072, 3072)</td>
      <td id="T_721ed_row31_col14" class="data row31 col14" >16</td>
      <td id="T_721ed_row31_col15" class="data row31 col15" >16.000000</td>
      <td id="T_721ed_row31_col16" class="data row31 col16" >1.000000</td>
      <td id="T_721ed_row31_col17" class="data row31 col17" >llama.model.layers.4.self_attn.q_proj.lora_B.default.weight</td>
      <td id="T_721ed_row31_col18" class="data row31 col18" >llama.model.layers.4.self_attn.q_proj.lora_A.default.weight</td>
      <td id="T_721ed_row31_col19" class="data row31 col19" >nan</td>
    </tr>
    <tr>
      <th id="T_721ed_level0_row32" class="row_heading level0 row32" >32</th>
      <td id="T_721ed_row32_col0" class="data row32 col0" >model.layers.5.self_attn.q_proj.weight</td>
      <td id="T_721ed_row32_col1" class="data row32 col1" >model.layers.5.self_attn.q_proj.weight</td>
      <td id="T_721ed_row32_col2" class="data row32 col2" >llama.model.layers.5.self_attn.q_proj</td>
      <td id="T_721ed_row32_col3" class="data row32 col3" >4.75416</td>
      <td id="T_721ed_row32_col4" class="data row32 col4" >77.7967</td>
      <td id="T_721ed_row32_col5" class="data row32 col5" >77.9398</td>
      <td id="T_721ed_row32_col6" class="data row32 col6" >0.0611101</td>
      <td id="T_721ed_row32_col7" class="data row32 col7" >0.998138</td>
      <td id="T_721ed_row32_col8" class="data row32 col8" >0.0333706</td>
      <td id="T_721ed_row32_col9" class="data row32 col9" >10568</td>
      <td id="T_721ed_row32_col10" class="data row32 col10" >1</td>
      <td id="T_721ed_row32_col11" class="data row32 col11" >9437184</td>
      <td id="T_721ed_row32_col12" class="data row32 col12" >torch.float32</td>
      <td id="T_721ed_row32_col13" class="data row32 col13" >(3072, 3072)</td>
      <td id="T_721ed_row32_col14" class="data row32 col14" >16</td>
      <td id="T_721ed_row32_col15" class="data row32 col15" >16.000000</td>
      <td id="T_721ed_row32_col16" class="data row32 col16" >1.000000</td>
      <td id="T_721ed_row32_col17" class="data row32 col17" >llama.model.layers.5.self_attn.q_proj.lora_B.default.weight</td>
      <td id="T_721ed_row32_col18" class="data row32 col18" >llama.model.layers.5.self_attn.q_proj.lora_A.default.weight</td>
      <td id="T_721ed_row32_col19" class="data row32 col19" >nan</td>
    </tr>
    <tr>
      <th id="T_721ed_level0_row33" class="row_heading level0 row33" >33</th>
      <td id="T_721ed_row33_col0" class="data row33 col0" >model.layers.6.self_attn.q_proj.weight</td>
      <td id="T_721ed_row33_col1" class="data row33 col1" >model.layers.6.self_attn.q_proj.weight</td>
      <td id="T_721ed_row33_col2" class="data row33 col2" >llama.model.layers.6.self_attn.q_proj</td>
      <td id="T_721ed_row33_col3" class="data row33 col3" >4.73139</td>
      <td id="T_721ed_row33_col4" class="data row33 col4" >78.8319</td>
      <td id="T_721ed_row33_col5" class="data row33 col5" >78.9701</td>
      <td id="T_721ed_row33_col6" class="data row33 col6" >0.0600188</td>
      <td id="T_721ed_row33_col7" class="data row33 col7" >0.998204</td>
      <td id="T_721ed_row33_col8" class="data row33 col8" >0.0240963</td>
      <td id="T_721ed_row33_col9" class="data row33 col9" >10697.3</td>
      <td id="T_721ed_row33_col10" class="data row33 col10" >1</td>
      <td id="T_721ed_row33_col11" class="data row33 col11" >9437184</td>
      <td id="T_721ed_row33_col12" class="data row33 col12" >torch.float32</td>
      <td id="T_721ed_row33_col13" class="data row33 col13" >(3072, 3072)</td>
      <td id="T_721ed_row33_col14" class="data row33 col14" >16</td>
      <td id="T_721ed_row33_col15" class="data row33 col15" >16.000000</td>
      <td id="T_721ed_row33_col16" class="data row33 col16" >1.000000</td>
      <td id="T_721ed_row33_col17" class="data row33 col17" >llama.model.layers.6.self_attn.q_proj.lora_B.default.weight</td>
      <td id="T_721ed_row33_col18" class="data row33 col18" >llama.model.layers.6.self_attn.q_proj.lora_A.default.weight</td>
      <td id="T_721ed_row33_col19" class="data row33 col19" >nan</td>
    </tr>
    <tr>
      <th id="T_721ed_level0_row34" class="row_heading level0 row34" >34</th>
      <td id="T_721ed_row34_col0" class="data row34 col0" >model.layers.8.self_attn.q_proj.weight</td>
      <td id="T_721ed_row34_col1" class="data row34 col1" >model.layers.8.self_attn.q_proj.weight</td>
      <td id="T_721ed_row34_col2" class="data row34 col2" >llama.model.layers.8.self_attn.q_proj</td>
      <td id="T_721ed_row34_col3" class="data row34 col3" >4.71906</td>
      <td id="T_721ed_row34_col4" class="data row34 col4" >76.094</td>
      <td id="T_721ed_row34_col5" class="data row34 col5" >76.2416</td>
      <td id="T_721ed_row34_col6" class="data row34 col6" >0.0620162</td>
      <td id="T_721ed_row34_col7" class="data row34 col7" >0.998083</td>
      <td id="T_721ed_row34_col8" class="data row34 col8" >0.0264883</td>
      <td id="T_721ed_row34_col9" class="data row34 col9" >10480.7</td>
      <td id="T_721ed_row34_col10" class="data row34 col10" >1</td>
      <td id="T_721ed_row34_col11" class="data row34 col11" >9437184</td>
      <td id="T_721ed_row34_col12" class="data row34 col12" >torch.float32</td>
      <td id="T_721ed_row34_col13" class="data row34 col13" >(3072, 3072)</td>
      <td id="T_721ed_row34_col14" class="data row34 col14" >16</td>
      <td id="T_721ed_row34_col15" class="data row34 col15" >16.000000</td>
      <td id="T_721ed_row34_col16" class="data row34 col16" >1.000000</td>
      <td id="T_721ed_row34_col17" class="data row34 col17" >llama.model.layers.8.self_attn.q_proj.lora_B.default.weight</td>
      <td id="T_721ed_row34_col18" class="data row34 col18" >llama.model.layers.8.self_attn.q_proj.lora_A.default.weight</td>
      <td id="T_721ed_row34_col19" class="data row34 col19" >nan</td>
    </tr>
    <tr>
      <th id="T_721ed_level0_row35" class="row_heading level0 row35" >35</th>
      <td id="T_721ed_row35_col0" class="data row35 col0" >model.layers.16.self_attn.o_proj.weight</td>
      <td id="T_721ed_row35_col1" class="data row35 col1" >model.layers.16.self_attn.o_proj.weight</td>
      <td id="T_721ed_row35_col2" class="data row35 col2" >llama.model.layers.16.self_attn.o_proj</td>
      <td id="T_721ed_row35_col3" class="data row35 col3" >4.66776</td>
      <td id="T_721ed_row35_col4" class="data row35 col4" >47.8316</td>
      <td id="T_721ed_row35_col5" class="data row35 col5" >48.0513</td>
      <td id="T_721ed_row35_col6" class="data row35 col6" >0.0975874</td>
      <td id="T_721ed_row35_col7" class="data row35 col7" >0.995271</td>
      <td id="T_721ed_row35_col8" class="data row35 col8" >0.0171497</td>
      <td id="T_721ed_row35_col9" class="data row35 col9" >11067.5</td>
      <td id="T_721ed_row35_col10" class="data row35 col10" >1</td>
      <td id="T_721ed_row35_col11" class="data row35 col11" >9437184</td>
      <td id="T_721ed_row35_col12" class="data row35 col12" >torch.float32</td>
      <td id="T_721ed_row35_col13" class="data row35 col13" >(3072, 3072)</td>
      <td id="T_721ed_row35_col14" class="data row35 col14" >16</td>
      <td id="T_721ed_row35_col15" class="data row35 col15" >16.000000</td>
      <td id="T_721ed_row35_col16" class="data row35 col16" >1.000000</td>
      <td id="T_721ed_row35_col17" class="data row35 col17" >llama.model.layers.16.self_attn.o_proj.lora_B.default.weight</td>
      <td id="T_721ed_row35_col18" class="data row35 col18" >llama.model.layers.16.self_attn.o_proj.lora_A.default.weight</td>
      <td id="T_721ed_row35_col19" class="data row35 col19" >nan</td>
    </tr>
    <tr>
      <th id="T_721ed_level0_row36" class="row_heading level0 row36" >36</th>
      <td id="T_721ed_row36_col0" class="data row36 col0" >model.layers.1.self_attn.k_proj.weight</td>
      <td id="T_721ed_row36_col1" class="data row36 col1" >model.layers.1.self_attn.k_proj.weight</td>
      <td id="T_721ed_row36_col2" class="data row36 col2" >llama.model.layers.1.self_attn.k_proj</td>
      <td id="T_721ed_row36_col3" class="data row36 col3" >4.64981</td>
      <td id="T_721ed_row36_col4" class="data row36 col4" >65.3295</td>
      <td id="T_721ed_row36_col5" class="data row36 col5" >65.5216</td>
      <td id="T_721ed_row36_col6" class="data row36 col6" >0.0711747</td>
      <td id="T_721ed_row36_col7" class="data row36 col7" >0.997479</td>
      <td id="T_721ed_row36_col8" class="data row36 col8" >0.0472331</td>
      <td id="T_721ed_row36_col9" class="data row36 col9" >5513.22</td>
      <td id="T_721ed_row36_col10" class="data row36 col10" >1</td>
      <td id="T_721ed_row36_col11" class="data row36 col11" >3145728</td>
      <td id="T_721ed_row36_col12" class="data row36 col12" >torch.float32</td>
      <td id="T_721ed_row36_col13" class="data row36 col13" >(1024, 3072)</td>
      <td id="T_721ed_row36_col14" class="data row36 col14" >16</td>
      <td id="T_721ed_row36_col15" class="data row36 col15" >16.000000</td>
      <td id="T_721ed_row36_col16" class="data row36 col16" >1.000000</td>
      <td id="T_721ed_row36_col17" class="data row36 col17" >llama.model.layers.1.self_attn.k_proj.lora_B.default.weight</td>
      <td id="T_721ed_row36_col18" class="data row36 col18" >llama.model.layers.1.self_attn.k_proj.lora_A.default.weight</td>
      <td id="T_721ed_row36_col19" class="data row36 col19" >nan</td>
    </tr>
    <tr>
      <th id="T_721ed_level0_row37" class="row_heading level0 row37" >37</th>
      <td id="T_721ed_row37_col0" class="data row37 col0" >model.layers.11.self_attn.q_proj.weight</td>
      <td id="T_721ed_row37_col1" class="data row37 col1" >model.layers.11.self_attn.q_proj.weight</td>
      <td id="T_721ed_row37_col2" class="data row37 col2" >llama.model.layers.11.self_attn.q_proj</td>
      <td id="T_721ed_row37_col3" class="data row37 col3" >4.61343</td>
      <td id="T_721ed_row37_col4" class="data row37 col4" >74.2114</td>
      <td id="T_721ed_row37_col5" class="data row37 col5" >74.349</td>
      <td id="T_721ed_row37_col6" class="data row37 col6" >0.0621661</td>
      <td id="T_721ed_row37_col7" class="data row37 col7" >0.998073</td>
      <td id="T_721ed_row37_col8" class="data row37 col8" >0.0302069</td>
      <td id="T_721ed_row37_col9" class="data row37 col9" >10200.4</td>
      <td id="T_721ed_row37_col10" class="data row37 col10" >1</td>
      <td id="T_721ed_row37_col11" class="data row37 col11" >9437184</td>
      <td id="T_721ed_row37_col12" class="data row37 col12" >torch.float32</td>
      <td id="T_721ed_row37_col13" class="data row37 col13" >(3072, 3072)</td>
      <td id="T_721ed_row37_col14" class="data row37 col14" >16</td>
      <td id="T_721ed_row37_col15" class="data row37 col15" >16.000000</td>
      <td id="T_721ed_row37_col16" class="data row37 col16" >1.000000</td>
      <td id="T_721ed_row37_col17" class="data row37 col17" >llama.model.layers.11.self_attn.q_proj.lora_B.default.weight</td>
      <td id="T_721ed_row37_col18" class="data row37 col18" >llama.model.layers.11.self_attn.q_proj.lora_A.default.weight</td>
      <td id="T_721ed_row37_col19" class="data row37 col19" >nan</td>
    </tr>
    <tr>
      <th id="T_721ed_level0_row38" class="row_heading level0 row38" >38</th>
      <td id="T_721ed_row38_col0" class="data row38 col0" >model.layers.12.self_attn.q_proj.weight</td>
      <td id="T_721ed_row38_col1" class="data row38 col1" >model.layers.12.self_attn.q_proj.weight</td>
      <td id="T_721ed_row38_col2" class="data row38 col2" >llama.model.layers.12.self_attn.q_proj</td>
      <td id="T_721ed_row38_col3" class="data row38 col3" >4.59978</td>
      <td id="T_721ed_row38_col4" class="data row38 col4" >75.1555</td>
      <td id="T_721ed_row38_col5" class="data row38 col5" >75.2984</td>
      <td id="T_721ed_row38_col6" class="data row38 col6" >0.0612035</td>
      <td id="T_721ed_row38_col7" class="data row38 col7" >0.998132</td>
      <td id="T_721ed_row38_col8" class="data row38 col8" >0.0193363</td>
      <td id="T_721ed_row38_col9" class="data row38 col9" >10327.3</td>
      <td id="T_721ed_row38_col10" class="data row38 col10" >1</td>
      <td id="T_721ed_row38_col11" class="data row38 col11" >9437184</td>
      <td id="T_721ed_row38_col12" class="data row38 col12" >torch.float32</td>
      <td id="T_721ed_row38_col13" class="data row38 col13" >(3072, 3072)</td>
      <td id="T_721ed_row38_col14" class="data row38 col14" >16</td>
      <td id="T_721ed_row38_col15" class="data row38 col15" >16.000000</td>
      <td id="T_721ed_row38_col16" class="data row38 col16" >1.000000</td>
      <td id="T_721ed_row38_col17" class="data row38 col17" >llama.model.layers.12.self_attn.q_proj.lora_B.default.weight</td>
      <td id="T_721ed_row38_col18" class="data row38 col18" >llama.model.layers.12.self_attn.q_proj.lora_A.default.weight</td>
      <td id="T_721ed_row38_col19" class="data row38 col19" >nan</td>
    </tr>
    <tr>
      <th id="T_721ed_level0_row39" class="row_heading level0 row39" >39</th>
      <td id="T_721ed_row39_col0" class="data row39 col0" >model.layers.10.self_attn.q_proj.weight</td>
      <td id="T_721ed_row39_col1" class="data row39 col1" >model.layers.10.self_attn.q_proj.weight</td>
      <td id="T_721ed_row39_col2" class="data row39 col2" >llama.model.layers.10.self_attn.q_proj</td>
      <td id="T_721ed_row39_col3" class="data row39 col3" >4.41161</td>
      <td id="T_721ed_row39_col4" class="data row39 col4" >74.2835</td>
      <td id="T_721ed_row39_col5" class="data row39 col5" >74.4143</td>
      <td id="T_721ed_row39_col6" class="data row39 col6" >0.0593887</td>
      <td id="T_721ed_row39_col7" class="data row39 col7" >0.998241</td>
      <td id="T_721ed_row39_col8" class="data row39 col8" >0.0218871</td>
      <td id="T_721ed_row39_col9" class="data row39 col9" >10003.9</td>
      <td id="T_721ed_row39_col10" class="data row39 col10" >1</td>
      <td id="T_721ed_row39_col11" class="data row39 col11" >9437184</td>
      <td id="T_721ed_row39_col12" class="data row39 col12" >torch.float32</td>
      <td id="T_721ed_row39_col13" class="data row39 col13" >(3072, 3072)</td>
      <td id="T_721ed_row39_col14" class="data row39 col14" >16</td>
      <td id="T_721ed_row39_col15" class="data row39 col15" >16.000000</td>
      <td id="T_721ed_row39_col16" class="data row39 col16" >1.000000</td>
      <td id="T_721ed_row39_col17" class="data row39 col17" >llama.model.layers.10.self_attn.q_proj.lora_B.default.weight</td>
      <td id="T_721ed_row39_col18" class="data row39 col18" >llama.model.layers.10.self_attn.q_proj.lora_A.default.weight</td>
      <td id="T_721ed_row39_col19" class="data row39 col19" >nan</td>
    </tr>
    <tr>
      <th id="T_721ed_level0_row40" class="row_heading level0 row40" >40</th>
      <td id="T_721ed_row40_col0" class="data row40 col0" >model.layers.9.self_attn.q_proj.weight</td>
      <td id="T_721ed_row40_col1" class="data row40 col1" >model.layers.9.self_attn.q_proj.weight</td>
      <td id="T_721ed_row40_col2" class="data row40 col2" >llama.model.layers.9.self_attn.q_proj</td>
      <td id="T_721ed_row40_col3" class="data row40 col3" >4.13194</td>
      <td id="T_721ed_row40_col4" class="data row40 col4" >75.5784</td>
      <td id="T_721ed_row40_col5" class="data row40 col5" >75.6894</td>
      <td id="T_721ed_row40_col6" class="data row40 col6" >0.0546709</td>
      <td id="T_721ed_row40_col7" class="data row40 col7" >0.998509</td>
      <td id="T_721ed_row40_col8" class="data row40 col8" >0.0170265</td>
      <td id="T_721ed_row40_col9" class="data row40 col9" >9426.72</td>
      <td id="T_721ed_row40_col10" class="data row40 col10" >1</td>
      <td id="T_721ed_row40_col11" class="data row40 col11" >9437184</td>
      <td id="T_721ed_row40_col12" class="data row40 col12" >torch.float32</td>
      <td id="T_721ed_row40_col13" class="data row40 col13" >(3072, 3072)</td>
      <td id="T_721ed_row40_col14" class="data row40 col14" >16</td>
      <td id="T_721ed_row40_col15" class="data row40 col15" >16.000000</td>
      <td id="T_721ed_row40_col16" class="data row40 col16" >1.000000</td>
      <td id="T_721ed_row40_col17" class="data row40 col17" >llama.model.layers.9.self_attn.q_proj.lora_B.default.weight</td>
      <td id="T_721ed_row40_col18" class="data row40 col18" >llama.model.layers.9.self_attn.q_proj.lora_A.default.weight</td>
      <td id="T_721ed_row40_col19" class="data row40 col19" >nan</td>
    </tr>
    <tr>
      <th id="T_721ed_level0_row41" class="row_heading level0 row41" >41</th>
      <td id="T_721ed_row41_col0" class="data row41 col0" >model.layers.24.self_attn.k_proj.weight</td>
      <td id="T_721ed_row41_col1" class="data row41 col1" >model.layers.24.self_attn.k_proj.weight</td>
      <td id="T_721ed_row41_col2" class="data row41 col2" >llama.model.layers.24.self_attn.k_proj</td>
      <td id="T_721ed_row41_col3" class="data row41 col3" >4.10956</td>
      <td id="T_721ed_row41_col4" class="data row41 col4" >48.8202</td>
      <td id="T_721ed_row41_col5" class="data row41 col5" >48.9905</td>
      <td id="T_721ed_row41_col6" class="data row41 col6" >0.0841774</td>
      <td id="T_721ed_row41_col7" class="data row41 col7" >0.996475</td>
      <td id="T_721ed_row41_col8" class="data row41 col8" >0.0267256</td>
      <td id="T_721ed_row41_col9" class="data row41 col9" >5217.81</td>
      <td id="T_721ed_row41_col10" class="data row41 col10" >1</td>
      <td id="T_721ed_row41_col11" class="data row41 col11" >3145728</td>
      <td id="T_721ed_row41_col12" class="data row41 col12" >torch.float32</td>
      <td id="T_721ed_row41_col13" class="data row41 col13" >(1024, 3072)</td>
      <td id="T_721ed_row41_col14" class="data row41 col14" >16</td>
      <td id="T_721ed_row41_col15" class="data row41 col15" >16.000000</td>
      <td id="T_721ed_row41_col16" class="data row41 col16" >1.000000</td>
      <td id="T_721ed_row41_col17" class="data row41 col17" >llama.model.layers.24.self_attn.k_proj.lora_B.default.weight</td>
      <td id="T_721ed_row41_col18" class="data row41 col18" >llama.model.layers.24.self_attn.k_proj.lora_A.default.weight</td>
      <td id="T_721ed_row41_col19" class="data row41 col19" >nan</td>
    </tr>
    <tr>
      <th id="T_721ed_level0_row42" class="row_heading level0 row42" >42</th>
      <td id="T_721ed_row42_col0" class="data row42 col0" >model.layers.7.self_attn.q_proj.weight</td>
      <td id="T_721ed_row42_col1" class="data row42 col1" >model.layers.7.self_attn.q_proj.weight</td>
      <td id="T_721ed_row42_col2" class="data row42 col2" >llama.model.layers.7.self_attn.q_proj</td>
      <td id="T_721ed_row42_col3" class="data row42 col3" >4.07308</td>
      <td id="T_721ed_row42_col4" class="data row42 col4" >76.2703</td>
      <td id="T_721ed_row42_col5" class="data row42 col5" >76.3756</td>
      <td id="T_721ed_row42_col6" class="data row42 col6" >0.0534032</td>
      <td id="T_721ed_row42_col7" class="data row42 col7" >0.998577</td>
      <td id="T_721ed_row42_col8" class="data row42 col8" >0.0137544</td>
      <td id="T_721ed_row42_col9" class="data row42 col9" >9268.7</td>
      <td id="T_721ed_row42_col10" class="data row42 col10" >1</td>
      <td id="T_721ed_row42_col11" class="data row42 col11" >9437184</td>
      <td id="T_721ed_row42_col12" class="data row42 col12" >torch.float32</td>
      <td id="T_721ed_row42_col13" class="data row42 col13" >(3072, 3072)</td>
      <td id="T_721ed_row42_col14" class="data row42 col14" >16</td>
      <td id="T_721ed_row42_col15" class="data row42 col15" >16.000000</td>
      <td id="T_721ed_row42_col16" class="data row42 col16" >1.000000</td>
      <td id="T_721ed_row42_col17" class="data row42 col17" >llama.model.layers.7.self_attn.q_proj.lora_B.default.weight</td>
      <td id="T_721ed_row42_col18" class="data row42 col18" >llama.model.layers.7.self_attn.q_proj.lora_A.default.weight</td>
      <td id="T_721ed_row42_col19" class="data row42 col19" >nan</td>
    </tr>
    <tr>
      <th id="T_721ed_level0_row43" class="row_heading level0 row43" >43</th>
      <td id="T_721ed_row43_col0" class="data row43 col0" >model.layers.15.self_attn.o_proj.weight</td>
      <td id="T_721ed_row43_col1" class="data row43 col1" >model.layers.15.self_attn.o_proj.weight</td>
      <td id="T_721ed_row43_col2" class="data row43 col2" >llama.model.layers.15.self_attn.o_proj</td>
      <td id="T_721ed_row43_col3" class="data row43 col3" >3.94571</td>
      <td id="T_721ed_row43_col4" class="data row43 col4" >48.0643</td>
      <td id="T_721ed_row43_col5" class="data row43 col5" >48.2241</td>
      <td id="T_721ed_row43_col6" class="data row43 col6" >0.0820923</td>
      <td id="T_721ed_row43_col7" class="data row43 col7" >0.996647</td>
      <td id="T_721ed_row43_col8" class="data row43 col8" >0.0218263</td>
      <td id="T_721ed_row43_col9" class="data row43 col9" >9328.1</td>
      <td id="T_721ed_row43_col10" class="data row43 col10" >1</td>
      <td id="T_721ed_row43_col11" class="data row43 col11" >9437184</td>
      <td id="T_721ed_row43_col12" class="data row43 col12" >torch.float32</td>
      <td id="T_721ed_row43_col13" class="data row43 col13" >(3072, 3072)</td>
      <td id="T_721ed_row43_col14" class="data row43 col14" >16</td>
      <td id="T_721ed_row43_col15" class="data row43 col15" >16.000000</td>
      <td id="T_721ed_row43_col16" class="data row43 col16" >1.000000</td>
      <td id="T_721ed_row43_col17" class="data row43 col17" >llama.model.layers.15.self_attn.o_proj.lora_B.default.weight</td>
      <td id="T_721ed_row43_col18" class="data row43 col18" >llama.model.layers.15.self_attn.o_proj.lora_A.default.weight</td>
      <td id="T_721ed_row43_col19" class="data row43 col19" >nan</td>
    </tr>
    <tr>
      <th id="T_721ed_level0_row44" class="row_heading level0 row44" >44</th>
      <td id="T_721ed_row44_col0" class="data row44 col0" >model.layers.26.self_attn.k_proj.weight</td>
      <td id="T_721ed_row44_col1" class="data row44 col1" >model.layers.26.self_attn.k_proj.weight</td>
      <td id="T_721ed_row44_col2" class="data row44 col2" >llama.model.layers.26.self_attn.k_proj</td>
      <td id="T_721ed_row44_col3" class="data row44 col3" >3.90482</td>
      <td id="T_721ed_row44_col4" class="data row44 col4" >43.7926</td>
      <td id="T_721ed_row44_col5" class="data row44 col5" >43.956</td>
      <td id="T_721ed_row44_col6" class="data row44 col6" >0.0891661</td>
      <td id="T_721ed_row44_col7" class="data row44 col7" >0.996046</td>
      <td id="T_721ed_row44_col8" class="data row44 col8" >0.022806</td>
      <td id="T_721ed_row44_col9" class="data row44 col9" >5092.97</td>
      <td id="T_721ed_row44_col10" class="data row44 col10" >1</td>
      <td id="T_721ed_row44_col11" class="data row44 col11" >3145728</td>
      <td id="T_721ed_row44_col12" class="data row44 col12" >torch.float32</td>
      <td id="T_721ed_row44_col13" class="data row44 col13" >(1024, 3072)</td>
      <td id="T_721ed_row44_col14" class="data row44 col14" >16</td>
      <td id="T_721ed_row44_col15" class="data row44 col15" >16.000000</td>
      <td id="T_721ed_row44_col16" class="data row44 col16" >1.000000</td>
      <td id="T_721ed_row44_col17" class="data row44 col17" >llama.model.layers.26.self_attn.k_proj.lora_B.default.weight</td>
      <td id="T_721ed_row44_col18" class="data row44 col18" >llama.model.layers.26.self_attn.k_proj.lora_A.default.weight</td>
      <td id="T_721ed_row44_col19" class="data row44 col19" >nan</td>
    </tr>
    <tr>
      <th id="T_721ed_level0_row45" class="row_heading level0 row45" >45</th>
      <td id="T_721ed_row45_col0" class="data row45 col0" >model.layers.14.self_attn.o_proj.weight</td>
      <td id="T_721ed_row45_col1" class="data row45 col1" >model.layers.14.self_attn.o_proj.weight</td>
      <td id="T_721ed_row45_col2" class="data row45 col2" >llama.model.layers.14.self_attn.o_proj</td>
      <td id="T_721ed_row45_col3" class="data row45 col3" >3.89463</td>
      <td id="T_721ed_row45_col4" class="data row45 col4" >48.2612</td>
      <td id="T_721ed_row45_col5" class="data row45 col5" >48.4105</td>
      <td id="T_721ed_row45_col6" class="data row45 col6" >0.0806989</td>
      <td id="T_721ed_row45_col7" class="data row45 col7" >0.996759</td>
      <td id="T_721ed_row45_col8" class="data row45 col8" >0.0170148</td>
      <td id="T_721ed_row45_col9" class="data row45 col9" >9063.5</td>
      <td id="T_721ed_row45_col10" class="data row45 col10" >1</td>
      <td id="T_721ed_row45_col11" class="data row45 col11" >9437184</td>
      <td id="T_721ed_row45_col12" class="data row45 col12" >torch.float32</td>
      <td id="T_721ed_row45_col13" class="data row45 col13" >(3072, 3072)</td>
      <td id="T_721ed_row45_col14" class="data row45 col14" >16</td>
      <td id="T_721ed_row45_col15" class="data row45 col15" >16.000000</td>
      <td id="T_721ed_row45_col16" class="data row45 col16" >1.000000</td>
      <td id="T_721ed_row45_col17" class="data row45 col17" >llama.model.layers.14.self_attn.o_proj.lora_B.default.weight</td>
      <td id="T_721ed_row45_col18" class="data row45 col18" >llama.model.layers.14.self_attn.o_proj.lora_A.default.weight</td>
      <td id="T_721ed_row45_col19" class="data row45 col19" >nan</td>
    </tr>
    <tr>
      <th id="T_721ed_level0_row46" class="row_heading level0 row46" >46</th>
      <td id="T_721ed_row46_col0" class="data row46 col0" >model.layers.21.self_attn.k_proj.weight</td>
      <td id="T_721ed_row46_col1" class="data row46 col1" >model.layers.21.self_attn.k_proj.weight</td>
      <td id="T_721ed_row46_col2" class="data row46 col2" >llama.model.layers.21.self_attn.k_proj</td>
      <td id="T_721ed_row46_col3" class="data row46 col3" >3.89235</td>
      <td id="T_721ed_row46_col4" class="data row46 col4" >49.4378</td>
      <td id="T_721ed_row46_col5" class="data row46 col5" >49.5896</td>
      <td id="T_721ed_row46_col6" class="data row46 col6" >0.0787323</td>
      <td id="T_721ed_row46_col7" class="data row46 col7" >0.996915</td>
      <td id="T_721ed_row46_col8" class="data row46 col8" >0.0302526</td>
      <td id="T_721ed_row46_col9" class="data row46 col9" >4915.29</td>
      <td id="T_721ed_row46_col10" class="data row46 col10" >1</td>
      <td id="T_721ed_row46_col11" class="data row46 col11" >3145728</td>
      <td id="T_721ed_row46_col12" class="data row46 col12" >torch.float32</td>
      <td id="T_721ed_row46_col13" class="data row46 col13" >(1024, 3072)</td>
      <td id="T_721ed_row46_col14" class="data row46 col14" >16</td>
      <td id="T_721ed_row46_col15" class="data row46 col15" >16.000000</td>
      <td id="T_721ed_row46_col16" class="data row46 col16" >1.000000</td>
      <td id="T_721ed_row46_col17" class="data row46 col17" >llama.model.layers.21.self_attn.k_proj.lora_B.default.weight</td>
      <td id="T_721ed_row46_col18" class="data row46 col18" >llama.model.layers.21.self_attn.k_proj.lora_A.default.weight</td>
      <td id="T_721ed_row46_col19" class="data row46 col19" >nan</td>
    </tr>
    <tr>
      <th id="T_721ed_level0_row47" class="row_heading level0 row47" >47</th>
      <td id="T_721ed_row47_col0" class="data row47 col0" >model.layers.20.self_attn.k_proj.weight</td>
      <td id="T_721ed_row47_col1" class="data row47 col1" >model.layers.20.self_attn.k_proj.weight</td>
      <td id="T_721ed_row47_col2" class="data row47 col2" >llama.model.layers.20.self_attn.k_proj</td>
      <td id="T_721ed_row47_col3" class="data row47 col3" >3.88659</td>
      <td id="T_721ed_row47_col4" class="data row47 col4" >52.0836</td>
      <td id="T_721ed_row47_col5" class="data row47 col5" >52.2314</td>
      <td id="T_721ed_row47_col6" class="data row47 col6" >0.0746222</td>
      <td id="T_721ed_row47_col7" class="data row47 col7" >0.997228</td>
      <td id="T_721ed_row47_col8" class="data row47 col8" >0.0235871</td>
      <td id="T_721ed_row47_col9" class="data row47 col9" >4984.08</td>
      <td id="T_721ed_row47_col10" class="data row47 col10" >1</td>
      <td id="T_721ed_row47_col11" class="data row47 col11" >3145728</td>
      <td id="T_721ed_row47_col12" class="data row47 col12" >torch.float32</td>
      <td id="T_721ed_row47_col13" class="data row47 col13" >(1024, 3072)</td>
      <td id="T_721ed_row47_col14" class="data row47 col14" >16</td>
      <td id="T_721ed_row47_col15" class="data row47 col15" >16.000000</td>
      <td id="T_721ed_row47_col16" class="data row47 col16" >1.000000</td>
      <td id="T_721ed_row47_col17" class="data row47 col17" >llama.model.layers.20.self_attn.k_proj.lora_B.default.weight</td>
      <td id="T_721ed_row47_col18" class="data row47 col18" >llama.model.layers.20.self_attn.k_proj.lora_A.default.weight</td>
      <td id="T_721ed_row47_col19" class="data row47 col19" >nan</td>
    </tr>
    <tr>
      <th id="T_721ed_level0_row48" class="row_heading level0 row48" >48</th>
      <td id="T_721ed_row48_col0" class="data row48 col0" >model.layers.3.self_attn.o_proj.weight</td>
      <td id="T_721ed_row48_col1" class="data row48 col1" >model.layers.3.self_attn.o_proj.weight</td>
      <td id="T_721ed_row48_col2" class="data row48 col2" >llama.model.layers.3.self_attn.o_proj</td>
      <td id="T_721ed_row48_col3" class="data row48 col3" >3.82804</td>
      <td id="T_721ed_row48_col4" class="data row48 col4" >46.0072</td>
      <td id="T_721ed_row48_col5" class="data row48 col5" >46.1661</td>
      <td id="T_721ed_row48_col6" class="data row48 col6" >0.0832052</td>
      <td id="T_721ed_row48_col7" class="data row48 col7" >0.996556</td>
      <td id="T_721ed_row48_col8" class="data row48 col8" >0.0137587</td>
      <td id="T_721ed_row48_col9" class="data row48 col9" >9051.45</td>
      <td id="T_721ed_row48_col10" class="data row48 col10" >1</td>
      <td id="T_721ed_row48_col11" class="data row48 col11" >9437184</td>
      <td id="T_721ed_row48_col12" class="data row48 col12" >torch.float32</td>
      <td id="T_721ed_row48_col13" class="data row48 col13" >(3072, 3072)</td>
      <td id="T_721ed_row48_col14" class="data row48 col14" >16</td>
      <td id="T_721ed_row48_col15" class="data row48 col15" >16.000000</td>
      <td id="T_721ed_row48_col16" class="data row48 col16" >1.000000</td>
      <td id="T_721ed_row48_col17" class="data row48 col17" >llama.model.layers.3.self_attn.o_proj.lora_B.default.weight</td>
      <td id="T_721ed_row48_col18" class="data row48 col18" >llama.model.layers.3.self_attn.o_proj.lora_A.default.weight</td>
      <td id="T_721ed_row48_col19" class="data row48 col19" >nan</td>
    </tr>
    <tr>
      <th id="T_721ed_level0_row49" class="row_heading level0 row49" >49</th>
      <td id="T_721ed_row49_col0" class="data row49 col0" >model.layers.23.self_attn.k_proj.weight</td>
      <td id="T_721ed_row49_col1" class="data row49 col1" >model.layers.23.self_attn.k_proj.weight</td>
      <td id="T_721ed_row49_col2" class="data row49 col2" >llama.model.layers.23.self_attn.k_proj</td>
      <td id="T_721ed_row49_col3" class="data row49 col3" >3.81568</td>
      <td id="T_721ed_row49_col4" class="data row49 col4" >50.6905</td>
      <td id="T_721ed_row49_col5" class="data row49 col5" >50.8292</td>
      <td id="T_721ed_row49_col6" class="data row49 col6" >0.0752741</td>
      <td id="T_721ed_row49_col7" class="data row49 col7" >0.997178</td>
      <td id="T_721ed_row49_col8" class="data row49 col8" >0.0263263</td>
      <td id="T_721ed_row49_col9" class="data row49 col9" >4905.3</td>
      <td id="T_721ed_row49_col10" class="data row49 col10" >1</td>
      <td id="T_721ed_row49_col11" class="data row49 col11" >3145728</td>
      <td id="T_721ed_row49_col12" class="data row49 col12" >torch.float32</td>
      <td id="T_721ed_row49_col13" class="data row49 col13" >(1024, 3072)</td>
      <td id="T_721ed_row49_col14" class="data row49 col14" >16</td>
      <td id="T_721ed_row49_col15" class="data row49 col15" >16.000000</td>
      <td id="T_721ed_row49_col16" class="data row49 col16" >1.000000</td>
      <td id="T_721ed_row49_col17" class="data row49 col17" >llama.model.layers.23.self_attn.k_proj.lora_B.default.weight</td>
      <td id="T_721ed_row49_col18" class="data row49 col18" >llama.model.layers.23.self_attn.k_proj.lora_A.default.weight</td>
      <td id="T_721ed_row49_col19" class="data row49 col19" >nan</td>
    </tr>
    <tr>
      <th id="T_721ed_level0_row50" class="row_heading level0 row50" >50</th>
      <td id="T_721ed_row50_col0" class="data row50 col0" >model.layers.2.self_attn.o_proj.weight</td>
      <td id="T_721ed_row50_col1" class="data row50 col1" >model.layers.2.self_attn.o_proj.weight</td>
      <td id="T_721ed_row50_col2" class="data row50 col2" >llama.model.layers.2.self_attn.o_proj</td>
      <td id="T_721ed_row50_col3" class="data row50 col3" >3.76435</td>
      <td id="T_721ed_row50_col4" class="data row50 col4" >40.3328</td>
      <td id="T_721ed_row50_col5" class="data row50 col5" >40.5061</td>
      <td id="T_721ed_row50_col6" class="data row50 col6" >0.0933321</td>
      <td id="T_721ed_row50_col7" class="data row50 col7" >0.995672</td>
      <td id="T_721ed_row50_col8" class="data row50 col8" >0.0118767</td>
      <td id="T_721ed_row50_col9" class="data row50 col9" >8880.41</td>
      <td id="T_721ed_row50_col10" class="data row50 col10" >1</td>
      <td id="T_721ed_row50_col11" class="data row50 col11" >9437184</td>
      <td id="T_721ed_row50_col12" class="data row50 col12" >torch.float32</td>
      <td id="T_721ed_row50_col13" class="data row50 col13" >(3072, 3072)</td>
      <td id="T_721ed_row50_col14" class="data row50 col14" >16</td>
      <td id="T_721ed_row50_col15" class="data row50 col15" >16.000000</td>
      <td id="T_721ed_row50_col16" class="data row50 col16" >1.000000</td>
      <td id="T_721ed_row50_col17" class="data row50 col17" >llama.model.layers.2.self_attn.o_proj.lora_B.default.weight</td>
      <td id="T_721ed_row50_col18" class="data row50 col18" >llama.model.layers.2.self_attn.o_proj.lora_A.default.weight</td>
      <td id="T_721ed_row50_col19" class="data row50 col19" >nan</td>
    </tr>
    <tr>
      <th id="T_721ed_level0_row51" class="row_heading level0 row51" >51</th>
      <td id="T_721ed_row51_col0" class="data row51 col0" >model.layers.21.self_attn.v_proj.weight</td>
      <td id="T_721ed_row51_col1" class="data row51 col1" >model.layers.21.self_attn.v_proj.weight</td>
      <td id="T_721ed_row51_col2" class="data row51 col2" >llama.model.layers.21.self_attn.v_proj</td>
      <td id="T_721ed_row51_col3" class="data row51 col3" >3.73275</td>
      <td id="T_721ed_row51_col4" class="data row51 col4" >31.6731</td>
      <td id="T_721ed_row51_col5" class="data row51 col5" >31.8905</td>
      <td id="T_721ed_row51_col6" class="data row51 col6" >0.117852</td>
      <td id="T_721ed_row51_col7" class="data row51 col7" >0.993126</td>
      <td id="T_721ed_row51_col8" class="data row51 col8" >0.0177208</td>
      <td id="T_721ed_row51_col9" class="data row51 col9" >5055.22</td>
      <td id="T_721ed_row51_col10" class="data row51 col10" >1</td>
      <td id="T_721ed_row51_col11" class="data row51 col11" >3145728</td>
      <td id="T_721ed_row51_col12" class="data row51 col12" >torch.float32</td>
      <td id="T_721ed_row51_col13" class="data row51 col13" >(1024, 3072)</td>
      <td id="T_721ed_row51_col14" class="data row51 col14" >16</td>
      <td id="T_721ed_row51_col15" class="data row51 col15" >16.000000</td>
      <td id="T_721ed_row51_col16" class="data row51 col16" >1.000000</td>
      <td id="T_721ed_row51_col17" class="data row51 col17" >llama.model.layers.21.self_attn.v_proj.lora_B.default.weight</td>
      <td id="T_721ed_row51_col18" class="data row51 col18" >llama.model.layers.21.self_attn.v_proj.lora_A.default.weight</td>
      <td id="T_721ed_row51_col19" class="data row51 col19" >nan</td>
    </tr>
    <tr>
      <th id="T_721ed_level0_row52" class="row_heading level0 row52" >52</th>
      <td id="T_721ed_row52_col0" class="data row52 col0" >model.layers.24.self_attn.v_proj.weight</td>
      <td id="T_721ed_row52_col1" class="data row52 col1" >model.layers.24.self_attn.v_proj.weight</td>
      <td id="T_721ed_row52_col2" class="data row52 col2" >llama.model.layers.24.self_attn.v_proj</td>
      <td id="T_721ed_row52_col3" class="data row52 col3" >3.69548</td>
      <td id="T_721ed_row52_col4" class="data row52 col4" >34.6935</td>
      <td id="T_721ed_row52_col5" class="data row52 col5" >34.891</td>
      <td id="T_721ed_row52_col6" class="data row52 col6" >0.106518</td>
      <td id="T_721ed_row52_col7" class="data row52 col7" >0.994375</td>
      <td id="T_721ed_row52_col8" class="data row52 col8" >0.0219952</td>
      <td id="T_721ed_row52_col9" class="data row52 col9" >5010.74</td>
      <td id="T_721ed_row52_col10" class="data row52 col10" >1</td>
      <td id="T_721ed_row52_col11" class="data row52 col11" >3145728</td>
      <td id="T_721ed_row52_col12" class="data row52 col12" >torch.float32</td>
      <td id="T_721ed_row52_col13" class="data row52 col13" >(1024, 3072)</td>
      <td id="T_721ed_row52_col14" class="data row52 col14" >16</td>
      <td id="T_721ed_row52_col15" class="data row52 col15" >16.000000</td>
      <td id="T_721ed_row52_col16" class="data row52 col16" >1.000000</td>
      <td id="T_721ed_row52_col17" class="data row52 col17" >llama.model.layers.24.self_attn.v_proj.lora_B.default.weight</td>
      <td id="T_721ed_row52_col18" class="data row52 col18" >llama.model.layers.24.self_attn.v_proj.lora_A.default.weight</td>
      <td id="T_721ed_row52_col19" class="data row52 col19" >nan</td>
    </tr>
    <tr>
      <th id="T_721ed_level0_row53" class="row_heading level0 row53" >53</th>
      <td id="T_721ed_row53_col0" class="data row53 col0" >model.layers.4.self_attn.o_proj.weight</td>
      <td id="T_721ed_row53_col1" class="data row53 col1" >model.layers.4.self_attn.o_proj.weight</td>
      <td id="T_721ed_row53_col2" class="data row53 col2" >llama.model.layers.4.self_attn.o_proj</td>
      <td id="T_721ed_row53_col3" class="data row53 col3" >3.68955</td>
      <td id="T_721ed_row53_col4" class="data row53 col4" >47.1752</td>
      <td id="T_721ed_row53_col5" class="data row53 col5" >47.3177</td>
      <td id="T_721ed_row53_col6" class="data row53 col6" >0.0782094</td>
      <td id="T_721ed_row53_col7" class="data row53 col7" >0.996955</td>
      <td id="T_721ed_row53_col8" class="data row53 col8" >0.0259983</td>
      <td id="T_721ed_row53_col9" class="data row53 col9" >8712.22</td>
      <td id="T_721ed_row53_col10" class="data row53 col10" >1</td>
      <td id="T_721ed_row53_col11" class="data row53 col11" >9437184</td>
      <td id="T_721ed_row53_col12" class="data row53 col12" >torch.float32</td>
      <td id="T_721ed_row53_col13" class="data row53 col13" >(3072, 3072)</td>
      <td id="T_721ed_row53_col14" class="data row53 col14" >16</td>
      <td id="T_721ed_row53_col15" class="data row53 col15" >16.000000</td>
      <td id="T_721ed_row53_col16" class="data row53 col16" >1.000000</td>
      <td id="T_721ed_row53_col17" class="data row53 col17" >llama.model.layers.4.self_attn.o_proj.lora_B.default.weight</td>
      <td id="T_721ed_row53_col18" class="data row53 col18" >llama.model.layers.4.self_attn.o_proj.lora_A.default.weight</td>
      <td id="T_721ed_row53_col19" class="data row53 col19" >nan</td>
    </tr>
    <tr>
      <th id="T_721ed_level0_row54" class="row_heading level0 row54" >54</th>
      <td id="T_721ed_row54_col0" class="data row54 col0" >model.layers.22.self_attn.v_proj.weight</td>
      <td id="T_721ed_row54_col1" class="data row54 col1" >model.layers.22.self_attn.v_proj.weight</td>
      <td id="T_721ed_row54_col2" class="data row54 col2" >llama.model.layers.22.self_attn.v_proj</td>
      <td id="T_721ed_row54_col3" class="data row54 col3" >3.54595</td>
      <td id="T_721ed_row54_col4" class="data row54 col4" >31.9637</td>
      <td id="T_721ed_row54_col5" class="data row54 col5" >32.1577</td>
      <td id="T_721ed_row54_col6" class="data row54 col6" >0.110937</td>
      <td id="T_721ed_row54_col7" class="data row54 col7" >0.993902</td>
      <td id="T_721ed_row54_col8" class="data row54 col8" >0.0200987</td>
      <td id="T_721ed_row54_col9" class="data row54 col9" >4820.19</td>
      <td id="T_721ed_row54_col10" class="data row54 col10" >1</td>
      <td id="T_721ed_row54_col11" class="data row54 col11" >3145728</td>
      <td id="T_721ed_row54_col12" class="data row54 col12" >torch.float32</td>
      <td id="T_721ed_row54_col13" class="data row54 col13" >(1024, 3072)</td>
      <td id="T_721ed_row54_col14" class="data row54 col14" >16</td>
      <td id="T_721ed_row54_col15" class="data row54 col15" >16.000000</td>
      <td id="T_721ed_row54_col16" class="data row54 col16" >1.000000</td>
      <td id="T_721ed_row54_col17" class="data row54 col17" >llama.model.layers.22.self_attn.v_proj.lora_B.default.weight</td>
      <td id="T_721ed_row54_col18" class="data row54 col18" >llama.model.layers.22.self_attn.v_proj.lora_A.default.weight</td>
      <td id="T_721ed_row54_col19" class="data row54 col19" >nan</td>
    </tr>
    <tr>
      <th id="T_721ed_level0_row55" class="row_heading level0 row55" >55</th>
      <td id="T_721ed_row55_col0" class="data row55 col0" >model.layers.22.self_attn.k_proj.weight</td>
      <td id="T_721ed_row55_col1" class="data row55 col1" >model.layers.22.self_attn.k_proj.weight</td>
      <td id="T_721ed_row55_col2" class="data row55 col2" >llama.model.layers.22.self_attn.k_proj</td>
      <td id="T_721ed_row55_col3" class="data row55 col3" >3.54257</td>
      <td id="T_721ed_row55_col4" class="data row55 col4" >48.8439</td>
      <td id="T_721ed_row55_col5" class="data row55 col5" >48.9736</td>
      <td id="T_721ed_row55_col6" class="data row55 col6" >0.0725284</td>
      <td id="T_721ed_row55_col7" class="data row55 col7" >0.99738</td>
      <td id="T_721ed_row55_col8" class="data row55 col8" >0.0186145</td>
      <td id="T_721ed_row55_col9" class="data row55 col9" >4645.85</td>
      <td id="T_721ed_row55_col10" class="data row55 col10" >1</td>
      <td id="T_721ed_row55_col11" class="data row55 col11" >3145728</td>
      <td id="T_721ed_row55_col12" class="data row55 col12" >torch.float32</td>
      <td id="T_721ed_row55_col13" class="data row55 col13" >(1024, 3072)</td>
      <td id="T_721ed_row55_col14" class="data row55 col14" >16</td>
      <td id="T_721ed_row55_col15" class="data row55 col15" >16.000000</td>
      <td id="T_721ed_row55_col16" class="data row55 col16" >1.000000</td>
      <td id="T_721ed_row55_col17" class="data row55 col17" >llama.model.layers.22.self_attn.k_proj.lora_B.default.weight</td>
      <td id="T_721ed_row55_col18" class="data row55 col18" >llama.model.layers.22.self_attn.k_proj.lora_A.default.weight</td>
      <td id="T_721ed_row55_col19" class="data row55 col19" >nan</td>
    </tr>
    <tr>
      <th id="T_721ed_level0_row56" class="row_heading level0 row56" >56</th>
      <td id="T_721ed_row56_col0" class="data row56 col0" >model.layers.20.self_attn.v_proj.weight</td>
      <td id="T_721ed_row56_col1" class="data row56 col1" >model.layers.20.self_attn.v_proj.weight</td>
      <td id="T_721ed_row56_col2" class="data row56 col2" >llama.model.layers.20.self_attn.v_proj</td>
      <td id="T_721ed_row56_col3" class="data row56 col3" >3.54081</td>
      <td id="T_721ed_row56_col4" class="data row56 col4" >28.9502</td>
      <td id="T_721ed_row56_col5" class="data row56 col5" >29.1631</td>
      <td id="T_721ed_row56_col6" class="data row56 col6" >0.122307</td>
      <td id="T_721ed_row56_col7" class="data row56 col7" >0.992602</td>
      <td id="T_721ed_row56_col8" class="data row56 col8" >0.0168691</td>
      <td id="T_721ed_row56_col9" class="data row56 col9" >4839.21</td>
      <td id="T_721ed_row56_col10" class="data row56 col10" >1</td>
      <td id="T_721ed_row56_col11" class="data row56 col11" >3145728</td>
      <td id="T_721ed_row56_col12" class="data row56 col12" >torch.float32</td>
      <td id="T_721ed_row56_col13" class="data row56 col13" >(1024, 3072)</td>
      <td id="T_721ed_row56_col14" class="data row56 col14" >16</td>
      <td id="T_721ed_row56_col15" class="data row56 col15" >16.000000</td>
      <td id="T_721ed_row56_col16" class="data row56 col16" >1.000000</td>
      <td id="T_721ed_row56_col17" class="data row56 col17" >llama.model.layers.20.self_attn.v_proj.lora_B.default.weight</td>
      <td id="T_721ed_row56_col18" class="data row56 col18" >llama.model.layers.20.self_attn.v_proj.lora_A.default.weight</td>
      <td id="T_721ed_row56_col19" class="data row56 col19" >nan</td>
    </tr>
    <tr>
      <th id="T_721ed_level0_row57" class="row_heading level0 row57" >57</th>
      <td id="T_721ed_row57_col0" class="data row57 col0" >model.layers.27.self_attn.k_proj.weight</td>
      <td id="T_721ed_row57_col1" class="data row57 col1" >model.layers.27.self_attn.k_proj.weight</td>
      <td id="T_721ed_row57_col2" class="data row57 col2" >llama.model.layers.27.self_attn.k_proj</td>
      <td id="T_721ed_row57_col3" class="data row57 col3" >3.52973</td>
      <td id="T_721ed_row57_col4" class="data row57 col4" >44.1462</td>
      <td id="T_721ed_row57_col5" class="data row57 col5" >44.2774</td>
      <td id="T_721ed_row57_col6" class="data row57 col6" >0.0799555</td>
      <td id="T_721ed_row57_col7" class="data row57 col7" >0.996817</td>
      <td id="T_721ed_row57_col8" class="data row57 col8" >0.0222999</td>
      <td id="T_721ed_row57_col9" class="data row57 col9" >4633.48</td>
      <td id="T_721ed_row57_col10" class="data row57 col10" >1</td>
      <td id="T_721ed_row57_col11" class="data row57 col11" >3145728</td>
      <td id="T_721ed_row57_col12" class="data row57 col12" >torch.float32</td>
      <td id="T_721ed_row57_col13" class="data row57 col13" >(1024, 3072)</td>
      <td id="T_721ed_row57_col14" class="data row57 col14" >16</td>
      <td id="T_721ed_row57_col15" class="data row57 col15" >16.000000</td>
      <td id="T_721ed_row57_col16" class="data row57 col16" >1.000000</td>
      <td id="T_721ed_row57_col17" class="data row57 col17" >llama.model.layers.27.self_attn.k_proj.lora_B.default.weight</td>
      <td id="T_721ed_row57_col18" class="data row57 col18" >llama.model.layers.27.self_attn.k_proj.lora_A.default.weight</td>
      <td id="T_721ed_row57_col19" class="data row57 col19" >nan</td>
    </tr>
    <tr>
      <th id="T_721ed_level0_row58" class="row_heading level0 row58" >58</th>
      <td id="T_721ed_row58_col0" class="data row58 col0" >model.layers.5.self_attn.o_proj.weight</td>
      <td id="T_721ed_row58_col1" class="data row58 col1" >model.layers.5.self_attn.o_proj.weight</td>
      <td id="T_721ed_row58_col2" class="data row58 col2" >llama.model.layers.5.self_attn.o_proj</td>
      <td id="T_721ed_row58_col3" class="data row58 col3" >3.5089</td>
      <td id="T_721ed_row58_col4" class="data row58 col4" >42.6888</td>
      <td id="T_721ed_row58_col5" class="data row58 col5" >42.8314</td>
      <td id="T_721ed_row58_col6" class="data row58 col6" >0.0821972</td>
      <td id="T_721ed_row58_col7" class="data row58 col7" >0.996639</td>
      <td id="T_721ed_row58_col8" class="data row58 col8" >0.0137769</td>
      <td id="T_721ed_row58_col9" class="data row58 col9" >8288.62</td>
      <td id="T_721ed_row58_col10" class="data row58 col10" >1</td>
      <td id="T_721ed_row58_col11" class="data row58 col11" >9437184</td>
      <td id="T_721ed_row58_col12" class="data row58 col12" >torch.float32</td>
      <td id="T_721ed_row58_col13" class="data row58 col13" >(3072, 3072)</td>
      <td id="T_721ed_row58_col14" class="data row58 col14" >16</td>
      <td id="T_721ed_row58_col15" class="data row58 col15" >16.000000</td>
      <td id="T_721ed_row58_col16" class="data row58 col16" >1.000000</td>
      <td id="T_721ed_row58_col17" class="data row58 col17" >llama.model.layers.5.self_attn.o_proj.lora_B.default.weight</td>
      <td id="T_721ed_row58_col18" class="data row58 col18" >llama.model.layers.5.self_attn.o_proj.lora_A.default.weight</td>
      <td id="T_721ed_row58_col19" class="data row58 col19" >nan</td>
    </tr>
    <tr>
      <th id="T_721ed_level0_row59" class="row_heading level0 row59" >59</th>
      <td id="T_721ed_row59_col0" class="data row59 col0" >model.layers.16.self_attn.k_proj.weight</td>
      <td id="T_721ed_row59_col1" class="data row59 col1" >model.layers.16.self_attn.k_proj.weight</td>
      <td id="T_721ed_row59_col2" class="data row59 col2" >llama.model.layers.16.self_attn.k_proj</td>
      <td id="T_721ed_row59_col3" class="data row59 col3" >3.44934</td>
      <td id="T_721ed_row59_col4" class="data row59 col4" >56.0838</td>
      <td id="T_721ed_row59_col5" class="data row59 col5" >56.1898</td>
      <td id="T_721ed_row59_col6" class="data row59 col6" >0.0615034</td>
      <td id="T_721ed_row59_col7" class="data row59 col7" >0.998114</td>
      <td id="T_721ed_row59_col8" class="data row59 col8" >0.0205087</td>
      <td id="T_721ed_row59_col9" class="data row59 col9" >4538.47</td>
      <td id="T_721ed_row59_col10" class="data row59 col10" >1</td>
      <td id="T_721ed_row59_col11" class="data row59 col11" >3145728</td>
      <td id="T_721ed_row59_col12" class="data row59 col12" >torch.float32</td>
      <td id="T_721ed_row59_col13" class="data row59 col13" >(1024, 3072)</td>
      <td id="T_721ed_row59_col14" class="data row59 col14" >16</td>
      <td id="T_721ed_row59_col15" class="data row59 col15" >16.000000</td>
      <td id="T_721ed_row59_col16" class="data row59 col16" >1.000000</td>
      <td id="T_721ed_row59_col17" class="data row59 col17" >llama.model.layers.16.self_attn.k_proj.lora_B.default.weight</td>
      <td id="T_721ed_row59_col18" class="data row59 col18" >llama.model.layers.16.self_attn.k_proj.lora_A.default.weight</td>
      <td id="T_721ed_row59_col19" class="data row59 col19" >nan</td>
    </tr>
    <tr>
      <th id="T_721ed_level0_row60" class="row_heading level0 row60" >60</th>
      <td id="T_721ed_row60_col0" class="data row60 col0" >model.layers.6.self_attn.o_proj.weight</td>
      <td id="T_721ed_row60_col1" class="data row60 col1" >model.layers.6.self_attn.o_proj.weight</td>
      <td id="T_721ed_row60_col2" class="data row60 col2" >llama.model.layers.6.self_attn.o_proj</td>
      <td id="T_721ed_row60_col3" class="data row60 col3" >3.44871</td>
      <td id="T_721ed_row60_col4" class="data row60 col4" >44.1922</td>
      <td id="T_721ed_row60_col5" class="data row60 col5" >44.3258</td>
      <td id="T_721ed_row60_col6" class="data row60 col6" >0.0780389</td>
      <td id="T_721ed_row60_col7" class="data row60 col7" >0.996969</td>
      <td id="T_721ed_row60_col8" class="data row60 col8" >0.0106917</td>
      <td id="T_721ed_row60_col9" class="data row60 col9" >8176.47</td>
      <td id="T_721ed_row60_col10" class="data row60 col10" >1</td>
      <td id="T_721ed_row60_col11" class="data row60 col11" >9437184</td>
      <td id="T_721ed_row60_col12" class="data row60 col12" >torch.float32</td>
      <td id="T_721ed_row60_col13" class="data row60 col13" >(3072, 3072)</td>
      <td id="T_721ed_row60_col14" class="data row60 col14" >16</td>
      <td id="T_721ed_row60_col15" class="data row60 col15" >16.000000</td>
      <td id="T_721ed_row60_col16" class="data row60 col16" >1.000000</td>
      <td id="T_721ed_row60_col17" class="data row60 col17" >llama.model.layers.6.self_attn.o_proj.lora_B.default.weight</td>
      <td id="T_721ed_row60_col18" class="data row60 col18" >llama.model.layers.6.self_attn.o_proj.lora_A.default.weight</td>
      <td id="T_721ed_row60_col19" class="data row60 col19" >nan</td>
    </tr>
    <tr>
      <th id="T_721ed_level0_row61" class="row_heading level0 row61" >61</th>
      <td id="T_721ed_row61_col0" class="data row61 col0" >model.layers.19.self_attn.k_proj.weight</td>
      <td id="T_721ed_row61_col1" class="data row61 col1" >model.layers.19.self_attn.k_proj.weight</td>
      <td id="T_721ed_row61_col2" class="data row61 col2" >llama.model.layers.19.self_attn.k_proj</td>
      <td id="T_721ed_row61_col3" class="data row61 col3" >3.42483</td>
      <td id="T_721ed_row61_col4" class="data row61 col4" >52.492</td>
      <td id="T_721ed_row61_col5" class="data row61 col5" >52.6017</td>
      <td id="T_721ed_row61_col6" class="data row61 col6" >0.0652448</td>
      <td id="T_721ed_row61_col7" class="data row61 col7" >0.997878</td>
      <td id="T_721ed_row61_col8" class="data row61 col8" >0.0284021</td>
      <td id="T_721ed_row61_col9" class="data row61 col9" >4443.78</td>
      <td id="T_721ed_row61_col10" class="data row61 col10" >1</td>
      <td id="T_721ed_row61_col11" class="data row61 col11" >3145728</td>
      <td id="T_721ed_row61_col12" class="data row61 col12" >torch.float32</td>
      <td id="T_721ed_row61_col13" class="data row61 col13" >(1024, 3072)</td>
      <td id="T_721ed_row61_col14" class="data row61 col14" >16</td>
      <td id="T_721ed_row61_col15" class="data row61 col15" >16.000000</td>
      <td id="T_721ed_row61_col16" class="data row61 col16" >1.000000</td>
      <td id="T_721ed_row61_col17" class="data row61 col17" >llama.model.layers.19.self_attn.k_proj.lora_B.default.weight</td>
      <td id="T_721ed_row61_col18" class="data row61 col18" >llama.model.layers.19.self_attn.k_proj.lora_A.default.weight</td>
      <td id="T_721ed_row61_col19" class="data row61 col19" >nan</td>
    </tr>
    <tr>
      <th id="T_721ed_level0_row62" class="row_heading level0 row62" >62</th>
      <td id="T_721ed_row62_col0" class="data row62 col0" >model.layers.27.self_attn.v_proj.weight</td>
      <td id="T_721ed_row62_col1" class="data row62 col1" >model.layers.27.self_attn.v_proj.weight</td>
      <td id="T_721ed_row62_col2" class="data row62 col2" >llama.model.layers.27.self_attn.v_proj</td>
      <td id="T_721ed_row62_col3" class="data row62 col3" >3.41067</td>
      <td id="T_721ed_row62_col4" class="data row62 col4" >35.4355</td>
      <td id="T_721ed_row62_col5" class="data row62 col5" >35.6028</td>
      <td id="T_721ed_row62_col6" class="data row62 col6" >0.09625</td>
      <td id="T_721ed_row62_col7" class="data row62 col7" >0.995401</td>
      <td id="T_721ed_row62_col8" class="data row62 col8" >0.015142</td>
      <td id="T_721ed_row62_col9" class="data row62 col9" >4687.16</td>
      <td id="T_721ed_row62_col10" class="data row62 col10" >1</td>
      <td id="T_721ed_row62_col11" class="data row62 col11" >3145728</td>
      <td id="T_721ed_row62_col12" class="data row62 col12" >torch.float32</td>
      <td id="T_721ed_row62_col13" class="data row62 col13" >(1024, 3072)</td>
      <td id="T_721ed_row62_col14" class="data row62 col14" >16</td>
      <td id="T_721ed_row62_col15" class="data row62 col15" >16.000000</td>
      <td id="T_721ed_row62_col16" class="data row62 col16" >1.000000</td>
      <td id="T_721ed_row62_col17" class="data row62 col17" >llama.model.layers.27.self_attn.v_proj.lora_B.default.weight</td>
      <td id="T_721ed_row62_col18" class="data row62 col18" >llama.model.layers.27.self_attn.v_proj.lora_A.default.weight</td>
      <td id="T_721ed_row62_col19" class="data row62 col19" >nan</td>
    </tr>
    <tr>
      <th id="T_721ed_level0_row63" class="row_heading level0 row63" >63</th>
      <td id="T_721ed_row63_col0" class="data row63 col0" >model.layers.17.self_attn.k_proj.weight</td>
      <td id="T_721ed_row63_col1" class="data row63 col1" >model.layers.17.self_attn.k_proj.weight</td>
      <td id="T_721ed_row63_col2" class="data row63 col2" >llama.model.layers.17.self_attn.k_proj</td>
      <td id="T_721ed_row63_col3" class="data row63 col3" >3.39119</td>
      <td id="T_721ed_row63_col4" class="data row63 col4" >53.4052</td>
      <td id="T_721ed_row63_col5" class="data row63 col5" >53.5131</td>
      <td id="T_721ed_row63_col6" class="data row63 col6" >0.0634993</td>
      <td id="T_721ed_row63_col7" class="data row63 col7" >0.99799</td>
      <td id="T_721ed_row63_col8" class="data row63 col8" >0.0200278</td>
      <td id="T_721ed_row63_col9" class="data row63 col9" >4390.47</td>
      <td id="T_721ed_row63_col10" class="data row63 col10" >1</td>
      <td id="T_721ed_row63_col11" class="data row63 col11" >3145728</td>
      <td id="T_721ed_row63_col12" class="data row63 col12" >torch.float32</td>
      <td id="T_721ed_row63_col13" class="data row63 col13" >(1024, 3072)</td>
      <td id="T_721ed_row63_col14" class="data row63 col14" >16</td>
      <td id="T_721ed_row63_col15" class="data row63 col15" >16.000000</td>
      <td id="T_721ed_row63_col16" class="data row63 col16" >1.000000</td>
      <td id="T_721ed_row63_col17" class="data row63 col17" >llama.model.layers.17.self_attn.k_proj.lora_B.default.weight</td>
      <td id="T_721ed_row63_col18" class="data row63 col18" >llama.model.layers.17.self_attn.k_proj.lora_A.default.weight</td>
      <td id="T_721ed_row63_col19" class="data row63 col19" >nan</td>
    </tr>
    <tr>
      <th id="T_721ed_level0_row64" class="row_heading level0 row64" >64</th>
      <td id="T_721ed_row64_col0" class="data row64 col0" >model.layers.26.self_attn.v_proj.weight</td>
      <td id="T_721ed_row64_col1" class="data row64 col1" >model.layers.26.self_attn.v_proj.weight</td>
      <td id="T_721ed_row64_col2" class="data row64 col2" >llama.model.layers.26.self_attn.v_proj</td>
      <td id="T_721ed_row64_col3" class="data row64 col3" >3.38821</td>
      <td id="T_721ed_row64_col4" class="data row64 col4" >40.4075</td>
      <td id="T_721ed_row64_col5" class="data row64 col5" >40.5449</td>
      <td id="T_721ed_row64_col6" class="data row64 col6" >0.0838509</td>
      <td id="T_721ed_row64_col7" class="data row64 col7" >0.996502</td>
      <td id="T_721ed_row64_col8" class="data row64 col8" >0.0179836</td>
      <td id="T_721ed_row64_col9" class="data row64 col9" >4630.07</td>
      <td id="T_721ed_row64_col10" class="data row64 col10" >1</td>
      <td id="T_721ed_row64_col11" class="data row64 col11" >3145728</td>
      <td id="T_721ed_row64_col12" class="data row64 col12" >torch.float32</td>
      <td id="T_721ed_row64_col13" class="data row64 col13" >(1024, 3072)</td>
      <td id="T_721ed_row64_col14" class="data row64 col14" >16</td>
      <td id="T_721ed_row64_col15" class="data row64 col15" >16.000000</td>
      <td id="T_721ed_row64_col16" class="data row64 col16" >1.000000</td>
      <td id="T_721ed_row64_col17" class="data row64 col17" >llama.model.layers.26.self_attn.v_proj.lora_B.default.weight</td>
      <td id="T_721ed_row64_col18" class="data row64 col18" >llama.model.layers.26.self_attn.v_proj.lora_A.default.weight</td>
      <td id="T_721ed_row64_col19" class="data row64 col19" >nan</td>
    </tr>
    <tr>
      <th id="T_721ed_level0_row65" class="row_heading level0 row65" >65</th>
      <td id="T_721ed_row65_col0" class="data row65 col0" >model.layers.25.self_attn.k_proj.weight</td>
      <td id="T_721ed_row65_col1" class="data row65 col1" >model.layers.25.self_attn.k_proj.weight</td>
      <td id="T_721ed_row65_col2" class="data row65 col2" >llama.model.layers.25.self_attn.k_proj</td>
      <td id="T_721ed_row65_col3" class="data row65 col3" >3.382</td>
      <td id="T_721ed_row65_col4" class="data row65 col4" >48.1312</td>
      <td id="T_721ed_row65_col5" class="data row65 col5" >48.2483</td>
      <td id="T_721ed_row65_col6" class="data row65 col6" >0.0702662</td>
      <td id="T_721ed_row65_col7" class="data row65 col7" >0.99754</td>
      <td id="T_721ed_row65_col8" class="data row65 col8" >0.022551</td>
      <td id="T_721ed_row65_col9" class="data row65 col9" >4448.4</td>
      <td id="T_721ed_row65_col10" class="data row65 col10" >1</td>
      <td id="T_721ed_row65_col11" class="data row65 col11" >3145728</td>
      <td id="T_721ed_row65_col12" class="data row65 col12" >torch.float32</td>
      <td id="T_721ed_row65_col13" class="data row65 col13" >(1024, 3072)</td>
      <td id="T_721ed_row65_col14" class="data row65 col14" >16</td>
      <td id="T_721ed_row65_col15" class="data row65 col15" >16.000000</td>
      <td id="T_721ed_row65_col16" class="data row65 col16" >1.000000</td>
      <td id="T_721ed_row65_col17" class="data row65 col17" >llama.model.layers.25.self_attn.k_proj.lora_B.default.weight</td>
      <td id="T_721ed_row65_col18" class="data row65 col18" >llama.model.layers.25.self_attn.k_proj.lora_A.default.weight</td>
      <td id="T_721ed_row65_col19" class="data row65 col19" >nan</td>
    </tr>
    <tr>
      <th id="T_721ed_level0_row66" class="row_heading level0 row66" >66</th>
      <td id="T_721ed_row66_col0" class="data row66 col0" >model.layers.18.self_attn.k_proj.weight</td>
      <td id="T_721ed_row66_col1" class="data row66 col1" >model.layers.18.self_attn.k_proj.weight</td>
      <td id="T_721ed_row66_col2" class="data row66 col2" >llama.model.layers.18.self_attn.k_proj</td>
      <td id="T_721ed_row66_col3" class="data row66 col3" >3.38186</td>
      <td id="T_721ed_row66_col4" class="data row66 col4" >52.5219</td>
      <td id="T_721ed_row66_col5" class="data row66 col5" >52.625</td>
      <td id="T_721ed_row66_col6" class="data row66 col6" >0.0643895</td>
      <td id="T_721ed_row66_col7" class="data row66 col7" >0.997933</td>
      <td id="T_721ed_row66_col8" class="data row66 col8" >0.0185424</td>
      <td id="T_721ed_row66_col9" class="data row66 col9" >4435.71</td>
      <td id="T_721ed_row66_col10" class="data row66 col10" >1</td>
      <td id="T_721ed_row66_col11" class="data row66 col11" >3145728</td>
      <td id="T_721ed_row66_col12" class="data row66 col12" >torch.float32</td>
      <td id="T_721ed_row66_col13" class="data row66 col13" >(1024, 3072)</td>
      <td id="T_721ed_row66_col14" class="data row66 col14" >16</td>
      <td id="T_721ed_row66_col15" class="data row66 col15" >16.000000</td>
      <td id="T_721ed_row66_col16" class="data row66 col16" >1.000000</td>
      <td id="T_721ed_row66_col17" class="data row66 col17" >llama.model.layers.18.self_attn.k_proj.lora_B.default.weight</td>
      <td id="T_721ed_row66_col18" class="data row66 col18" >llama.model.layers.18.self_attn.k_proj.lora_A.default.weight</td>
      <td id="T_721ed_row66_col19" class="data row66 col19" >nan</td>
    </tr>
    <tr>
      <th id="T_721ed_level0_row67" class="row_heading level0 row67" >67</th>
      <td id="T_721ed_row67_col0" class="data row67 col0" >model.layers.9.self_attn.o_proj.weight</td>
      <td id="T_721ed_row67_col1" class="data row67 col1" >model.layers.9.self_attn.o_proj.weight</td>
      <td id="T_721ed_row67_col2" class="data row67 col2" >llama.model.layers.9.self_attn.o_proj</td>
      <td id="T_721ed_row67_col3" class="data row67 col3" >3.36016</td>
      <td id="T_721ed_row67_col4" class="data row67 col4" >47.4526</td>
      <td id="T_721ed_row67_col5" class="data row67 col5" >47.5717</td>
      <td id="T_721ed_row67_col6" class="data row67 col6" >0.0708109</td>
      <td id="T_721ed_row67_col7" class="data row67 col7" >0.997502</td>
      <td id="T_721ed_row67_col8" class="data row67 col8" >0.013936</td>
      <td id="T_721ed_row67_col9" class="data row67 col9" >7843.05</td>
      <td id="T_721ed_row67_col10" class="data row67 col10" >1</td>
      <td id="T_721ed_row67_col11" class="data row67 col11" >9437184</td>
      <td id="T_721ed_row67_col12" class="data row67 col12" >torch.float32</td>
      <td id="T_721ed_row67_col13" class="data row67 col13" >(3072, 3072)</td>
      <td id="T_721ed_row67_col14" class="data row67 col14" >16</td>
      <td id="T_721ed_row67_col15" class="data row67 col15" >16.000000</td>
      <td id="T_721ed_row67_col16" class="data row67 col16" >1.000000</td>
      <td id="T_721ed_row67_col17" class="data row67 col17" >llama.model.layers.9.self_attn.o_proj.lora_B.default.weight</td>
      <td id="T_721ed_row67_col18" class="data row67 col18" >llama.model.layers.9.self_attn.o_proj.lora_A.default.weight</td>
      <td id="T_721ed_row67_col19" class="data row67 col19" >nan</td>
    </tr>
    <tr>
      <th id="T_721ed_level0_row68" class="row_heading level0 row68" >68</th>
      <td id="T_721ed_row68_col0" class="data row68 col0" >model.layers.15.self_attn.k_proj.weight</td>
      <td id="T_721ed_row68_col1" class="data row68 col1" >model.layers.15.self_attn.k_proj.weight</td>
      <td id="T_721ed_row68_col2" class="data row68 col2" >llama.model.layers.15.self_attn.k_proj</td>
      <td id="T_721ed_row68_col3" class="data row68 col3" >3.32899</td>
      <td id="T_721ed_row68_col4" class="data row68 col4" >54.1376</td>
      <td id="T_721ed_row68_col5" class="data row68 col5" >54.2429</td>
      <td id="T_721ed_row68_col6" class="data row68 col6" >0.0614913</td>
      <td id="T_721ed_row68_col7" class="data row68 col7" >0.998115</td>
      <td id="T_721ed_row68_col8" class="data row68 col8" >0.0217712</td>
      <td id="T_721ed_row68_col9" class="data row68 col9" >4304.22</td>
      <td id="T_721ed_row68_col10" class="data row68 col10" >1</td>
      <td id="T_721ed_row68_col11" class="data row68 col11" >3145728</td>
      <td id="T_721ed_row68_col12" class="data row68 col12" >torch.float32</td>
      <td id="T_721ed_row68_col13" class="data row68 col13" >(1024, 3072)</td>
      <td id="T_721ed_row68_col14" class="data row68 col14" >16</td>
      <td id="T_721ed_row68_col15" class="data row68 col15" >16.000000</td>
      <td id="T_721ed_row68_col16" class="data row68 col16" >1.000000</td>
      <td id="T_721ed_row68_col17" class="data row68 col17" >llama.model.layers.15.self_attn.k_proj.lora_B.default.weight</td>
      <td id="T_721ed_row68_col18" class="data row68 col18" >llama.model.layers.15.self_attn.k_proj.lora_A.default.weight</td>
      <td id="T_721ed_row68_col19" class="data row68 col19" >nan</td>
    </tr>
    <tr>
      <th id="T_721ed_level0_row69" class="row_heading level0 row69" >69</th>
      <td id="T_721ed_row69_col0" class="data row69 col0" >model.layers.25.self_attn.v_proj.weight</td>
      <td id="T_721ed_row69_col1" class="data row69 col1" >model.layers.25.self_attn.v_proj.weight</td>
      <td id="T_721ed_row69_col2" class="data row69 col2" >llama.model.layers.25.self_attn.v_proj</td>
      <td id="T_721ed_row69_col3" class="data row69 col3" >3.31715</td>
      <td id="T_721ed_row69_col4" class="data row69 col4" >35.382</td>
      <td id="T_721ed_row69_col5" class="data row69 col5" >35.5368</td>
      <td id="T_721ed_row69_col6" class="data row69 col6" >0.0937525</td>
      <td id="T_721ed_row69_col7" class="data row69 col7" >0.995634</td>
      <td id="T_721ed_row69_col8" class="data row69 col8" >0.0166401</td>
      <td id="T_721ed_row69_col9" class="data row69 col9" >4538.82</td>
      <td id="T_721ed_row69_col10" class="data row69 col10" >1</td>
      <td id="T_721ed_row69_col11" class="data row69 col11" >3145728</td>
      <td id="T_721ed_row69_col12" class="data row69 col12" >torch.float32</td>
      <td id="T_721ed_row69_col13" class="data row69 col13" >(1024, 3072)</td>
      <td id="T_721ed_row69_col14" class="data row69 col14" >16</td>
      <td id="T_721ed_row69_col15" class="data row69 col15" >16.000000</td>
      <td id="T_721ed_row69_col16" class="data row69 col16" >1.000000</td>
      <td id="T_721ed_row69_col17" class="data row69 col17" >llama.model.layers.25.self_attn.v_proj.lora_B.default.weight</td>
      <td id="T_721ed_row69_col18" class="data row69 col18" >llama.model.layers.25.self_attn.v_proj.lora_A.default.weight</td>
      <td id="T_721ed_row69_col19" class="data row69 col19" >nan</td>
    </tr>
    <tr>
      <th id="T_721ed_level0_row70" class="row_heading level0 row70" >70</th>
      <td id="T_721ed_row70_col0" class="data row70 col0" >model.layers.12.self_attn.o_proj.weight</td>
      <td id="T_721ed_row70_col1" class="data row70 col1" >model.layers.12.self_attn.o_proj.weight</td>
      <td id="T_721ed_row70_col2" class="data row70 col2" >llama.model.layers.12.self_attn.o_proj</td>
      <td id="T_721ed_row70_col3" class="data row70 col3" >3.29568</td>
      <td id="T_721ed_row70_col4" class="data row70 col4" >45.1866</td>
      <td id="T_721ed_row70_col5" class="data row70 col5" >45.3066</td>
      <td id="T_721ed_row70_col6" class="data row70 col6" >0.0729349</td>
      <td id="T_721ed_row70_col7" class="data row70 col7" >0.997351</td>
      <td id="T_721ed_row70_col8" class="data row70 col8" >0.0158934</td>
      <td id="T_721ed_row70_col9" class="data row70 col9" >7689.06</td>
      <td id="T_721ed_row70_col10" class="data row70 col10" >1</td>
      <td id="T_721ed_row70_col11" class="data row70 col11" >9437184</td>
      <td id="T_721ed_row70_col12" class="data row70 col12" >torch.float32</td>
      <td id="T_721ed_row70_col13" class="data row70 col13" >(3072, 3072)</td>
      <td id="T_721ed_row70_col14" class="data row70 col14" >16</td>
      <td id="T_721ed_row70_col15" class="data row70 col15" >16.000000</td>
      <td id="T_721ed_row70_col16" class="data row70 col16" >1.000000</td>
      <td id="T_721ed_row70_col17" class="data row70 col17" >llama.model.layers.12.self_attn.o_proj.lora_B.default.weight</td>
      <td id="T_721ed_row70_col18" class="data row70 col18" >llama.model.layers.12.self_attn.o_proj.lora_A.default.weight</td>
      <td id="T_721ed_row70_col19" class="data row70 col19" >nan</td>
    </tr>
    <tr>
      <th id="T_721ed_level0_row71" class="row_heading level0 row71" >71</th>
      <td id="T_721ed_row71_col0" class="data row71 col0" >model.layers.1.self_attn.o_proj.weight</td>
      <td id="T_721ed_row71_col1" class="data row71 col1" >model.layers.1.self_attn.o_proj.weight</td>
      <td id="T_721ed_row71_col2" class="data row71 col2" >llama.model.layers.1.self_attn.o_proj</td>
      <td id="T_721ed_row71_col3" class="data row71 col3" >3.29292</td>
      <td id="T_721ed_row71_col4" class="data row71 col4" >43.1644</td>
      <td id="T_721ed_row71_col5" class="data row71 col5" >43.2917</td>
      <td id="T_721ed_row71_col6" class="data row71 col6" >0.0762878</td>
      <td id="T_721ed_row71_col7" class="data row71 col7" >0.997103</td>
      <td id="T_721ed_row71_col8" class="data row71 col8" >0.020778</td>
      <td id="T_721ed_row71_col9" class="data row71 col9" >7823.87</td>
      <td id="T_721ed_row71_col10" class="data row71 col10" >1</td>
      <td id="T_721ed_row71_col11" class="data row71 col11" >9437184</td>
      <td id="T_721ed_row71_col12" class="data row71 col12" >torch.float32</td>
      <td id="T_721ed_row71_col13" class="data row71 col13" >(3072, 3072)</td>
      <td id="T_721ed_row71_col14" class="data row71 col14" >16</td>
      <td id="T_721ed_row71_col15" class="data row71 col15" >16.000000</td>
      <td id="T_721ed_row71_col16" class="data row71 col16" >1.000000</td>
      <td id="T_721ed_row71_col17" class="data row71 col17" >llama.model.layers.1.self_attn.o_proj.lora_B.default.weight</td>
      <td id="T_721ed_row71_col18" class="data row71 col18" >llama.model.layers.1.self_attn.o_proj.lora_A.default.weight</td>
      <td id="T_721ed_row71_col19" class="data row71 col19" >nan</td>
    </tr>
    <tr>
      <th id="T_721ed_level0_row72" class="row_heading level0 row72" >72</th>
      <td id="T_721ed_row72_col0" class="data row72 col0" >model.layers.0.self_attn.o_proj.weight</td>
      <td id="T_721ed_row72_col1" class="data row72 col1" >model.layers.0.self_attn.o_proj.weight</td>
      <td id="T_721ed_row72_col2" class="data row72 col2" >llama.model.layers.0.self_attn.o_proj</td>
      <td id="T_721ed_row72_col3" class="data row72 col3" >3.29226</td>
      <td id="T_721ed_row72_col4" class="data row72 col4" >37.5543</td>
      <td id="T_721ed_row72_col5" class="data row72 col5" >37.6981</td>
      <td id="T_721ed_row72_col6" class="data row72 col6" >0.0876668</td>
      <td id="T_721ed_row72_col7" class="data row72 col7" >0.996179</td>
      <td id="T_721ed_row72_col8" class="data row72 col8" >0.0149092</td>
      <td id="T_721ed_row72_col9" class="data row72 col9" >7740.98</td>
      <td id="T_721ed_row72_col10" class="data row72 col10" >1</td>
      <td id="T_721ed_row72_col11" class="data row72 col11" >9437184</td>
      <td id="T_721ed_row72_col12" class="data row72 col12" >torch.float32</td>
      <td id="T_721ed_row72_col13" class="data row72 col13" >(3072, 3072)</td>
      <td id="T_721ed_row72_col14" class="data row72 col14" >16</td>
      <td id="T_721ed_row72_col15" class="data row72 col15" >16.000000</td>
      <td id="T_721ed_row72_col16" class="data row72 col16" >1.000000</td>
      <td id="T_721ed_row72_col17" class="data row72 col17" >llama.model.layers.0.self_attn.o_proj.lora_B.default.weight</td>
      <td id="T_721ed_row72_col18" class="data row72 col18" >llama.model.layers.0.self_attn.o_proj.lora_A.default.weight</td>
      <td id="T_721ed_row72_col19" class="data row72 col19" >nan</td>
    </tr>
    <tr>
      <th id="T_721ed_level0_row73" class="row_heading level0 row73" >73</th>
      <td id="T_721ed_row73_col0" class="data row73 col0" >model.layers.8.self_attn.o_proj.weight</td>
      <td id="T_721ed_row73_col1" class="data row73 col1" >model.layers.8.self_attn.o_proj.weight</td>
      <td id="T_721ed_row73_col2" class="data row73 col2" >llama.model.layers.8.self_attn.o_proj</td>
      <td id="T_721ed_row73_col3" class="data row73 col3" >3.25259</td>
      <td id="T_721ed_row73_col4" class="data row73 col4" >44.7133</td>
      <td id="T_721ed_row73_col5" class="data row73 col5" >44.8307</td>
      <td id="T_721ed_row73_col6" class="data row73 col6" >0.0727432</td>
      <td id="T_721ed_row73_col7" class="data row73 col7" >0.997365</td>
      <td id="T_721ed_row73_col8" class="data row73 col8" >0.0145317</td>
      <td id="T_721ed_row73_col9" class="data row73 col9" >7632.86</td>
      <td id="T_721ed_row73_col10" class="data row73 col10" >1</td>
      <td id="T_721ed_row73_col11" class="data row73 col11" >9437184</td>
      <td id="T_721ed_row73_col12" class="data row73 col12" >torch.float32</td>
      <td id="T_721ed_row73_col13" class="data row73 col13" >(3072, 3072)</td>
      <td id="T_721ed_row73_col14" class="data row73 col14" >16</td>
      <td id="T_721ed_row73_col15" class="data row73 col15" >16.000000</td>
      <td id="T_721ed_row73_col16" class="data row73 col16" >1.000000</td>
      <td id="T_721ed_row73_col17" class="data row73 col17" >llama.model.layers.8.self_attn.o_proj.lora_B.default.weight</td>
      <td id="T_721ed_row73_col18" class="data row73 col18" >llama.model.layers.8.self_attn.o_proj.lora_A.default.weight</td>
      <td id="T_721ed_row73_col19" class="data row73 col19" >nan</td>
    </tr>
    <tr>
      <th id="T_721ed_level0_row74" class="row_heading level0 row74" >74</th>
      <td id="T_721ed_row74_col0" class="data row74 col0" >model.layers.23.self_attn.v_proj.weight</td>
      <td id="T_721ed_row74_col1" class="data row74 col1" >model.layers.23.self_attn.v_proj.weight</td>
      <td id="T_721ed_row74_col2" class="data row74 col2" >llama.model.layers.23.self_attn.v_proj</td>
      <td id="T_721ed_row74_col3" class="data row74 col3" >3.21941</td>
      <td id="T_721ed_row74_col4" class="data row74 col4" >32.9944</td>
      <td id="T_721ed_row74_col5" class="data row74 col5" >33.1514</td>
      <td id="T_721ed_row74_col6" class="data row74 col6" >0.0975746</td>
      <td id="T_721ed_row74_col7" class="data row74 col7" >0.995273</td>
      <td id="T_721ed_row74_col8" class="data row74 col8" >0.0171923</td>
      <td id="T_721ed_row74_col9" class="data row74 col9" >4401.74</td>
      <td id="T_721ed_row74_col10" class="data row74 col10" >1</td>
      <td id="T_721ed_row74_col11" class="data row74 col11" >3145728</td>
      <td id="T_721ed_row74_col12" class="data row74 col12" >torch.float32</td>
      <td id="T_721ed_row74_col13" class="data row74 col13" >(1024, 3072)</td>
      <td id="T_721ed_row74_col14" class="data row74 col14" >16</td>
      <td id="T_721ed_row74_col15" class="data row74 col15" >16.000000</td>
      <td id="T_721ed_row74_col16" class="data row74 col16" >1.000000</td>
      <td id="T_721ed_row74_col17" class="data row74 col17" >llama.model.layers.23.self_attn.v_proj.lora_B.default.weight</td>
      <td id="T_721ed_row74_col18" class="data row74 col18" >llama.model.layers.23.self_attn.v_proj.lora_A.default.weight</td>
      <td id="T_721ed_row74_col19" class="data row74 col19" >nan</td>
    </tr>
    <tr>
      <th id="T_721ed_level0_row75" class="row_heading level0 row75" >75</th>
      <td id="T_721ed_row75_col0" class="data row75 col0" >model.layers.13.self_attn.o_proj.weight</td>
      <td id="T_721ed_row75_col1" class="data row75 col1" >model.layers.13.self_attn.o_proj.weight</td>
      <td id="T_721ed_row75_col2" class="data row75 col2" >llama.model.layers.13.self_attn.o_proj</td>
      <td id="T_721ed_row75_col3" class="data row75 col3" >3.1943</td>
      <td id="T_721ed_row75_col4" class="data row75 col4" >46.0997</td>
      <td id="T_721ed_row75_col5" class="data row75 col5" >46.2049</td>
      <td id="T_721ed_row75_col6" class="data row75 col6" >0.0692912</td>
      <td id="T_721ed_row75_col7" class="data row75 col7" >0.997607</td>
      <td id="T_721ed_row75_col8" class="data row75 col8" >0.014246</td>
      <td id="T_721ed_row75_col9" class="data row75 col9" >7548.17</td>
      <td id="T_721ed_row75_col10" class="data row75 col10" >1</td>
      <td id="T_721ed_row75_col11" class="data row75 col11" >9437184</td>
      <td id="T_721ed_row75_col12" class="data row75 col12" >torch.float32</td>
      <td id="T_721ed_row75_col13" class="data row75 col13" >(3072, 3072)</td>
      <td id="T_721ed_row75_col14" class="data row75 col14" >16</td>
      <td id="T_721ed_row75_col15" class="data row75 col15" >16.000000</td>
      <td id="T_721ed_row75_col16" class="data row75 col16" >1.000000</td>
      <td id="T_721ed_row75_col17" class="data row75 col17" >llama.model.layers.13.self_attn.o_proj.lora_B.default.weight</td>
      <td id="T_721ed_row75_col18" class="data row75 col18" >llama.model.layers.13.self_attn.o_proj.lora_A.default.weight</td>
      <td id="T_721ed_row75_col19" class="data row75 col19" >nan</td>
    </tr>
    <tr>
      <th id="T_721ed_level0_row76" class="row_heading level0 row76" >76</th>
      <td id="T_721ed_row76_col0" class="data row76 col0" >model.layers.13.self_attn.k_proj.weight</td>
      <td id="T_721ed_row76_col1" class="data row76 col1" >model.layers.13.self_attn.k_proj.weight</td>
      <td id="T_721ed_row76_col2" class="data row76 col2" >llama.model.layers.13.self_attn.k_proj</td>
      <td id="T_721ed_row76_col3" class="data row76 col3" >3.14688</td>
      <td id="T_721ed_row76_col4" class="data row76 col4" >56.2583</td>
      <td id="T_721ed_row76_col5" class="data row76 col5" >56.3512</td>
      <td id="T_721ed_row76_col6" class="data row76 col6" >0.0559364</td>
      <td id="T_721ed_row76_col7" class="data row76 col7" >0.99844</td>
      <td id="T_721ed_row76_col8" class="data row76 col8" >0.0220464</td>
      <td id="T_721ed_row76_col9" class="data row76 col9" >3951.25</td>
      <td id="T_721ed_row76_col10" class="data row76 col10" >1</td>
      <td id="T_721ed_row76_col11" class="data row76 col11" >3145728</td>
      <td id="T_721ed_row76_col12" class="data row76 col12" >torch.float32</td>
      <td id="T_721ed_row76_col13" class="data row76 col13" >(1024, 3072)</td>
      <td id="T_721ed_row76_col14" class="data row76 col14" >16</td>
      <td id="T_721ed_row76_col15" class="data row76 col15" >16.000000</td>
      <td id="T_721ed_row76_col16" class="data row76 col16" >1.000000</td>
      <td id="T_721ed_row76_col17" class="data row76 col17" >llama.model.layers.13.self_attn.k_proj.lora_B.default.weight</td>
      <td id="T_721ed_row76_col18" class="data row76 col18" >llama.model.layers.13.self_attn.k_proj.lora_A.default.weight</td>
      <td id="T_721ed_row76_col19" class="data row76 col19" >nan</td>
    </tr>
    <tr>
      <th id="T_721ed_level0_row77" class="row_heading level0 row77" >77</th>
      <td id="T_721ed_row77_col0" class="data row77 col0" >model.layers.10.self_attn.o_proj.weight</td>
      <td id="T_721ed_row77_col1" class="data row77 col1" >model.layers.10.self_attn.o_proj.weight</td>
      <td id="T_721ed_row77_col2" class="data row77 col2" >llama.model.layers.10.self_attn.o_proj</td>
      <td id="T_721ed_row77_col3" class="data row77 col3" >3.13568</td>
      <td id="T_721ed_row77_col4" class="data row77 col4" >44.177</td>
      <td id="T_721ed_row77_col5" class="data row77 col5" >44.2868</td>
      <td id="T_721ed_row77_col6" class="data row77 col6" >0.07098</td>
      <td id="T_721ed_row77_col7" class="data row77 col7" >0.99749</td>
      <td id="T_721ed_row77_col8" class="data row77 col8" >0.0179735</td>
      <td id="T_721ed_row77_col9" class="data row77 col9" >7277.37</td>
      <td id="T_721ed_row77_col10" class="data row77 col10" >1</td>
      <td id="T_721ed_row77_col11" class="data row77 col11" >9437184</td>
      <td id="T_721ed_row77_col12" class="data row77 col12" >torch.float32</td>
      <td id="T_721ed_row77_col13" class="data row77 col13" >(3072, 3072)</td>
      <td id="T_721ed_row77_col14" class="data row77 col14" >16</td>
      <td id="T_721ed_row77_col15" class="data row77 col15" >16.000000</td>
      <td id="T_721ed_row77_col16" class="data row77 col16" >1.000000</td>
      <td id="T_721ed_row77_col17" class="data row77 col17" >llama.model.layers.10.self_attn.o_proj.lora_B.default.weight</td>
      <td id="T_721ed_row77_col18" class="data row77 col18" >llama.model.layers.10.self_attn.o_proj.lora_A.default.weight</td>
      <td id="T_721ed_row77_col19" class="data row77 col19" >nan</td>
    </tr>
    <tr>
      <th id="T_721ed_level0_row78" class="row_heading level0 row78" >78</th>
      <td id="T_721ed_row78_col0" class="data row78 col0" >model.layers.7.self_attn.o_proj.weight</td>
      <td id="T_721ed_row78_col1" class="data row78 col1" >model.layers.7.self_attn.o_proj.weight</td>
      <td id="T_721ed_row78_col2" class="data row78 col2" >llama.model.layers.7.self_attn.o_proj</td>
      <td id="T_721ed_row78_col3" class="data row78 col3" >3.09999</td>
      <td id="T_721ed_row78_col4" class="data row78 col4" >44.6235</td>
      <td id="T_721ed_row78_col5" class="data row78 col5" >44.7323</td>
      <td id="T_721ed_row78_col6" class="data row78 col6" >0.0694699</td>
      <td id="T_721ed_row78_col7" class="data row78 col7" >0.997596</td>
      <td id="T_721ed_row78_col8" class="data row78 col8" >0.0146254</td>
      <td id="T_721ed_row78_col9" class="data row78 col9" >7268.71</td>
      <td id="T_721ed_row78_col10" class="data row78 col10" >1</td>
      <td id="T_721ed_row78_col11" class="data row78 col11" >9437184</td>
      <td id="T_721ed_row78_col12" class="data row78 col12" >torch.float32</td>
      <td id="T_721ed_row78_col13" class="data row78 col13" >(3072, 3072)</td>
      <td id="T_721ed_row78_col14" class="data row78 col14" >16</td>
      <td id="T_721ed_row78_col15" class="data row78 col15" >16.000000</td>
      <td id="T_721ed_row78_col16" class="data row78 col16" >1.000000</td>
      <td id="T_721ed_row78_col17" class="data row78 col17" >llama.model.layers.7.self_attn.o_proj.lora_B.default.weight</td>
      <td id="T_721ed_row78_col18" class="data row78 col18" >llama.model.layers.7.self_attn.o_proj.lora_A.default.weight</td>
      <td id="T_721ed_row78_col19" class="data row78 col19" >nan</td>
    </tr>
    <tr>
      <th id="T_721ed_level0_row79" class="row_heading level0 row79" >79</th>
      <td id="T_721ed_row79_col0" class="data row79 col0" >model.layers.11.self_attn.o_proj.weight</td>
      <td id="T_721ed_row79_col1" class="data row79 col1" >model.layers.11.self_attn.o_proj.weight</td>
      <td id="T_721ed_row79_col2" class="data row79 col2" >llama.model.layers.11.self_attn.o_proj</td>
      <td id="T_721ed_row79_col3" class="data row79 col3" >3.09603</td>
      <td id="T_721ed_row79_col4" class="data row79 col4" >47.7475</td>
      <td id="T_721ed_row79_col5" class="data row79 col5" >47.8459</td>
      <td id="T_721ed_row79_col6" class="data row79 col6" >0.0648418</td>
      <td id="T_721ed_row79_col7" class="data row79 col7" >0.997904</td>
      <td id="T_721ed_row79_col8" class="data row79 col8" >0.0145802</td>
      <td id="T_721ed_row79_col9" class="data row79 col9" >7295.84</td>
      <td id="T_721ed_row79_col10" class="data row79 col10" >1</td>
      <td id="T_721ed_row79_col11" class="data row79 col11" >9437184</td>
      <td id="T_721ed_row79_col12" class="data row79 col12" >torch.float32</td>
      <td id="T_721ed_row79_col13" class="data row79 col13" >(3072, 3072)</td>
      <td id="T_721ed_row79_col14" class="data row79 col14" >16</td>
      <td id="T_721ed_row79_col15" class="data row79 col15" >16.000000</td>
      <td id="T_721ed_row79_col16" class="data row79 col16" >1.000000</td>
      <td id="T_721ed_row79_col17" class="data row79 col17" >llama.model.layers.11.self_attn.o_proj.lora_B.default.weight</td>
      <td id="T_721ed_row79_col18" class="data row79 col18" >llama.model.layers.11.self_attn.o_proj.lora_A.default.weight</td>
      <td id="T_721ed_row79_col19" class="data row79 col19" >nan</td>
    </tr>
    <tr>
      <th id="T_721ed_level0_row80" class="row_heading level0 row80" >80</th>
      <td id="T_721ed_row80_col0" class="data row80 col0" >model.layers.14.self_attn.k_proj.weight</td>
      <td id="T_721ed_row80_col1" class="data row80 col1" >model.layers.14.self_attn.k_proj.weight</td>
      <td id="T_721ed_row80_col2" class="data row80 col2" >llama.model.layers.14.self_attn.k_proj</td>
      <td id="T_721ed_row80_col3" class="data row80 col3" >3.07362</td>
      <td id="T_721ed_row80_col4" class="data row80 col4" >53.7351</td>
      <td id="T_721ed_row80_col5" class="data row80 col5" >53.8246</td>
      <td id="T_721ed_row80_col6" class="data row80 col6" >0.0571995</td>
      <td id="T_721ed_row80_col7" class="data row80 col7" >0.998368</td>
      <td id="T_721ed_row80_col8" class="data row80 col8" >0.0212996</td>
      <td id="T_721ed_row80_col9" class="data row80 col9" >4045.09</td>
      <td id="T_721ed_row80_col10" class="data row80 col10" >1</td>
      <td id="T_721ed_row80_col11" class="data row80 col11" >3145728</td>
      <td id="T_721ed_row80_col12" class="data row80 col12" >torch.float32</td>
      <td id="T_721ed_row80_col13" class="data row80 col13" >(1024, 3072)</td>
      <td id="T_721ed_row80_col14" class="data row80 col14" >16</td>
      <td id="T_721ed_row80_col15" class="data row80 col15" >16.000000</td>
      <td id="T_721ed_row80_col16" class="data row80 col16" >1.000000</td>
      <td id="T_721ed_row80_col17" class="data row80 col17" >llama.model.layers.14.self_attn.k_proj.lora_B.default.weight</td>
      <td id="T_721ed_row80_col18" class="data row80 col18" >llama.model.layers.14.self_attn.k_proj.lora_A.default.weight</td>
      <td id="T_721ed_row80_col19" class="data row80 col19" >nan</td>
    </tr>
    <tr>
      <th id="T_721ed_level0_row81" class="row_heading level0 row81" >81</th>
      <td id="T_721ed_row81_col0" class="data row81 col0" >model.layers.18.self_attn.v_proj.weight</td>
      <td id="T_721ed_row81_col1" class="data row81 col1" >model.layers.18.self_attn.v_proj.weight</td>
      <td id="T_721ed_row81_col2" class="data row81 col2" >llama.model.layers.18.self_attn.v_proj</td>
      <td id="T_721ed_row81_col3" class="data row81 col3" >3.03</td>
      <td id="T_721ed_row81_col4" class="data row81 col4" >25.7239</td>
      <td id="T_721ed_row81_col5" class="data row81 col5" >25.902</td>
      <td id="T_721ed_row81_col6" class="data row81 col6" >0.117789</td>
      <td id="T_721ed_row81_col7" class="data row81 col7" >0.993134</td>
      <td id="T_721ed_row81_col8" class="data row81 col8" >0.0137038</td>
      <td id="T_721ed_row81_col9" class="data row81 col9" >4149</td>
      <td id="T_721ed_row81_col10" class="data row81 col10" >1</td>
      <td id="T_721ed_row81_col11" class="data row81 col11" >3145728</td>
      <td id="T_721ed_row81_col12" class="data row81 col12" >torch.float32</td>
      <td id="T_721ed_row81_col13" class="data row81 col13" >(1024, 3072)</td>
      <td id="T_721ed_row81_col14" class="data row81 col14" >16</td>
      <td id="T_721ed_row81_col15" class="data row81 col15" >16.000000</td>
      <td id="T_721ed_row81_col16" class="data row81 col16" >1.000000</td>
      <td id="T_721ed_row81_col17" class="data row81 col17" >llama.model.layers.18.self_attn.v_proj.lora_B.default.weight</td>
      <td id="T_721ed_row81_col18" class="data row81 col18" >llama.model.layers.18.self_attn.v_proj.lora_A.default.weight</td>
      <td id="T_721ed_row81_col19" class="data row81 col19" >nan</td>
    </tr>
    <tr>
      <th id="T_721ed_level0_row82" class="row_heading level0 row82" >82</th>
      <td id="T_721ed_row82_col0" class="data row82 col0" >model.layers.8.self_attn.k_proj.weight</td>
      <td id="T_721ed_row82_col1" class="data row82 col1" >model.layers.8.self_attn.k_proj.weight</td>
      <td id="T_721ed_row82_col2" class="data row82 col2" >llama.model.layers.8.self_attn.k_proj</td>
      <td id="T_721ed_row82_col3" class="data row82 col3" >3.00426</td>
      <td id="T_721ed_row82_col4" class="data row82 col4" >57.3202</td>
      <td id="T_721ed_row82_col5" class="data row82 col5" >57.4029</td>
      <td id="T_721ed_row82_col6" class="data row82 col6" >0.0524119</td>
      <td id="T_721ed_row82_col7" class="data row82 col7" >0.99863</td>
      <td id="T_721ed_row82_col8" class="data row82 col8" >0.0285971</td>
      <td id="T_721ed_row82_col9" class="data row82 col9" >3760.68</td>
      <td id="T_721ed_row82_col10" class="data row82 col10" >1</td>
      <td id="T_721ed_row82_col11" class="data row82 col11" >3145728</td>
      <td id="T_721ed_row82_col12" class="data row82 col12" >torch.float32</td>
      <td id="T_721ed_row82_col13" class="data row82 col13" >(1024, 3072)</td>
      <td id="T_721ed_row82_col14" class="data row82 col14" >16</td>
      <td id="T_721ed_row82_col15" class="data row82 col15" >16.000000</td>
      <td id="T_721ed_row82_col16" class="data row82 col16" >1.000000</td>
      <td id="T_721ed_row82_col17" class="data row82 col17" >llama.model.layers.8.self_attn.k_proj.lora_B.default.weight</td>
      <td id="T_721ed_row82_col18" class="data row82 col18" >llama.model.layers.8.self_attn.k_proj.lora_A.default.weight</td>
      <td id="T_721ed_row82_col19" class="data row82 col19" >nan</td>
    </tr>
    <tr>
      <th id="T_721ed_level0_row83" class="row_heading level0 row83" >83</th>
      <td id="T_721ed_row83_col0" class="data row83 col0" >model.layers.2.self_attn.k_proj.weight</td>
      <td id="T_721ed_row83_col1" class="data row83 col1" >model.layers.2.self_attn.k_proj.weight</td>
      <td id="T_721ed_row83_col2" class="data row83 col2" >llama.model.layers.2.self_attn.k_proj</td>
      <td id="T_721ed_row83_col3" class="data row83 col3" >2.93963</td>
      <td id="T_721ed_row83_col4" class="data row83 col4" >59.1663</td>
      <td id="T_721ed_row83_col5" class="data row83 col5" >59.2411</td>
      <td id="T_721ed_row83_col6" class="data row83 col6" >0.0496842</td>
      <td id="T_721ed_row83_col7" class="data row83 col7" >0.998768</td>
      <td id="T_721ed_row83_col8" class="data row83 col8" >0.0289586</td>
      <td id="T_721ed_row83_col9" class="data row83 col9" >3695.61</td>
      <td id="T_721ed_row83_col10" class="data row83 col10" >1</td>
      <td id="T_721ed_row83_col11" class="data row83 col11" >3145728</td>
      <td id="T_721ed_row83_col12" class="data row83 col12" >torch.float32</td>
      <td id="T_721ed_row83_col13" class="data row83 col13" >(1024, 3072)</td>
      <td id="T_721ed_row83_col14" class="data row83 col14" >16</td>
      <td id="T_721ed_row83_col15" class="data row83 col15" >16.000000</td>
      <td id="T_721ed_row83_col16" class="data row83 col16" >1.000000</td>
      <td id="T_721ed_row83_col17" class="data row83 col17" >llama.model.layers.2.self_attn.k_proj.lora_B.default.weight</td>
      <td id="T_721ed_row83_col18" class="data row83 col18" >llama.model.layers.2.self_attn.k_proj.lora_A.default.weight</td>
      <td id="T_721ed_row83_col19" class="data row83 col19" >nan</td>
    </tr>
    <tr>
      <th id="T_721ed_level0_row84" class="row_heading level0 row84" >84</th>
      <td id="T_721ed_row84_col0" class="data row84 col0" >model.layers.12.self_attn.k_proj.weight</td>
      <td id="T_721ed_row84_col1" class="data row84 col1" >model.layers.12.self_attn.k_proj.weight</td>
      <td id="T_721ed_row84_col2" class="data row84 col2" >llama.model.layers.12.self_attn.k_proj</td>
      <td id="T_721ed_row84_col3" class="data row84 col3" >2.9074</td>
      <td id="T_721ed_row84_col4" class="data row84 col4" >56.418</td>
      <td id="T_721ed_row84_col5" class="data row84 col5" >56.4928</td>
      <td id="T_721ed_row84_col6" class="data row84 col6" >0.0515332</td>
      <td id="T_721ed_row84_col7" class="data row84 col7" >0.998675</td>
      <td id="T_721ed_row84_col8" class="data row84 col8" >0.0225244</td>
      <td id="T_721ed_row84_col9" class="data row84 col9" >3817.35</td>
      <td id="T_721ed_row84_col10" class="data row84 col10" >1</td>
      <td id="T_721ed_row84_col11" class="data row84 col11" >3145728</td>
      <td id="T_721ed_row84_col12" class="data row84 col12" >torch.float32</td>
      <td id="T_721ed_row84_col13" class="data row84 col13" >(1024, 3072)</td>
      <td id="T_721ed_row84_col14" class="data row84 col14" >16</td>
      <td id="T_721ed_row84_col15" class="data row84 col15" >16.000000</td>
      <td id="T_721ed_row84_col16" class="data row84 col16" >1.000000</td>
      <td id="T_721ed_row84_col17" class="data row84 col17" >llama.model.layers.12.self_attn.k_proj.lora_B.default.weight</td>
      <td id="T_721ed_row84_col18" class="data row84 col18" >llama.model.layers.12.self_attn.k_proj.lora_A.default.weight</td>
      <td id="T_721ed_row84_col19" class="data row84 col19" >nan</td>
    </tr>
    <tr>
      <th id="T_721ed_level0_row85" class="row_heading level0 row85" >85</th>
      <td id="T_721ed_row85_col0" class="data row85 col0" >model.layers.6.self_attn.k_proj.weight</td>
      <td id="T_721ed_row85_col1" class="data row85 col1" >model.layers.6.self_attn.k_proj.weight</td>
      <td id="T_721ed_row85_col2" class="data row85 col2" >llama.model.layers.6.self_attn.k_proj</td>
      <td id="T_721ed_row85_col3" class="data row85 col3" >2.89224</td>
      <td id="T_721ed_row85_col4" class="data row85 col4" >57.3127</td>
      <td id="T_721ed_row85_col5" class="data row85 col5" >57.3803</td>
      <td id="T_721ed_row85_col6" class="data row85 col6" >0.0504642</td>
      <td id="T_721ed_row85_col7" class="data row85 col7" >0.998729</td>
      <td id="T_721ed_row85_col8" class="data row85 col8" >0.0192696</td>
      <td id="T_721ed_row85_col9" class="data row85 col9" >3721.71</td>
      <td id="T_721ed_row85_col10" class="data row85 col10" >1</td>
      <td id="T_721ed_row85_col11" class="data row85 col11" >3145728</td>
      <td id="T_721ed_row85_col12" class="data row85 col12" >torch.float32</td>
      <td id="T_721ed_row85_col13" class="data row85 col13" >(1024, 3072)</td>
      <td id="T_721ed_row85_col14" class="data row85 col14" >16</td>
      <td id="T_721ed_row85_col15" class="data row85 col15" >16.000000</td>
      <td id="T_721ed_row85_col16" class="data row85 col16" >1.000000</td>
      <td id="T_721ed_row85_col17" class="data row85 col17" >llama.model.layers.6.self_attn.k_proj.lora_B.default.weight</td>
      <td id="T_721ed_row85_col18" class="data row85 col18" >llama.model.layers.6.self_attn.k_proj.lora_A.default.weight</td>
      <td id="T_721ed_row85_col19" class="data row85 col19" >nan</td>
    </tr>
    <tr>
      <th id="T_721ed_level0_row86" class="row_heading level0 row86" >86</th>
      <td id="T_721ed_row86_col0" class="data row86 col0" >model.layers.4.self_attn.k_proj.weight</td>
      <td id="T_721ed_row86_col1" class="data row86 col1" >model.layers.4.self_attn.k_proj.weight</td>
      <td id="T_721ed_row86_col2" class="data row86 col2" >llama.model.layers.4.self_attn.k_proj</td>
      <td id="T_721ed_row86_col3" class="data row86 col3" >2.80383</td>
      <td id="T_721ed_row86_col4" class="data row86 col4" >56.6293</td>
      <td id="T_721ed_row86_col5" class="data row86 col5" >56.6999</td>
      <td id="T_721ed_row86_col6" class="data row86 col6" >0.049512</td>
      <td id="T_721ed_row86_col7" class="data row86 col7" >0.998777</td>
      <td id="T_721ed_row86_col8" class="data row86 col8" >0.0183982</td>
      <td id="T_721ed_row86_col9" class="data row86 col9" >3639.36</td>
      <td id="T_721ed_row86_col10" class="data row86 col10" >1</td>
      <td id="T_721ed_row86_col11" class="data row86 col11" >3145728</td>
      <td id="T_721ed_row86_col12" class="data row86 col12" >torch.float32</td>
      <td id="T_721ed_row86_col13" class="data row86 col13" >(1024, 3072)</td>
      <td id="T_721ed_row86_col14" class="data row86 col14" >16</td>
      <td id="T_721ed_row86_col15" class="data row86 col15" >16.000000</td>
      <td id="T_721ed_row86_col16" class="data row86 col16" >1.000000</td>
      <td id="T_721ed_row86_col17" class="data row86 col17" >llama.model.layers.4.self_attn.k_proj.lora_B.default.weight</td>
      <td id="T_721ed_row86_col18" class="data row86 col18" >llama.model.layers.4.self_attn.k_proj.lora_A.default.weight</td>
      <td id="T_721ed_row86_col19" class="data row86 col19" >nan</td>
    </tr>
    <tr>
      <th id="T_721ed_level0_row87" class="row_heading level0 row87" >87</th>
      <td id="T_721ed_row87_col0" class="data row87 col0" >model.layers.10.self_attn.k_proj.weight</td>
      <td id="T_721ed_row87_col1" class="data row87 col1" >model.layers.10.self_attn.k_proj.weight</td>
      <td id="T_721ed_row87_col2" class="data row87 col2" >llama.model.layers.10.self_attn.k_proj</td>
      <td id="T_721ed_row87_col3" class="data row87 col3" >2.74085</td>
      <td id="T_721ed_row87_col4" class="data row87 col4" >56.4646</td>
      <td id="T_721ed_row87_col5" class="data row87 col5" >56.5299</td>
      <td id="T_721ed_row87_col6" class="data row87 col6" >0.0485411</td>
      <td id="T_721ed_row87_col7" class="data row87 col7" >0.998824</td>
      <td id="T_721ed_row87_col8" class="data row87 col8" >0.0177984</td>
      <td id="T_721ed_row87_col9" class="data row87 col9" >3537.54</td>
      <td id="T_721ed_row87_col10" class="data row87 col10" >1</td>
      <td id="T_721ed_row87_col11" class="data row87 col11" >3145728</td>
      <td id="T_721ed_row87_col12" class="data row87 col12" >torch.float32</td>
      <td id="T_721ed_row87_col13" class="data row87 col13" >(1024, 3072)</td>
      <td id="T_721ed_row87_col14" class="data row87 col14" >16</td>
      <td id="T_721ed_row87_col15" class="data row87 col15" >16.000000</td>
      <td id="T_721ed_row87_col16" class="data row87 col16" >1.000000</td>
      <td id="T_721ed_row87_col17" class="data row87 col17" >llama.model.layers.10.self_attn.k_proj.lora_B.default.weight</td>
      <td id="T_721ed_row87_col18" class="data row87 col18" >llama.model.layers.10.self_attn.k_proj.lora_A.default.weight</td>
      <td id="T_721ed_row87_col19" class="data row87 col19" >nan</td>
    </tr>
    <tr>
      <th id="T_721ed_level0_row88" class="row_heading level0 row88" >88</th>
      <td id="T_721ed_row88_col0" class="data row88 col0" >model.layers.7.self_attn.k_proj.weight</td>
      <td id="T_721ed_row88_col1" class="data row88 col1" >model.layers.7.self_attn.k_proj.weight</td>
      <td id="T_721ed_row88_col2" class="data row88 col2" >llama.model.layers.7.self_attn.k_proj</td>
      <td id="T_721ed_row88_col3" class="data row88 col3" >2.7248</td>
      <td id="T_721ed_row88_col4" class="data row88 col4" >58.7602</td>
      <td id="T_721ed_row88_col5" class="data row88 col5" >58.8154</td>
      <td id="T_721ed_row88_col6" class="data row88 col6" >0.0463715</td>
      <td id="T_721ed_row88_col7" class="data row88 col7" >0.998926</td>
      <td id="T_721ed_row88_col8" class="data row88 col8" >0.0167915</td>
      <td id="T_721ed_row88_col9" class="data row88 col9" >3503.95</td>
      <td id="T_721ed_row88_col10" class="data row88 col10" >1</td>
      <td id="T_721ed_row88_col11" class="data row88 col11" >3145728</td>
      <td id="T_721ed_row88_col12" class="data row88 col12" >torch.float32</td>
      <td id="T_721ed_row88_col13" class="data row88 col13" >(1024, 3072)</td>
      <td id="T_721ed_row88_col14" class="data row88 col14" >16</td>
      <td id="T_721ed_row88_col15" class="data row88 col15" >16.000000</td>
      <td id="T_721ed_row88_col16" class="data row88 col16" >1.000000</td>
      <td id="T_721ed_row88_col17" class="data row88 col17" >llama.model.layers.7.self_attn.k_proj.lora_B.default.weight</td>
      <td id="T_721ed_row88_col18" class="data row88 col18" >llama.model.layers.7.self_attn.k_proj.lora_A.default.weight</td>
      <td id="T_721ed_row88_col19" class="data row88 col19" >nan</td>
    </tr>
    <tr>
      <th id="T_721ed_level0_row89" class="row_heading level0 row89" >89</th>
      <td id="T_721ed_row89_col0" class="data row89 col0" >model.layers.9.self_attn.k_proj.weight</td>
      <td id="T_721ed_row89_col1" class="data row89 col1" >model.layers.9.self_attn.k_proj.weight</td>
      <td id="T_721ed_row89_col2" class="data row89 col2" >llama.model.layers.9.self_attn.k_proj</td>
      <td id="T_721ed_row89_col3" class="data row89 col3" >2.71634</td>
      <td id="T_721ed_row89_col4" class="data row89 col4" >57.2564</td>
      <td id="T_721ed_row89_col5" class="data row89 col5" >57.322</td>
      <td id="T_721ed_row89_col6" class="data row89 col6" >0.0474417</td>
      <td id="T_721ed_row89_col7" class="data row89 col7" >0.998877</td>
      <td id="T_721ed_row89_col8" class="data row89 col8" >0.0164159</td>
      <td id="T_721ed_row89_col9" class="data row89 col9" >3537.52</td>
      <td id="T_721ed_row89_col10" class="data row89 col10" >1</td>
      <td id="T_721ed_row89_col11" class="data row89 col11" >3145728</td>
      <td id="T_721ed_row89_col12" class="data row89 col12" >torch.float32</td>
      <td id="T_721ed_row89_col13" class="data row89 col13" >(1024, 3072)</td>
      <td id="T_721ed_row89_col14" class="data row89 col14" >16</td>
      <td id="T_721ed_row89_col15" class="data row89 col15" >16.000000</td>
      <td id="T_721ed_row89_col16" class="data row89 col16" >1.000000</td>
      <td id="T_721ed_row89_col17" class="data row89 col17" >llama.model.layers.9.self_attn.k_proj.lora_B.default.weight</td>
      <td id="T_721ed_row89_col18" class="data row89 col18" >llama.model.layers.9.self_attn.k_proj.lora_A.default.weight</td>
      <td id="T_721ed_row89_col19" class="data row89 col19" >nan</td>
    </tr>
    <tr>
      <th id="T_721ed_level0_row90" class="row_heading level0 row90" >90</th>
      <td id="T_721ed_row90_col0" class="data row90 col0" >model.layers.11.self_attn.k_proj.weight</td>
      <td id="T_721ed_row90_col1" class="data row90 col1" >model.layers.11.self_attn.k_proj.weight</td>
      <td id="T_721ed_row90_col2" class="data row90 col2" >llama.model.layers.11.self_attn.k_proj</td>
      <td id="T_721ed_row90_col3" class="data row90 col3" >2.71303</td>
      <td id="T_721ed_row90_col4" class="data row90 col4" >55.4222</td>
      <td id="T_721ed_row90_col5" class="data row90 col5" >55.4891</td>
      <td id="T_721ed_row90_col6" class="data row90 col6" >0.0489521</td>
      <td id="T_721ed_row90_col7" class="data row90 col7" >0.998804</td>
      <td id="T_721ed_row90_col8" class="data row90 col8" >0.0187035</td>
      <td id="T_721ed_row90_col9" class="data row90 col9" >3467.07</td>
      <td id="T_721ed_row90_col10" class="data row90 col10" >1</td>
      <td id="T_721ed_row90_col11" class="data row90 col11" >3145728</td>
      <td id="T_721ed_row90_col12" class="data row90 col12" >torch.float32</td>
      <td id="T_721ed_row90_col13" class="data row90 col13" >(1024, 3072)</td>
      <td id="T_721ed_row90_col14" class="data row90 col14" >16</td>
      <td id="T_721ed_row90_col15" class="data row90 col15" >16.000000</td>
      <td id="T_721ed_row90_col16" class="data row90 col16" >1.000000</td>
      <td id="T_721ed_row90_col17" class="data row90 col17" >llama.model.layers.11.self_attn.k_proj.lora_B.default.weight</td>
      <td id="T_721ed_row90_col18" class="data row90 col18" >llama.model.layers.11.self_attn.k_proj.lora_A.default.weight</td>
      <td id="T_721ed_row90_col19" class="data row90 col19" >nan</td>
    </tr>
    <tr>
      <th id="T_721ed_level0_row91" class="row_heading level0 row91" >91</th>
      <td id="T_721ed_row91_col0" class="data row91 col0" >model.layers.3.self_attn.k_proj.weight</td>
      <td id="T_721ed_row91_col1" class="data row91 col1" >model.layers.3.self_attn.k_proj.weight</td>
      <td id="T_721ed_row91_col2" class="data row91 col2" >llama.model.layers.3.self_attn.k_proj</td>
      <td id="T_721ed_row91_col3" class="data row91 col3" >2.70736</td>
      <td id="T_721ed_row91_col4" class="data row91 col4" >57.7874</td>
      <td id="T_721ed_row91_col5" class="data row91 col5" >57.8537</td>
      <td id="T_721ed_row91_col6" class="data row91 col6" >0.0468503</td>
      <td id="T_721ed_row91_col7" class="data row91 col7" >0.998904</td>
      <td id="T_721ed_row91_col8" class="data row91 col8" >0.0140924</td>
      <td id="T_721ed_row91_col9" class="data row91 col9" >3581.44</td>
      <td id="T_721ed_row91_col10" class="data row91 col10" >1</td>
      <td id="T_721ed_row91_col11" class="data row91 col11" >3145728</td>
      <td id="T_721ed_row91_col12" class="data row91 col12" >torch.float32</td>
      <td id="T_721ed_row91_col13" class="data row91 col13" >(1024, 3072)</td>
      <td id="T_721ed_row91_col14" class="data row91 col14" >16</td>
      <td id="T_721ed_row91_col15" class="data row91 col15" >16.000000</td>
      <td id="T_721ed_row91_col16" class="data row91 col16" >1.000000</td>
      <td id="T_721ed_row91_col17" class="data row91 col17" >llama.model.layers.3.self_attn.k_proj.lora_B.default.weight</td>
      <td id="T_721ed_row91_col18" class="data row91 col18" >llama.model.layers.3.self_attn.k_proj.lora_A.default.weight</td>
      <td id="T_721ed_row91_col19" class="data row91 col19" >nan</td>
    </tr>
    <tr>
      <th id="T_721ed_level0_row92" class="row_heading level0 row92" >92</th>
      <td id="T_721ed_row92_col0" class="data row92 col0" >model.layers.5.self_attn.k_proj.weight</td>
      <td id="T_721ed_row92_col1" class="data row92 col1" >model.layers.5.self_attn.k_proj.weight</td>
      <td id="T_721ed_row92_col2" class="data row92 col2" >llama.model.layers.5.self_attn.k_proj</td>
      <td id="T_721ed_row92_col3" class="data row92 col3" >2.67999</td>
      <td id="T_721ed_row92_col4" class="data row92 col4" >58.1664</td>
      <td id="T_721ed_row92_col5" class="data row92 col5" >58.2308</td>
      <td id="T_721ed_row92_col6" class="data row92 col6" >0.0460745</td>
      <td id="T_721ed_row92_col7" class="data row92 col7" >0.99894</td>
      <td id="T_721ed_row92_col8" class="data row92 col8" >0.0196248</td>
      <td id="T_721ed_row92_col9" class="data row92 col9" >3426.09</td>
      <td id="T_721ed_row92_col10" class="data row92 col10" >1</td>
      <td id="T_721ed_row92_col11" class="data row92 col11" >3145728</td>
      <td id="T_721ed_row92_col12" class="data row92 col12" >torch.float32</td>
      <td id="T_721ed_row92_col13" class="data row92 col13" >(1024, 3072)</td>
      <td id="T_721ed_row92_col14" class="data row92 col14" >16</td>
      <td id="T_721ed_row92_col15" class="data row92 col15" >16.000000</td>
      <td id="T_721ed_row92_col16" class="data row92 col16" >1.000000</td>
      <td id="T_721ed_row92_col17" class="data row92 col17" >llama.model.layers.5.self_attn.k_proj.lora_B.default.weight</td>
      <td id="T_721ed_row92_col18" class="data row92 col18" >llama.model.layers.5.self_attn.k_proj.lora_A.default.weight</td>
      <td id="T_721ed_row92_col19" class="data row92 col19" >nan</td>
    </tr>
    <tr>
      <th id="T_721ed_level0_row93" class="row_heading level0 row93" >93</th>
      <td id="T_721ed_row93_col0" class="data row93 col0" >model.layers.19.self_attn.v_proj.weight</td>
      <td id="T_721ed_row93_col1" class="data row93 col1" >model.layers.19.self_attn.v_proj.weight</td>
      <td id="T_721ed_row93_col2" class="data row93 col2" >llama.model.layers.19.self_attn.v_proj</td>
      <td id="T_721ed_row93_col3" class="data row93 col3" >2.6302</td>
      <td id="T_721ed_row93_col4" class="data row93 col4" >26.3498</td>
      <td id="T_721ed_row93_col5" class="data row93 col5" >26.4753</td>
      <td id="T_721ed_row93_col6" class="data row93 col6" >0.0998186</td>
      <td id="T_721ed_row93_col7" class="data row93 col7" >0.995053</td>
      <td id="T_721ed_row93_col8" class="data row93 col8" >0.0123851</td>
      <td id="T_721ed_row93_col9" class="data row93 col9" >3602.31</td>
      <td id="T_721ed_row93_col10" class="data row93 col10" >1</td>
      <td id="T_721ed_row93_col11" class="data row93 col11" >3145728</td>
      <td id="T_721ed_row93_col12" class="data row93 col12" >torch.float32</td>
      <td id="T_721ed_row93_col13" class="data row93 col13" >(1024, 3072)</td>
      <td id="T_721ed_row93_col14" class="data row93 col14" >16</td>
      <td id="T_721ed_row93_col15" class="data row93 col15" >16.000000</td>
      <td id="T_721ed_row93_col16" class="data row93 col16" >1.000000</td>
      <td id="T_721ed_row93_col17" class="data row93 col17" >llama.model.layers.19.self_attn.v_proj.lora_B.default.weight</td>
      <td id="T_721ed_row93_col18" class="data row93 col18" >llama.model.layers.19.self_attn.v_proj.lora_A.default.weight</td>
      <td id="T_721ed_row93_col19" class="data row93 col19" >nan</td>
    </tr>
    <tr>
      <th id="T_721ed_level0_row94" class="row_heading level0 row94" >94</th>
      <td id="T_721ed_row94_col0" class="data row94 col0" >model.layers.17.self_attn.v_proj.weight</td>
      <td id="T_721ed_row94_col1" class="data row94 col1" >model.layers.17.self_attn.v_proj.weight</td>
      <td id="T_721ed_row94_col2" class="data row94 col2" >llama.model.layers.17.self_attn.v_proj</td>
      <td id="T_721ed_row94_col3" class="data row94 col3" >2.59812</td>
      <td id="T_721ed_row94_col4" class="data row94 col4" >24.6078</td>
      <td id="T_721ed_row94_col5" class="data row94 col5" >24.7446</td>
      <td id="T_721ed_row94_col6" class="data row94 col6" >0.105581</td>
      <td id="T_721ed_row94_col7" class="data row94 col7" >0.994472</td>
      <td id="T_721ed_row94_col8" class="data row94 col8" >0.012167</td>
      <td id="T_721ed_row94_col9" class="data row94 col9" >3555.95</td>
      <td id="T_721ed_row94_col10" class="data row94 col10" >1</td>
      <td id="T_721ed_row94_col11" class="data row94 col11" >3145728</td>
      <td id="T_721ed_row94_col12" class="data row94 col12" >torch.float32</td>
      <td id="T_721ed_row94_col13" class="data row94 col13" >(1024, 3072)</td>
      <td id="T_721ed_row94_col14" class="data row94 col14" >16</td>
      <td id="T_721ed_row94_col15" class="data row94 col15" >16.000000</td>
      <td id="T_721ed_row94_col16" class="data row94 col16" >1.000000</td>
      <td id="T_721ed_row94_col17" class="data row94 col17" >llama.model.layers.17.self_attn.v_proj.lora_B.default.weight</td>
      <td id="T_721ed_row94_col18" class="data row94 col18" >llama.model.layers.17.self_attn.v_proj.lora_A.default.weight</td>
      <td id="T_721ed_row94_col19" class="data row94 col19" >nan</td>
    </tr>
    <tr>
      <th id="T_721ed_level0_row95" class="row_heading level0 row95" >95</th>
      <td id="T_721ed_row95_col0" class="data row95 col0" >model.layers.16.self_attn.v_proj.weight</td>
      <td id="T_721ed_row95_col1" class="data row95 col1" >model.layers.16.self_attn.v_proj.weight</td>
      <td id="T_721ed_row95_col2" class="data row95 col2" >llama.model.layers.16.self_attn.v_proj</td>
      <td id="T_721ed_row95_col3" class="data row95 col3" >2.40594</td>
      <td id="T_721ed_row95_col4" class="data row95 col4" >24.0853</td>
      <td id="T_721ed_row95_col5" class="data row95 col5" >24.2001</td>
      <td id="T_721ed_row95_col6" class="data row95 col6" >0.0998925</td>
      <td id="T_721ed_row95_col7" class="data row95 col7" >0.995046</td>
      <td id="T_721ed_row95_col8" class="data row95 col8" >0.0103807</td>
      <td id="T_721ed_row95_col9" class="data row95 col9" >3304.45</td>
      <td id="T_721ed_row95_col10" class="data row95 col10" >1</td>
      <td id="T_721ed_row95_col11" class="data row95 col11" >3145728</td>
      <td id="T_721ed_row95_col12" class="data row95 col12" >torch.float32</td>
      <td id="T_721ed_row95_col13" class="data row95 col13" >(1024, 3072)</td>
      <td id="T_721ed_row95_col14" class="data row95 col14" >16</td>
      <td id="T_721ed_row95_col15" class="data row95 col15" >16.000000</td>
      <td id="T_721ed_row95_col16" class="data row95 col16" >1.000000</td>
      <td id="T_721ed_row95_col17" class="data row95 col17" >llama.model.layers.16.self_attn.v_proj.lora_B.default.weight</td>
      <td id="T_721ed_row95_col18" class="data row95 col18" >llama.model.layers.16.self_attn.v_proj.lora_A.default.weight</td>
      <td id="T_721ed_row95_col19" class="data row95 col19" >nan</td>
    </tr>
    <tr>
      <th id="T_721ed_level0_row96" class="row_heading level0 row96" >96</th>
      <td id="T_721ed_row96_col0" class="data row96 col0" >model.layers.15.self_attn.v_proj.weight</td>
      <td id="T_721ed_row96_col1" class="data row96 col1" >model.layers.15.self_attn.v_proj.weight</td>
      <td id="T_721ed_row96_col2" class="data row96 col2" >llama.model.layers.15.self_attn.v_proj</td>
      <td id="T_721ed_row96_col3" class="data row96 col3" >2.27321</td>
      <td id="T_721ed_row96_col4" class="data row96 col4" >25.1801</td>
      <td id="T_721ed_row96_col5" class="data row96 col5" >25.2806</td>
      <td id="T_721ed_row96_col6" class="data row96 col6" >0.0902783</td>
      <td id="T_721ed_row96_col7" class="data row96 col7" >0.995949</td>
      <td id="T_721ed_row96_col8" class="data row96 col8" >0.0128759</td>
      <td id="T_721ed_row96_col9" class="data row96 col9" >3105.65</td>
      <td id="T_721ed_row96_col10" class="data row96 col10" >1</td>
      <td id="T_721ed_row96_col11" class="data row96 col11" >3145728</td>
      <td id="T_721ed_row96_col12" class="data row96 col12" >torch.float32</td>
      <td id="T_721ed_row96_col13" class="data row96 col13" >(1024, 3072)</td>
      <td id="T_721ed_row96_col14" class="data row96 col14" >16</td>
      <td id="T_721ed_row96_col15" class="data row96 col15" >16.000000</td>
      <td id="T_721ed_row96_col16" class="data row96 col16" >1.000000</td>
      <td id="T_721ed_row96_col17" class="data row96 col17" >llama.model.layers.15.self_attn.v_proj.lora_B.default.weight</td>
      <td id="T_721ed_row96_col18" class="data row96 col18" >llama.model.layers.15.self_attn.v_proj.lora_A.default.weight</td>
      <td id="T_721ed_row96_col19" class="data row96 col19" >nan</td>
    </tr>
    <tr>
      <th id="T_721ed_level0_row97" class="row_heading level0 row97" >97</th>
      <td id="T_721ed_row97_col0" class="data row97 col0" >model.layers.4.self_attn.v_proj.weight</td>
      <td id="T_721ed_row97_col1" class="data row97 col1" >model.layers.4.self_attn.v_proj.weight</td>
      <td id="T_721ed_row97_col2" class="data row97 col2" >llama.model.layers.4.self_attn.v_proj</td>
      <td id="T_721ed_row97_col3" class="data row97 col3" >2.23342</td>
      <td id="T_721ed_row97_col4" class="data row97 col4" >25.8087</td>
      <td id="T_721ed_row97_col5" class="data row97 col5" >25.9033</td>
      <td id="T_721ed_row97_col6" class="data row97 col6" >0.0865374</td>
      <td id="T_721ed_row97_col7" class="data row97 col7" >0.996276</td>
      <td id="T_721ed_row97_col8" class="data row97 col8" >0.0095604</td>
      <td id="T_721ed_row97_col9" class="data row97 col9" >3050.23</td>
      <td id="T_721ed_row97_col10" class="data row97 col10" >1</td>
      <td id="T_721ed_row97_col11" class="data row97 col11" >3145728</td>
      <td id="T_721ed_row97_col12" class="data row97 col12" >torch.float32</td>
      <td id="T_721ed_row97_col13" class="data row97 col13" >(1024, 3072)</td>
      <td id="T_721ed_row97_col14" class="data row97 col14" >16</td>
      <td id="T_721ed_row97_col15" class="data row97 col15" >16.000000</td>
      <td id="T_721ed_row97_col16" class="data row97 col16" >1.000000</td>
      <td id="T_721ed_row97_col17" class="data row97 col17" >llama.model.layers.4.self_attn.v_proj.lora_B.default.weight</td>
      <td id="T_721ed_row97_col18" class="data row97 col18" >llama.model.layers.4.self_attn.v_proj.lora_A.default.weight</td>
      <td id="T_721ed_row97_col19" class="data row97 col19" >nan</td>
    </tr>
    <tr>
      <th id="T_721ed_level0_row98" class="row_heading level0 row98" >98</th>
      <td id="T_721ed_row98_col0" class="data row98 col0" >model.layers.14.self_attn.v_proj.weight</td>
      <td id="T_721ed_row98_col1" class="data row98 col1" >model.layers.14.self_attn.v_proj.weight</td>
      <td id="T_721ed_row98_col2" class="data row98 col2" >llama.model.layers.14.self_attn.v_proj</td>
      <td id="T_721ed_row98_col3" class="data row98 col3" >2.18572</td>
      <td id="T_721ed_row98_col4" class="data row98 col4" >24.7311</td>
      <td id="T_721ed_row98_col5" class="data row98 col5" >24.8237</td>
      <td id="T_721ed_row98_col6" class="data row98 col6" >0.0883793</td>
      <td id="T_721ed_row98_col7" class="data row98 col7" >0.996116</td>
      <td id="T_721ed_row98_col8" class="data row98 col8" >0.00965623</td>
      <td id="T_721ed_row98_col9" class="data row98 col9" >2977.49</td>
      <td id="T_721ed_row98_col10" class="data row98 col10" >1</td>
      <td id="T_721ed_row98_col11" class="data row98 col11" >3145728</td>
      <td id="T_721ed_row98_col12" class="data row98 col12" >torch.float32</td>
      <td id="T_721ed_row98_col13" class="data row98 col13" >(1024, 3072)</td>
      <td id="T_721ed_row98_col14" class="data row98 col14" >16</td>
      <td id="T_721ed_row98_col15" class="data row98 col15" >16.000000</td>
      <td id="T_721ed_row98_col16" class="data row98 col16" >1.000000</td>
      <td id="T_721ed_row98_col17" class="data row98 col17" >llama.model.layers.14.self_attn.v_proj.lora_B.default.weight</td>
      <td id="T_721ed_row98_col18" class="data row98 col18" >llama.model.layers.14.self_attn.v_proj.lora_A.default.weight</td>
      <td id="T_721ed_row98_col19" class="data row98 col19" >nan</td>
    </tr>
    <tr>
      <th id="T_721ed_level0_row99" class="row_heading level0 row99" >99</th>
      <td id="T_721ed_row99_col0" class="data row99 col0" >model.layers.3.self_attn.v_proj.weight</td>
      <td id="T_721ed_row99_col1" class="data row99 col1" >model.layers.3.self_attn.v_proj.weight</td>
      <td id="T_721ed_row99_col2" class="data row99 col2" >llama.model.layers.3.self_attn.v_proj</td>
      <td id="T_721ed_row99_col3" class="data row99 col3" >2.17913</td>
      <td id="T_721ed_row99_col4" class="data row99 col4" >24.3715</td>
      <td id="T_721ed_row99_col5" class="data row99 col5" >24.467</td>
      <td id="T_721ed_row99_col6" class="data row99 col6" >0.0894129</td>
      <td id="T_721ed_row99_col7" class="data row99 col7" >0.996026</td>
      <td id="T_721ed_row99_col8" class="data row99 col8" >0.0100574</td>
      <td id="T_721ed_row99_col9" class="data row99 col9" >2992.94</td>
      <td id="T_721ed_row99_col10" class="data row99 col10" >1</td>
      <td id="T_721ed_row99_col11" class="data row99 col11" >3145728</td>
      <td id="T_721ed_row99_col12" class="data row99 col12" >torch.float32</td>
      <td id="T_721ed_row99_col13" class="data row99 col13" >(1024, 3072)</td>
      <td id="T_721ed_row99_col14" class="data row99 col14" >16</td>
      <td id="T_721ed_row99_col15" class="data row99 col15" >16.000000</td>
      <td id="T_721ed_row99_col16" class="data row99 col16" >1.000000</td>
      <td id="T_721ed_row99_col17" class="data row99 col17" >llama.model.layers.3.self_attn.v_proj.lora_B.default.weight</td>
      <td id="T_721ed_row99_col18" class="data row99 col18" >llama.model.layers.3.self_attn.v_proj.lora_A.default.weight</td>
      <td id="T_721ed_row99_col19" class="data row99 col19" >nan</td>
    </tr>
    <tr>
      <th id="T_721ed_level0_row100" class="row_heading level0 row100" >100</th>
      <td id="T_721ed_row100_col0" class="data row100 col0" >model.layers.2.self_attn.v_proj.weight</td>
      <td id="T_721ed_row100_col1" class="data row100 col1" >model.layers.2.self_attn.v_proj.weight</td>
      <td id="T_721ed_row100_col2" class="data row100 col2" >llama.model.layers.2.self_attn.v_proj</td>
      <td id="T_721ed_row100_col3" class="data row100 col3" >2.00725</td>
      <td id="T_721ed_row100_col4" class="data row100 col4" >20.8407</td>
      <td id="T_721ed_row100_col5" class="data row100 col5" >20.9329</td>
      <td id="T_721ed_row100_col6" class="data row100 col6" >0.096314</td>
      <td id="T_721ed_row100_col7" class="data row100 col7" >0.995392</td>
      <td id="T_721ed_row100_col8" class="data row100 col8" >0.0090035</td>
      <td id="T_721ed_row100_col9" class="data row100 col9" >2741.18</td>
      <td id="T_721ed_row100_col10" class="data row100 col10" >1</td>
      <td id="T_721ed_row100_col11" class="data row100 col11" >3145728</td>
      <td id="T_721ed_row100_col12" class="data row100 col12" >torch.float32</td>
      <td id="T_721ed_row100_col13" class="data row100 col13" >(1024, 3072)</td>
      <td id="T_721ed_row100_col14" class="data row100 col14" >16</td>
      <td id="T_721ed_row100_col15" class="data row100 col15" >16.000000</td>
      <td id="T_721ed_row100_col16" class="data row100 col16" >1.000000</td>
      <td id="T_721ed_row100_col17" class="data row100 col17" >llama.model.layers.2.self_attn.v_proj.lora_B.default.weight</td>
      <td id="T_721ed_row100_col18" class="data row100 col18" >llama.model.layers.2.self_attn.v_proj.lora_A.default.weight</td>
      <td id="T_721ed_row100_col19" class="data row100 col19" >nan</td>
    </tr>
    <tr>
      <th id="T_721ed_level0_row101" class="row_heading level0 row101" >101</th>
      <td id="T_721ed_row101_col0" class="data row101 col0" >model.layers.5.self_attn.v_proj.weight</td>
      <td id="T_721ed_row101_col1" class="data row101 col1" >model.layers.5.self_attn.v_proj.weight</td>
      <td id="T_721ed_row101_col2" class="data row101 col2" >llama.model.layers.5.self_attn.v_proj</td>
      <td id="T_721ed_row101_col3" class="data row101 col3" >2.00588</td>
      <td id="T_721ed_row101_col4" class="data row101 col4" >21.549</td>
      <td id="T_721ed_row101_col5" class="data row101 col5" >21.6405</td>
      <td id="T_721ed_row101_col6" class="data row101 col6" >0.0930846</td>
      <td id="T_721ed_row101_col7" class="data row101 col7" >0.995695</td>
      <td id="T_721ed_row101_col8" class="data row101 col8" >0.00935388</td>
      <td id="T_721ed_row101_col9" class="data row101 col9" >2725.08</td>
      <td id="T_721ed_row101_col10" class="data row101 col10" >1</td>
      <td id="T_721ed_row101_col11" class="data row101 col11" >3145728</td>
      <td id="T_721ed_row101_col12" class="data row101 col12" >torch.float32</td>
      <td id="T_721ed_row101_col13" class="data row101 col13" >(1024, 3072)</td>
      <td id="T_721ed_row101_col14" class="data row101 col14" >16</td>
      <td id="T_721ed_row101_col15" class="data row101 col15" >16.000000</td>
      <td id="T_721ed_row101_col16" class="data row101 col16" >1.000000</td>
      <td id="T_721ed_row101_col17" class="data row101 col17" >llama.model.layers.5.self_attn.v_proj.lora_B.default.weight</td>
      <td id="T_721ed_row101_col18" class="data row101 col18" >llama.model.layers.5.self_attn.v_proj.lora_A.default.weight</td>
      <td id="T_721ed_row101_col19" class="data row101 col19" >nan</td>
    </tr>
    <tr>
      <th id="T_721ed_level0_row102" class="row_heading level0 row102" >102</th>
      <td id="T_721ed_row102_col0" class="data row102 col0" >model.layers.12.self_attn.v_proj.weight</td>
      <td id="T_721ed_row102_col1" class="data row102 col1" >model.layers.12.self_attn.v_proj.weight</td>
      <td id="T_721ed_row102_col2" class="data row102 col2" >llama.model.layers.12.self_attn.v_proj</td>
      <td id="T_721ed_row102_col3" class="data row102 col3" >1.99133</td>
      <td id="T_721ed_row102_col4" class="data row102 col4" >22.7791</td>
      <td id="T_721ed_row102_col5" class="data row102 col5" >22.8646</td>
      <td id="T_721ed_row102_col6" class="data row102 col6" >0.0874192</td>
      <td id="T_721ed_row102_col7" class="data row102 col7" >0.9962</td>
      <td id="T_721ed_row102_col8" class="data row102 col8" >0.00916859</td>
      <td id="T_721ed_row102_col9" class="data row102 col9" >2702.27</td>
      <td id="T_721ed_row102_col10" class="data row102 col10" >1</td>
      <td id="T_721ed_row102_col11" class="data row102 col11" >3145728</td>
      <td id="T_721ed_row102_col12" class="data row102 col12" >torch.float32</td>
      <td id="T_721ed_row102_col13" class="data row102 col13" >(1024, 3072)</td>
      <td id="T_721ed_row102_col14" class="data row102 col14" >16</td>
      <td id="T_721ed_row102_col15" class="data row102 col15" >16.000000</td>
      <td id="T_721ed_row102_col16" class="data row102 col16" >1.000000</td>
      <td id="T_721ed_row102_col17" class="data row102 col17" >llama.model.layers.12.self_attn.v_proj.lora_B.default.weight</td>
      <td id="T_721ed_row102_col18" class="data row102 col18" >llama.model.layers.12.self_attn.v_proj.lora_A.default.weight</td>
      <td id="T_721ed_row102_col19" class="data row102 col19" >nan</td>
    </tr>
    <tr>
      <th id="T_721ed_level0_row103" class="row_heading level0 row103" >103</th>
      <td id="T_721ed_row103_col0" class="data row103 col0" >model.layers.9.self_attn.v_proj.weight</td>
      <td id="T_721ed_row103_col1" class="data row103 col1" >model.layers.9.self_attn.v_proj.weight</td>
      <td id="T_721ed_row103_col2" class="data row103 col2" >llama.model.layers.9.self_attn.v_proj</td>
      <td id="T_721ed_row103_col3" class="data row103 col3" >1.98482</td>
      <td id="T_721ed_row103_col4" class="data row103 col4" >24.779</td>
      <td id="T_721ed_row103_col5" class="data row103 col5" >24.8551</td>
      <td id="T_721ed_row103_col6" class="data row103 col6" >0.080101</td>
      <td id="T_721ed_row103_col7" class="data row103 col7" >0.996806</td>
      <td id="T_721ed_row103_col8" class="data row103 col8" >0.0136078</td>
      <td id="T_721ed_row103_col9" class="data row103 col9" >2671.23</td>
      <td id="T_721ed_row103_col10" class="data row103 col10" >1</td>
      <td id="T_721ed_row103_col11" class="data row103 col11" >3145728</td>
      <td id="T_721ed_row103_col12" class="data row103 col12" >torch.float32</td>
      <td id="T_721ed_row103_col13" class="data row103 col13" >(1024, 3072)</td>
      <td id="T_721ed_row103_col14" class="data row103 col14" >16</td>
      <td id="T_721ed_row103_col15" class="data row103 col15" >16.000000</td>
      <td id="T_721ed_row103_col16" class="data row103 col16" >1.000000</td>
      <td id="T_721ed_row103_col17" class="data row103 col17" >llama.model.layers.9.self_attn.v_proj.lora_B.default.weight</td>
      <td id="T_721ed_row103_col18" class="data row103 col18" >llama.model.layers.9.self_attn.v_proj.lora_A.default.weight</td>
      <td id="T_721ed_row103_col19" class="data row103 col19" >nan</td>
    </tr>
    <tr>
      <th id="T_721ed_level0_row104" class="row_heading level0 row104" >104</th>
      <td id="T_721ed_row104_col0" class="data row104 col0" >model.layers.13.self_attn.v_proj.weight</td>
      <td id="T_721ed_row104_col1" class="data row104 col1" >model.layers.13.self_attn.v_proj.weight</td>
      <td id="T_721ed_row104_col2" class="data row104 col2" >llama.model.layers.13.self_attn.v_proj</td>
      <td id="T_721ed_row104_col3" class="data row104 col3" >1.95808</td>
      <td id="T_721ed_row104_col4" class="data row104 col4" >23.5194</td>
      <td id="T_721ed_row104_col5" class="data row104 col5" >23.5941</td>
      <td id="T_721ed_row104_col6" class="data row104 col6" >0.0832537</td>
      <td id="T_721ed_row104_col7" class="data row104 col7" >0.99655</td>
      <td id="T_721ed_row104_col8" class="data row104 col8" >0.00865559</td>
      <td id="T_721ed_row104_col9" class="data row104 col9" >2675.15</td>
      <td id="T_721ed_row104_col10" class="data row104 col10" >1</td>
      <td id="T_721ed_row104_col11" class="data row104 col11" >3145728</td>
      <td id="T_721ed_row104_col12" class="data row104 col12" >torch.float32</td>
      <td id="T_721ed_row104_col13" class="data row104 col13" >(1024, 3072)</td>
      <td id="T_721ed_row104_col14" class="data row104 col14" >16</td>
      <td id="T_721ed_row104_col15" class="data row104 col15" >16.000000</td>
      <td id="T_721ed_row104_col16" class="data row104 col16" >1.000000</td>
      <td id="T_721ed_row104_col17" class="data row104 col17" >llama.model.layers.13.self_attn.v_proj.lora_B.default.weight</td>
      <td id="T_721ed_row104_col18" class="data row104 col18" >llama.model.layers.13.self_attn.v_proj.lora_A.default.weight</td>
      <td id="T_721ed_row104_col19" class="data row104 col19" >nan</td>
    </tr>
    <tr>
      <th id="T_721ed_level0_row105" class="row_heading level0 row105" >105</th>
      <td id="T_721ed_row105_col0" class="data row105 col0" >model.layers.8.self_attn.v_proj.weight</td>
      <td id="T_721ed_row105_col1" class="data row105 col1" >model.layers.8.self_attn.v_proj.weight</td>
      <td id="T_721ed_row105_col2" class="data row105 col2" >llama.model.layers.8.self_attn.v_proj</td>
      <td id="T_721ed_row105_col3" class="data row105 col3" >1.95505</td>
      <td id="T_721ed_row105_col4" class="data row105 col4" >22.55</td>
      <td id="T_721ed_row105_col5" class="data row105 col5" >22.6328</td>
      <td id="T_721ed_row105_col6" class="data row105 col6" >0.0866983</td>
      <td id="T_721ed_row105_col7" class="data row105 col7" >0.996262</td>
      <td id="T_721ed_row105_col8" class="data row105 col8" >0.00909192</td>
      <td id="T_721ed_row105_col9" class="data row105 col9" >2667.65</td>
      <td id="T_721ed_row105_col10" class="data row105 col10" >1</td>
      <td id="T_721ed_row105_col11" class="data row105 col11" >3145728</td>
      <td id="T_721ed_row105_col12" class="data row105 col12" >torch.float32</td>
      <td id="T_721ed_row105_col13" class="data row105 col13" >(1024, 3072)</td>
      <td id="T_721ed_row105_col14" class="data row105 col14" >16</td>
      <td id="T_721ed_row105_col15" class="data row105 col15" >16.000000</td>
      <td id="T_721ed_row105_col16" class="data row105 col16" >1.000000</td>
      <td id="T_721ed_row105_col17" class="data row105 col17" >llama.model.layers.8.self_attn.v_proj.lora_B.default.weight</td>
      <td id="T_721ed_row105_col18" class="data row105 col18" >llama.model.layers.8.self_attn.v_proj.lora_A.default.weight</td>
      <td id="T_721ed_row105_col19" class="data row105 col19" >nan</td>
    </tr>
    <tr>
      <th id="T_721ed_level0_row106" class="row_heading level0 row106" >106</th>
      <td id="T_721ed_row106_col0" class="data row106 col0" >model.layers.6.self_attn.v_proj.weight</td>
      <td id="T_721ed_row106_col1" class="data row106 col1" >model.layers.6.self_attn.v_proj.weight</td>
      <td id="T_721ed_row106_col2" class="data row106 col2" >llama.model.layers.6.self_attn.v_proj</td>
      <td id="T_721ed_row106_col3" class="data row106 col3" >1.93412</td>
      <td id="T_721ed_row106_col4" class="data row106 col4" >22.834</td>
      <td id="T_721ed_row106_col5" class="data row106 col5" >22.9137</td>
      <td id="T_721ed_row106_col6" class="data row106 col6" >0.0847035</td>
      <td id="T_721ed_row106_col7" class="data row106 col7" >0.996431</td>
      <td id="T_721ed_row106_col8" class="data row106 col8" >0.0113955</td>
      <td id="T_721ed_row106_col9" class="data row106 col9" >2636.51</td>
      <td id="T_721ed_row106_col10" class="data row106 col10" >1</td>
      <td id="T_721ed_row106_col11" class="data row106 col11" >3145728</td>
      <td id="T_721ed_row106_col12" class="data row106 col12" >torch.float32</td>
      <td id="T_721ed_row106_col13" class="data row106 col13" >(1024, 3072)</td>
      <td id="T_721ed_row106_col14" class="data row106 col14" >16</td>
      <td id="T_721ed_row106_col15" class="data row106 col15" >16.000000</td>
      <td id="T_721ed_row106_col16" class="data row106 col16" >1.000000</td>
      <td id="T_721ed_row106_col17" class="data row106 col17" >llama.model.layers.6.self_attn.v_proj.lora_B.default.weight</td>
      <td id="T_721ed_row106_col18" class="data row106 col18" >llama.model.layers.6.self_attn.v_proj.lora_A.default.weight</td>
      <td id="T_721ed_row106_col19" class="data row106 col19" >nan</td>
    </tr>
    <tr>
      <th id="T_721ed_level0_row107" class="row_heading level0 row107" >107</th>
      <td id="T_721ed_row107_col0" class="data row107 col0" >model.layers.11.self_attn.v_proj.weight</td>
      <td id="T_721ed_row107_col1" class="data row107 col1" >model.layers.11.self_attn.v_proj.weight</td>
      <td id="T_721ed_row107_col2" class="data row107 col2" >llama.model.layers.11.self_attn.v_proj</td>
      <td id="T_721ed_row107_col3" class="data row107 col3" >1.88426</td>
      <td id="T_721ed_row107_col4" class="data row107 col4" >25.3891</td>
      <td id="T_721ed_row107_col5" class="data row107 col5" >25.4562</td>
      <td id="T_721ed_row107_col6" class="data row107 col6" >0.0742152</td>
      <td id="T_721ed_row107_col7" class="data row107 col7" >0.997257</td>
      <td id="T_721ed_row107_col8" class="data row107 col8" >0.00936097</td>
      <td id="T_721ed_row107_col9" class="data row107 col9" >2562.45</td>
      <td id="T_721ed_row107_col10" class="data row107 col10" >1</td>
      <td id="T_721ed_row107_col11" class="data row107 col11" >3145728</td>
      <td id="T_721ed_row107_col12" class="data row107 col12" >torch.float32</td>
      <td id="T_721ed_row107_col13" class="data row107 col13" >(1024, 3072)</td>
      <td id="T_721ed_row107_col14" class="data row107 col14" >16</td>
      <td id="T_721ed_row107_col15" class="data row107 col15" >16.000000</td>
      <td id="T_721ed_row107_col16" class="data row107 col16" >1.000000</td>
      <td id="T_721ed_row107_col17" class="data row107 col17" >llama.model.layers.11.self_attn.v_proj.lora_B.default.weight</td>
      <td id="T_721ed_row107_col18" class="data row107 col18" >llama.model.layers.11.self_attn.v_proj.lora_A.default.weight</td>
      <td id="T_721ed_row107_col19" class="data row107 col19" >nan</td>
    </tr>
    <tr>
      <th id="T_721ed_level0_row108" class="row_heading level0 row108" >108</th>
      <td id="T_721ed_row108_col0" class="data row108 col0" >model.layers.10.self_attn.v_proj.weight</td>
      <td id="T_721ed_row108_col1" class="data row108 col1" >model.layers.10.self_attn.v_proj.weight</td>
      <td id="T_721ed_row108_col2" class="data row108 col2" >llama.model.layers.10.self_attn.v_proj</td>
      <td id="T_721ed_row108_col3" class="data row108 col3" >1.86883</td>
      <td id="T_721ed_row108_col4" class="data row108 col4" >21.4923</td>
      <td id="T_721ed_row108_col5" class="data row108 col5" >21.5683</td>
      <td id="T_721ed_row108_col6" class="data row108 col6" >0.0869534</td>
      <td id="T_721ed_row108_col7" class="data row108 col7" >0.996239</td>
      <td id="T_721ed_row108_col8" class="data row108 col8" >0.0114234</td>
      <td id="T_721ed_row108_col9" class="data row108 col9" >2527.99</td>
      <td id="T_721ed_row108_col10" class="data row108 col10" >1</td>
      <td id="T_721ed_row108_col11" class="data row108 col11" >3145728</td>
      <td id="T_721ed_row108_col12" class="data row108 col12" >torch.float32</td>
      <td id="T_721ed_row108_col13" class="data row108 col13" >(1024, 3072)</td>
      <td id="T_721ed_row108_col14" class="data row108 col14" >16</td>
      <td id="T_721ed_row108_col15" class="data row108 col15" >16.000000</td>
      <td id="T_721ed_row108_col16" class="data row108 col16" >1.000000</td>
      <td id="T_721ed_row108_col17" class="data row108 col17" >llama.model.layers.10.self_attn.v_proj.lora_B.default.weight</td>
      <td id="T_721ed_row108_col18" class="data row108 col18" >llama.model.layers.10.self_attn.v_proj.lora_A.default.weight</td>
      <td id="T_721ed_row108_col19" class="data row108 col19" >nan</td>
    </tr>
    <tr>
      <th id="T_721ed_level0_row109" class="row_heading level0 row109" >109</th>
      <td id="T_721ed_row109_col0" class="data row109 col0" >model.layers.1.self_attn.v_proj.weight</td>
      <td id="T_721ed_row109_col1" class="data row109 col1" >model.layers.1.self_attn.v_proj.weight</td>
      <td id="T_721ed_row109_col2" class="data row109 col2" >llama.model.layers.1.self_attn.v_proj</td>
      <td id="T_721ed_row109_col3" class="data row109 col3" >1.77331</td>
      <td id="T_721ed_row109_col4" class="data row109 col4" >23.5022</td>
      <td id="T_721ed_row109_col5" class="data row109 col5" >23.5723</td>
      <td id="T_721ed_row109_col6" class="data row109 col6" >0.0754528</td>
      <td id="T_721ed_row109_col7" class="data row109 col7" >0.997166</td>
      <td id="T_721ed_row109_col8" class="data row109 col8" >0.00958968</td>
      <td id="T_721ed_row109_col9" class="data row109 col9" >2425.8</td>
      <td id="T_721ed_row109_col10" class="data row109 col10" >1</td>
      <td id="T_721ed_row109_col11" class="data row109 col11" >3145728</td>
      <td id="T_721ed_row109_col12" class="data row109 col12" >torch.float32</td>
      <td id="T_721ed_row109_col13" class="data row109 col13" >(1024, 3072)</td>
      <td id="T_721ed_row109_col14" class="data row109 col14" >16</td>
      <td id="T_721ed_row109_col15" class="data row109 col15" >16.000000</td>
      <td id="T_721ed_row109_col16" class="data row109 col16" >1.000000</td>
      <td id="T_721ed_row109_col17" class="data row109 col17" >llama.model.layers.1.self_attn.v_proj.lora_B.default.weight</td>
      <td id="T_721ed_row109_col18" class="data row109 col18" >llama.model.layers.1.self_attn.v_proj.lora_A.default.weight</td>
      <td id="T_721ed_row109_col19" class="data row109 col19" >nan</td>
    </tr>
    <tr>
      <th id="T_721ed_level0_row110" class="row_heading level0 row110" >110</th>
      <td id="T_721ed_row110_col0" class="data row110 col0" >model.layers.7.self_attn.v_proj.weight</td>
      <td id="T_721ed_row110_col1" class="data row110 col1" >model.layers.7.self_attn.v_proj.weight</td>
      <td id="T_721ed_row110_col2" class="data row110 col2" >llama.model.layers.7.self_attn.v_proj</td>
      <td id="T_721ed_row110_col3" class="data row110 col3" >1.76134</td>
      <td id="T_721ed_row110_col4" class="data row110 col4" >22.2591</td>
      <td id="T_721ed_row110_col5" class="data row110 col5" >22.3312</td>
      <td id="T_721ed_row110_col6" class="data row110 col6" >0.079129</td>
      <td id="T_721ed_row110_col7" class="data row110 col7" >0.996885</td>
      <td id="T_721ed_row110_col8" class="data row110 col8" >0.00959792</td>
      <td id="T_721ed_row110_col9" class="data row110 col9" >2387.63</td>
      <td id="T_721ed_row110_col10" class="data row110 col10" >1</td>
      <td id="T_721ed_row110_col11" class="data row110 col11" >3145728</td>
      <td id="T_721ed_row110_col12" class="data row110 col12" >torch.float32</td>
      <td id="T_721ed_row110_col13" class="data row110 col13" >(1024, 3072)</td>
      <td id="T_721ed_row110_col14" class="data row110 col14" >16</td>
      <td id="T_721ed_row110_col15" class="data row110 col15" >16.000000</td>
      <td id="T_721ed_row110_col16" class="data row110 col16" >1.000000</td>
      <td id="T_721ed_row110_col17" class="data row110 col17" >llama.model.layers.7.self_attn.v_proj.lora_B.default.weight</td>
      <td id="T_721ed_row110_col18" class="data row110 col18" >llama.model.layers.7.self_attn.v_proj.lora_A.default.weight</td>
      <td id="T_721ed_row110_col19" class="data row110 col19" >nan</td>
    </tr>
    <tr>
      <th id="T_721ed_level0_row111" class="row_heading level0 row111" >111</th>
      <td id="T_721ed_row111_col0" class="data row111 col0" >model.layers.0.self_attn.v_proj.weight</td>
      <td id="T_721ed_row111_col1" class="data row111 col1" >model.layers.0.self_attn.v_proj.weight</td>
      <td id="T_721ed_row111_col2" class="data row111 col2" >llama.model.layers.0.self_attn.v_proj</td>
      <td id="T_721ed_row111_col3" class="data row111 col3" >1.72847</td>
      <td id="T_721ed_row111_col4" class="data row111 col4" >20.3472</td>
      <td id="T_721ed_row111_col5" class="data row111 col5" >20.4177</td>
      <td id="T_721ed_row111_col6" class="data row111 col6" >0.0849488</td>
      <td id="T_721ed_row111_col7" class="data row111 col7" >0.99641</td>
      <td id="T_721ed_row111_col8" class="data row111 col8" >0.010759</td>
      <td id="T_721ed_row111_col9" class="data row111 col9" >2317.34</td>
      <td id="T_721ed_row111_col10" class="data row111 col10" >1</td>
      <td id="T_721ed_row111_col11" class="data row111 col11" >3145728</td>
      <td id="T_721ed_row111_col12" class="data row111 col12" >torch.float32</td>
      <td id="T_721ed_row111_col13" class="data row111 col13" >(1024, 3072)</td>
      <td id="T_721ed_row111_col14" class="data row111 col14" >16</td>
      <td id="T_721ed_row111_col15" class="data row111 col15" >16.000000</td>
      <td id="T_721ed_row111_col16" class="data row111 col16" >1.000000</td>
      <td id="T_721ed_row111_col17" class="data row111 col17" >llama.model.layers.0.self_attn.v_proj.lora_B.default.weight</td>
      <td id="T_721ed_row111_col18" class="data row111 col18" >llama.model.layers.0.self_attn.v_proj.lora_A.default.weight</td>
      <td id="T_721ed_row111_col19" class="data row111 col19" >nan</td>
    </tr>
  </tbody>
</table>

  </body>
</html>
